{
  "best_metric": 2.304513454437256,
  "best_model_checkpoint": "./email_model/checkpoint-20000",
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 20000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01,
      "grad_norm": 19.385080337524414,
      "learning_rate": 1.9980000000000002e-05,
      "loss": 2.3341,
      "step": 50
    },
    {
      "epoch": 0.02,
      "grad_norm": 13.161770820617676,
      "learning_rate": 1.9960000000000002e-05,
      "loss": 2.3542,
      "step": 100
    },
    {
      "epoch": 0.03,
      "grad_norm": 14.307844161987305,
      "learning_rate": 1.9940000000000002e-05,
      "loss": 2.3319,
      "step": 150
    },
    {
      "epoch": 0.04,
      "grad_norm": 14.867508888244629,
      "learning_rate": 1.9920000000000002e-05,
      "loss": 2.3312,
      "step": 200
    },
    {
      "epoch": 0.05,
      "grad_norm": 12.671855926513672,
      "learning_rate": 1.9900000000000003e-05,
      "loss": 2.3214,
      "step": 250
    },
    {
      "epoch": 0.06,
      "grad_norm": 15.80639362335205,
      "learning_rate": 1.9880000000000003e-05,
      "loss": 2.3178,
      "step": 300
    },
    {
      "epoch": 0.07,
      "grad_norm": 10.494821548461914,
      "learning_rate": 1.9860000000000003e-05,
      "loss": 2.3207,
      "step": 350
    },
    {
      "epoch": 0.08,
      "grad_norm": 14.765535354614258,
      "learning_rate": 1.9840000000000003e-05,
      "loss": 2.3178,
      "step": 400
    },
    {
      "epoch": 0.09,
      "grad_norm": 10.924908638000488,
      "learning_rate": 1.982e-05,
      "loss": 2.332,
      "step": 450
    },
    {
      "epoch": 0.1,
      "grad_norm": 12.74946403503418,
      "learning_rate": 1.98e-05,
      "loss": 2.3365,
      "step": 500
    },
    {
      "epoch": 0.11,
      "grad_norm": 15.624679565429688,
      "learning_rate": 1.978e-05,
      "loss": 2.3199,
      "step": 550
    },
    {
      "epoch": 0.12,
      "grad_norm": 12.352532386779785,
      "learning_rate": 1.976e-05,
      "loss": 2.3214,
      "step": 600
    },
    {
      "epoch": 0.13,
      "grad_norm": 11.360438346862793,
      "learning_rate": 1.974e-05,
      "loss": 2.3056,
      "step": 650
    },
    {
      "epoch": 0.14,
      "grad_norm": 15.717706680297852,
      "learning_rate": 1.972e-05,
      "loss": 2.3308,
      "step": 700
    },
    {
      "epoch": 0.15,
      "grad_norm": 13.675253868103027,
      "learning_rate": 1.97e-05,
      "loss": 2.3208,
      "step": 750
    },
    {
      "epoch": 0.16,
      "grad_norm": 13.025445938110352,
      "learning_rate": 1.968e-05,
      "loss": 2.3475,
      "step": 800
    },
    {
      "epoch": 0.17,
      "grad_norm": 11.246455192565918,
      "learning_rate": 1.966e-05,
      "loss": 2.3247,
      "step": 850
    },
    {
      "epoch": 0.18,
      "grad_norm": 10.59349250793457,
      "learning_rate": 1.9640000000000002e-05,
      "loss": 2.3338,
      "step": 900
    },
    {
      "epoch": 0.19,
      "grad_norm": 10.056619644165039,
      "learning_rate": 1.9620000000000002e-05,
      "loss": 2.3275,
      "step": 950
    },
    {
      "epoch": 0.2,
      "grad_norm": 7.479935646057129,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 2.3025,
      "step": 1000
    },
    {
      "epoch": 0.21,
      "grad_norm": 9.534283638000488,
      "learning_rate": 1.9580000000000002e-05,
      "loss": 2.3234,
      "step": 1050
    },
    {
      "epoch": 0.22,
      "grad_norm": 14.704282760620117,
      "learning_rate": 1.9560000000000002e-05,
      "loss": 2.3331,
      "step": 1100
    },
    {
      "epoch": 0.23,
      "grad_norm": 10.346384048461914,
      "learning_rate": 1.9540000000000003e-05,
      "loss": 2.3136,
      "step": 1150
    },
    {
      "epoch": 0.24,
      "grad_norm": 11.171239852905273,
      "learning_rate": 1.9520000000000003e-05,
      "loss": 2.331,
      "step": 1200
    },
    {
      "epoch": 0.25,
      "grad_norm": 10.557096481323242,
      "learning_rate": 1.95e-05,
      "loss": 2.3231,
      "step": 1250
    },
    {
      "epoch": 0.26,
      "grad_norm": 17.52431297302246,
      "learning_rate": 1.948e-05,
      "loss": 2.3308,
      "step": 1300
    },
    {
      "epoch": 0.27,
      "grad_norm": 17.11765480041504,
      "learning_rate": 1.946e-05,
      "loss": 2.3278,
      "step": 1350
    },
    {
      "epoch": 0.28,
      "grad_norm": 11.50122356414795,
      "learning_rate": 1.944e-05,
      "loss": 2.3105,
      "step": 1400
    },
    {
      "epoch": 0.29,
      "grad_norm": 9.483380317687988,
      "learning_rate": 1.942e-05,
      "loss": 2.31,
      "step": 1450
    },
    {
      "epoch": 0.3,
      "grad_norm": 11.612031936645508,
      "learning_rate": 1.94e-05,
      "loss": 2.3266,
      "step": 1500
    },
    {
      "epoch": 0.31,
      "grad_norm": 8.691301345825195,
      "learning_rate": 1.938e-05,
      "loss": 2.3374,
      "step": 1550
    },
    {
      "epoch": 0.32,
      "grad_norm": 14.132046699523926,
      "learning_rate": 1.936e-05,
      "loss": 2.3197,
      "step": 1600
    },
    {
      "epoch": 0.33,
      "grad_norm": 9.921758651733398,
      "learning_rate": 1.934e-05,
      "loss": 2.3284,
      "step": 1650
    },
    {
      "epoch": 0.34,
      "grad_norm": 8.381479263305664,
      "learning_rate": 1.932e-05,
      "loss": 2.3315,
      "step": 1700
    },
    {
      "epoch": 0.35,
      "grad_norm": 13.13210678100586,
      "learning_rate": 1.93e-05,
      "loss": 2.3151,
      "step": 1750
    },
    {
      "epoch": 0.36,
      "grad_norm": 11.878467559814453,
      "learning_rate": 1.9280000000000002e-05,
      "loss": 2.3179,
      "step": 1800
    },
    {
      "epoch": 0.37,
      "grad_norm": 10.282159805297852,
      "learning_rate": 1.9260000000000002e-05,
      "loss": 2.3165,
      "step": 1850
    },
    {
      "epoch": 0.38,
      "grad_norm": 6.800757884979248,
      "learning_rate": 1.9240000000000002e-05,
      "loss": 2.3115,
      "step": 1900
    },
    {
      "epoch": 0.39,
      "grad_norm": 8.903271675109863,
      "learning_rate": 1.9220000000000002e-05,
      "loss": 2.3193,
      "step": 1950
    },
    {
      "epoch": 0.4,
      "grad_norm": 9.88573169708252,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 2.326,
      "step": 2000
    },
    {
      "epoch": 0.41,
      "grad_norm": 11.043472290039062,
      "learning_rate": 1.918e-05,
      "loss": 2.3208,
      "step": 2050
    },
    {
      "epoch": 0.42,
      "grad_norm": 13.510946273803711,
      "learning_rate": 1.916e-05,
      "loss": 2.3239,
      "step": 2100
    },
    {
      "epoch": 0.43,
      "grad_norm": 14.747854232788086,
      "learning_rate": 1.914e-05,
      "loss": 2.3087,
      "step": 2150
    },
    {
      "epoch": 0.44,
      "grad_norm": 10.008430480957031,
      "learning_rate": 1.912e-05,
      "loss": 2.3128,
      "step": 2200
    },
    {
      "epoch": 0.45,
      "grad_norm": 14.800253868103027,
      "learning_rate": 1.91e-05,
      "loss": 2.3215,
      "step": 2250
    },
    {
      "epoch": 0.46,
      "grad_norm": 8.392619132995605,
      "learning_rate": 1.908e-05,
      "loss": 2.3174,
      "step": 2300
    },
    {
      "epoch": 0.47,
      "grad_norm": 9.418739318847656,
      "learning_rate": 1.906e-05,
      "loss": 2.3099,
      "step": 2350
    },
    {
      "epoch": 0.48,
      "grad_norm": 9.136834144592285,
      "learning_rate": 1.904e-05,
      "loss": 2.3334,
      "step": 2400
    },
    {
      "epoch": 0.49,
      "grad_norm": 11.362898826599121,
      "learning_rate": 1.902e-05,
      "loss": 2.316,
      "step": 2450
    },
    {
      "epoch": 0.5,
      "grad_norm": 8.661493301391602,
      "learning_rate": 1.9e-05,
      "loss": 2.3097,
      "step": 2500
    },
    {
      "epoch": 0.51,
      "grad_norm": 7.960152626037598,
      "learning_rate": 1.898e-05,
      "loss": 2.3271,
      "step": 2550
    },
    {
      "epoch": 0.52,
      "grad_norm": 12.086928367614746,
      "learning_rate": 1.896e-05,
      "loss": 2.308,
      "step": 2600
    },
    {
      "epoch": 0.53,
      "grad_norm": 10.56352710723877,
      "learning_rate": 1.894e-05,
      "loss": 2.2963,
      "step": 2650
    },
    {
      "epoch": 0.54,
      "grad_norm": 15.181665420532227,
      "learning_rate": 1.8920000000000002e-05,
      "loss": 2.3279,
      "step": 2700
    },
    {
      "epoch": 0.55,
      "grad_norm": 11.225553512573242,
      "learning_rate": 1.8900000000000002e-05,
      "loss": 2.3287,
      "step": 2750
    },
    {
      "epoch": 0.56,
      "grad_norm": 8.99458122253418,
      "learning_rate": 1.8880000000000002e-05,
      "loss": 2.3059,
      "step": 2800
    },
    {
      "epoch": 0.57,
      "grad_norm": 8.360197067260742,
      "learning_rate": 1.886e-05,
      "loss": 2.324,
      "step": 2850
    },
    {
      "epoch": 0.58,
      "grad_norm": 8.176400184631348,
      "learning_rate": 1.884e-05,
      "loss": 2.3054,
      "step": 2900
    },
    {
      "epoch": 0.59,
      "grad_norm": 9.855246543884277,
      "learning_rate": 1.882e-05,
      "loss": 2.3094,
      "step": 2950
    },
    {
      "epoch": 0.6,
      "grad_norm": 8.629568099975586,
      "learning_rate": 1.88e-05,
      "loss": 2.322,
      "step": 3000
    },
    {
      "epoch": 0.61,
      "grad_norm": 6.261651039123535,
      "learning_rate": 1.878e-05,
      "loss": 2.3263,
      "step": 3050
    },
    {
      "epoch": 0.62,
      "grad_norm": 9.921367645263672,
      "learning_rate": 1.876e-05,
      "loss": 2.3246,
      "step": 3100
    },
    {
      "epoch": 0.63,
      "grad_norm": 10.512733459472656,
      "learning_rate": 1.8740000000000004e-05,
      "loss": 2.3344,
      "step": 3150
    },
    {
      "epoch": 0.64,
      "grad_norm": 13.285110473632812,
      "learning_rate": 1.8720000000000004e-05,
      "loss": 2.3215,
      "step": 3200
    },
    {
      "epoch": 0.65,
      "grad_norm": 7.4466118812561035,
      "learning_rate": 1.8700000000000004e-05,
      "loss": 2.3159,
      "step": 3250
    },
    {
      "epoch": 0.66,
      "grad_norm": 9.474383354187012,
      "learning_rate": 1.8680000000000004e-05,
      "loss": 2.3157,
      "step": 3300
    },
    {
      "epoch": 0.67,
      "grad_norm": 10.68678092956543,
      "learning_rate": 1.866e-05,
      "loss": 2.3104,
      "step": 3350
    },
    {
      "epoch": 0.68,
      "grad_norm": 10.165528297424316,
      "learning_rate": 1.864e-05,
      "loss": 2.3329,
      "step": 3400
    },
    {
      "epoch": 0.69,
      "grad_norm": 6.324038505554199,
      "learning_rate": 1.862e-05,
      "loss": 2.3258,
      "step": 3450
    },
    {
      "epoch": 0.7,
      "grad_norm": 9.206775665283203,
      "learning_rate": 1.86e-05,
      "loss": 2.3007,
      "step": 3500
    },
    {
      "epoch": 0.71,
      "grad_norm": 7.852708339691162,
      "learning_rate": 1.858e-05,
      "loss": 2.3228,
      "step": 3550
    },
    {
      "epoch": 0.72,
      "grad_norm": 9.669377326965332,
      "learning_rate": 1.8560000000000002e-05,
      "loss": 2.3151,
      "step": 3600
    },
    {
      "epoch": 0.73,
      "grad_norm": 10.16727352142334,
      "learning_rate": 1.8540000000000002e-05,
      "loss": 2.3086,
      "step": 3650
    },
    {
      "epoch": 0.74,
      "grad_norm": 11.445466995239258,
      "learning_rate": 1.8520000000000002e-05,
      "loss": 2.3355,
      "step": 3700
    },
    {
      "epoch": 0.75,
      "grad_norm": 11.221419334411621,
      "learning_rate": 1.8500000000000002e-05,
      "loss": 2.3151,
      "step": 3750
    },
    {
      "epoch": 0.76,
      "grad_norm": 6.535282135009766,
      "learning_rate": 1.8480000000000003e-05,
      "loss": 2.3194,
      "step": 3800
    },
    {
      "epoch": 0.77,
      "grad_norm": 5.986711502075195,
      "learning_rate": 1.8460000000000003e-05,
      "loss": 2.3227,
      "step": 3850
    },
    {
      "epoch": 0.78,
      "grad_norm": 9.166412353515625,
      "learning_rate": 1.8440000000000003e-05,
      "loss": 2.3128,
      "step": 3900
    },
    {
      "epoch": 0.79,
      "grad_norm": 8.470988273620605,
      "learning_rate": 1.8420000000000003e-05,
      "loss": 2.3115,
      "step": 3950
    },
    {
      "epoch": 0.8,
      "grad_norm": 10.219285011291504,
      "learning_rate": 1.8400000000000003e-05,
      "loss": 2.306,
      "step": 4000
    },
    {
      "epoch": 0.81,
      "grad_norm": 9.251163482666016,
      "learning_rate": 1.8380000000000004e-05,
      "loss": 2.315,
      "step": 4050
    },
    {
      "epoch": 0.82,
      "grad_norm": 6.770888805389404,
      "learning_rate": 1.8360000000000004e-05,
      "loss": 2.3101,
      "step": 4100
    },
    {
      "epoch": 0.83,
      "grad_norm": 5.594916820526123,
      "learning_rate": 1.834e-05,
      "loss": 2.323,
      "step": 4150
    },
    {
      "epoch": 0.84,
      "grad_norm": 7.392215728759766,
      "learning_rate": 1.832e-05,
      "loss": 2.3068,
      "step": 4200
    },
    {
      "epoch": 0.85,
      "grad_norm": 5.590731143951416,
      "learning_rate": 1.83e-05,
      "loss": 2.326,
      "step": 4250
    },
    {
      "epoch": 0.86,
      "grad_norm": 5.061831951141357,
      "learning_rate": 1.828e-05,
      "loss": 2.3313,
      "step": 4300
    },
    {
      "epoch": 0.87,
      "grad_norm": 4.9249796867370605,
      "learning_rate": 1.826e-05,
      "loss": 2.327,
      "step": 4350
    },
    {
      "epoch": 0.88,
      "grad_norm": 5.29218053817749,
      "learning_rate": 1.824e-05,
      "loss": 2.3147,
      "step": 4400
    },
    {
      "epoch": 0.89,
      "grad_norm": 6.002513408660889,
      "learning_rate": 1.8220000000000002e-05,
      "loss": 2.3204,
      "step": 4450
    },
    {
      "epoch": 0.9,
      "grad_norm": 8.211124420166016,
      "learning_rate": 1.8200000000000002e-05,
      "loss": 2.3083,
      "step": 4500
    },
    {
      "epoch": 0.91,
      "grad_norm": 6.4096479415893555,
      "learning_rate": 1.8180000000000002e-05,
      "loss": 2.3113,
      "step": 4550
    },
    {
      "epoch": 0.92,
      "grad_norm": 5.323182106018066,
      "learning_rate": 1.8160000000000002e-05,
      "loss": 2.3137,
      "step": 4600
    },
    {
      "epoch": 0.93,
      "grad_norm": 5.445685863494873,
      "learning_rate": 1.8140000000000003e-05,
      "loss": 2.3183,
      "step": 4650
    },
    {
      "epoch": 0.94,
      "grad_norm": 6.645382404327393,
      "learning_rate": 1.8120000000000003e-05,
      "loss": 2.3155,
      "step": 4700
    },
    {
      "epoch": 0.95,
      "grad_norm": 5.411093235015869,
      "learning_rate": 1.8100000000000003e-05,
      "loss": 2.3093,
      "step": 4750
    },
    {
      "epoch": 0.96,
      "grad_norm": 5.59566593170166,
      "learning_rate": 1.8080000000000003e-05,
      "loss": 2.3181,
      "step": 4800
    },
    {
      "epoch": 0.97,
      "grad_norm": 4.804438591003418,
      "learning_rate": 1.8060000000000003e-05,
      "loss": 2.3102,
      "step": 4850
    },
    {
      "epoch": 0.98,
      "grad_norm": 5.247166633605957,
      "learning_rate": 1.8040000000000003e-05,
      "loss": 2.3136,
      "step": 4900
    },
    {
      "epoch": 0.99,
      "grad_norm": 6.232105731964111,
      "learning_rate": 1.802e-05,
      "loss": 2.3171,
      "step": 4950
    },
    {
      "epoch": 1.0,
      "grad_norm": 5.53768253326416,
      "learning_rate": 1.8e-05,
      "loss": 2.3126,
      "step": 5000
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.310702085494995,
      "eval_runtime": 64.8727,
      "eval_samples_per_second": 154.148,
      "eval_steps_per_second": 19.269,
      "step": 5000
    },
    {
      "epoch": 1.01,
      "grad_norm": 6.028878211975098,
      "learning_rate": 1.798e-05,
      "loss": 2.3181,
      "step": 5050
    },
    {
      "epoch": 1.02,
      "grad_norm": 5.792974472045898,
      "learning_rate": 1.796e-05,
      "loss": 2.3135,
      "step": 5100
    },
    {
      "epoch": 1.03,
      "grad_norm": 6.692221164703369,
      "learning_rate": 1.794e-05,
      "loss": 2.3046,
      "step": 5150
    },
    {
      "epoch": 1.04,
      "grad_norm": 7.3547682762146,
      "learning_rate": 1.792e-05,
      "loss": 2.3192,
      "step": 5200
    },
    {
      "epoch": 1.05,
      "grad_norm": 8.036142349243164,
      "learning_rate": 1.79e-05,
      "loss": 2.3056,
      "step": 5250
    },
    {
      "epoch": 1.06,
      "grad_norm": 5.976397514343262,
      "learning_rate": 1.788e-05,
      "loss": 2.3252,
      "step": 5300
    },
    {
      "epoch": 1.07,
      "grad_norm": 7.806346416473389,
      "learning_rate": 1.7860000000000002e-05,
      "loss": 2.3007,
      "step": 5350
    },
    {
      "epoch": 1.08,
      "grad_norm": 5.3514533042907715,
      "learning_rate": 1.7840000000000002e-05,
      "loss": 2.311,
      "step": 5400
    },
    {
      "epoch": 1.09,
      "grad_norm": 4.838935375213623,
      "learning_rate": 1.7820000000000002e-05,
      "loss": 2.3108,
      "step": 5450
    },
    {
      "epoch": 1.1,
      "grad_norm": 5.012179374694824,
      "learning_rate": 1.7800000000000002e-05,
      "loss": 2.3142,
      "step": 5500
    },
    {
      "epoch": 1.11,
      "grad_norm": 5.582799434661865,
      "learning_rate": 1.7780000000000003e-05,
      "loss": 2.3132,
      "step": 5550
    },
    {
      "epoch": 1.12,
      "grad_norm": 4.960293769836426,
      "learning_rate": 1.7760000000000003e-05,
      "loss": 2.3124,
      "step": 5600
    },
    {
      "epoch": 1.13,
      "grad_norm": 5.920013904571533,
      "learning_rate": 1.7740000000000003e-05,
      "loss": 2.3131,
      "step": 5650
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 3.6646342277526855,
      "learning_rate": 1.7720000000000003e-05,
      "loss": 2.3116,
      "step": 5700
    },
    {
      "epoch": 1.15,
      "grad_norm": 6.484598636627197,
      "learning_rate": 1.77e-05,
      "loss": 2.307,
      "step": 5750
    },
    {
      "epoch": 1.16,
      "grad_norm": 6.540764808654785,
      "learning_rate": 1.768e-05,
      "loss": 2.3083,
      "step": 5800
    },
    {
      "epoch": 1.17,
      "grad_norm": 6.659757137298584,
      "learning_rate": 1.766e-05,
      "loss": 2.3097,
      "step": 5850
    },
    {
      "epoch": 1.18,
      "grad_norm": 4.790577411651611,
      "learning_rate": 1.764e-05,
      "loss": 2.3095,
      "step": 5900
    },
    {
      "epoch": 1.19,
      "grad_norm": 4.686943531036377,
      "learning_rate": 1.762e-05,
      "loss": 2.3197,
      "step": 5950
    },
    {
      "epoch": 1.2,
      "grad_norm": 4.200990200042725,
      "learning_rate": 1.76e-05,
      "loss": 2.3046,
      "step": 6000
    },
    {
      "epoch": 1.21,
      "grad_norm": 6.180019378662109,
      "learning_rate": 1.758e-05,
      "loss": 2.3233,
      "step": 6050
    },
    {
      "epoch": 1.22,
      "grad_norm": 7.037835597991943,
      "learning_rate": 1.756e-05,
      "loss": 2.3151,
      "step": 6100
    },
    {
      "epoch": 1.23,
      "grad_norm": 5.64480447769165,
      "learning_rate": 1.754e-05,
      "loss": 2.3171,
      "step": 6150
    },
    {
      "epoch": 1.24,
      "grad_norm": 8.175661087036133,
      "learning_rate": 1.752e-05,
      "loss": 2.311,
      "step": 6200
    },
    {
      "epoch": 1.25,
      "grad_norm": 8.660012245178223,
      "learning_rate": 1.7500000000000002e-05,
      "loss": 2.3052,
      "step": 6250
    },
    {
      "epoch": 1.26,
      "grad_norm": 5.315845489501953,
      "learning_rate": 1.7480000000000002e-05,
      "loss": 2.3325,
      "step": 6300
    },
    {
      "epoch": 1.27,
      "grad_norm": 8.086030006408691,
      "learning_rate": 1.7460000000000002e-05,
      "loss": 2.3076,
      "step": 6350
    },
    {
      "epoch": 1.28,
      "grad_norm": 7.876940727233887,
      "learning_rate": 1.7440000000000002e-05,
      "loss": 2.3174,
      "step": 6400
    },
    {
      "epoch": 1.29,
      "grad_norm": 9.061759948730469,
      "learning_rate": 1.7420000000000003e-05,
      "loss": 2.3178,
      "step": 6450
    },
    {
      "epoch": 1.3,
      "grad_norm": 9.268248558044434,
      "learning_rate": 1.7400000000000003e-05,
      "loss": 2.3111,
      "step": 6500
    },
    {
      "epoch": 1.31,
      "grad_norm": 5.535234451293945,
      "learning_rate": 1.7380000000000003e-05,
      "loss": 2.3073,
      "step": 6550
    },
    {
      "epoch": 1.32,
      "grad_norm": 4.651727199554443,
      "learning_rate": 1.736e-05,
      "loss": 2.3107,
      "step": 6600
    },
    {
      "epoch": 1.33,
      "grad_norm": 4.36002779006958,
      "learning_rate": 1.734e-05,
      "loss": 2.3155,
      "step": 6650
    },
    {
      "epoch": 1.34,
      "grad_norm": 4.653507709503174,
      "learning_rate": 1.732e-05,
      "loss": 2.3124,
      "step": 6700
    },
    {
      "epoch": 1.35,
      "grad_norm": 5.896796703338623,
      "learning_rate": 1.73e-05,
      "loss": 2.3163,
      "step": 6750
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 4.974400043487549,
      "learning_rate": 1.728e-05,
      "loss": 2.3062,
      "step": 6800
    },
    {
      "epoch": 1.37,
      "grad_norm": 5.26999044418335,
      "learning_rate": 1.726e-05,
      "loss": 2.3127,
      "step": 6850
    },
    {
      "epoch": 1.38,
      "grad_norm": 5.759142875671387,
      "learning_rate": 1.724e-05,
      "loss": 2.3029,
      "step": 6900
    },
    {
      "epoch": 1.3900000000000001,
      "grad_norm": 3.6459169387817383,
      "learning_rate": 1.722e-05,
      "loss": 2.3058,
      "step": 6950
    },
    {
      "epoch": 1.4,
      "grad_norm": 5.068429946899414,
      "learning_rate": 1.72e-05,
      "loss": 2.3188,
      "step": 7000
    },
    {
      "epoch": 1.41,
      "grad_norm": 3.3782472610473633,
      "learning_rate": 1.718e-05,
      "loss": 2.3142,
      "step": 7050
    },
    {
      "epoch": 1.42,
      "grad_norm": 4.531368732452393,
      "learning_rate": 1.7160000000000002e-05,
      "loss": 2.3046,
      "step": 7100
    },
    {
      "epoch": 1.43,
      "grad_norm": 3.916994094848633,
      "learning_rate": 1.7140000000000002e-05,
      "loss": 2.3147,
      "step": 7150
    },
    {
      "epoch": 1.44,
      "grad_norm": 5.143563270568848,
      "learning_rate": 1.7120000000000002e-05,
      "loss": 2.3073,
      "step": 7200
    },
    {
      "epoch": 1.45,
      "grad_norm": 3.9445550441741943,
      "learning_rate": 1.7100000000000002e-05,
      "loss": 2.3104,
      "step": 7250
    },
    {
      "epoch": 1.46,
      "grad_norm": 4.125280857086182,
      "learning_rate": 1.7080000000000002e-05,
      "loss": 2.3053,
      "step": 7300
    },
    {
      "epoch": 1.47,
      "grad_norm": 4.985342979431152,
      "learning_rate": 1.7060000000000003e-05,
      "loss": 2.3125,
      "step": 7350
    },
    {
      "epoch": 1.48,
      "grad_norm": 4.524167537689209,
      "learning_rate": 1.704e-05,
      "loss": 2.3163,
      "step": 7400
    },
    {
      "epoch": 1.49,
      "grad_norm": 4.5826239585876465,
      "learning_rate": 1.702e-05,
      "loss": 2.3091,
      "step": 7450
    },
    {
      "epoch": 1.5,
      "grad_norm": 4.576885223388672,
      "learning_rate": 1.7e-05,
      "loss": 2.3043,
      "step": 7500
    },
    {
      "epoch": 1.51,
      "grad_norm": 4.996545791625977,
      "learning_rate": 1.698e-05,
      "loss": 2.3058,
      "step": 7550
    },
    {
      "epoch": 1.52,
      "grad_norm": 5.7647600173950195,
      "learning_rate": 1.696e-05,
      "loss": 2.3154,
      "step": 7600
    },
    {
      "epoch": 1.53,
      "grad_norm": 5.069006443023682,
      "learning_rate": 1.694e-05,
      "loss": 2.3127,
      "step": 7650
    },
    {
      "epoch": 1.54,
      "grad_norm": 5.627108097076416,
      "learning_rate": 1.692e-05,
      "loss": 2.303,
      "step": 7700
    },
    {
      "epoch": 1.55,
      "grad_norm": 6.677248001098633,
      "learning_rate": 1.69e-05,
      "loss": 2.3002,
      "step": 7750
    },
    {
      "epoch": 1.56,
      "grad_norm": 3.887768268585205,
      "learning_rate": 1.688e-05,
      "loss": 2.3152,
      "step": 7800
    },
    {
      "epoch": 1.5699999999999998,
      "grad_norm": 3.9421706199645996,
      "learning_rate": 1.686e-05,
      "loss": 2.3127,
      "step": 7850
    },
    {
      "epoch": 1.58,
      "grad_norm": 6.665843486785889,
      "learning_rate": 1.684e-05,
      "loss": 2.3074,
      "step": 7900
    },
    {
      "epoch": 1.5899999999999999,
      "grad_norm": 4.134314060211182,
      "learning_rate": 1.682e-05,
      "loss": 2.3126,
      "step": 7950
    },
    {
      "epoch": 1.6,
      "grad_norm": 3.595874071121216,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 2.3133,
      "step": 8000
    },
    {
      "epoch": 1.6099999999999999,
      "grad_norm": 6.913798809051514,
      "learning_rate": 1.6780000000000002e-05,
      "loss": 2.306,
      "step": 8050
    },
    {
      "epoch": 1.62,
      "grad_norm": 5.354004383087158,
      "learning_rate": 1.6760000000000002e-05,
      "loss": 2.3092,
      "step": 8100
    },
    {
      "epoch": 1.63,
      "grad_norm": 5.480829238891602,
      "learning_rate": 1.6740000000000002e-05,
      "loss": 2.3188,
      "step": 8150
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 4.864090919494629,
      "learning_rate": 1.672e-05,
      "loss": 2.3124,
      "step": 8200
    },
    {
      "epoch": 1.65,
      "grad_norm": 4.787199020385742,
      "learning_rate": 1.67e-05,
      "loss": 2.3053,
      "step": 8250
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 4.4177069664001465,
      "learning_rate": 1.668e-05,
      "loss": 2.3142,
      "step": 8300
    },
    {
      "epoch": 1.67,
      "grad_norm": 3.4972310066223145,
      "learning_rate": 1.666e-05,
      "loss": 2.3112,
      "step": 8350
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 3.415234327316284,
      "learning_rate": 1.664e-05,
      "loss": 2.2986,
      "step": 8400
    },
    {
      "epoch": 1.69,
      "grad_norm": 5.758885383605957,
      "learning_rate": 1.662e-05,
      "loss": 2.3214,
      "step": 8450
    },
    {
      "epoch": 1.7,
      "grad_norm": 3.8563232421875,
      "learning_rate": 1.66e-05,
      "loss": 2.3013,
      "step": 8500
    },
    {
      "epoch": 1.71,
      "grad_norm": 4.324464797973633,
      "learning_rate": 1.658e-05,
      "loss": 2.316,
      "step": 8550
    },
    {
      "epoch": 1.72,
      "grad_norm": 3.387227773666382,
      "learning_rate": 1.656e-05,
      "loss": 2.3108,
      "step": 8600
    },
    {
      "epoch": 1.73,
      "grad_norm": 4.130061149597168,
      "learning_rate": 1.654e-05,
      "loss": 2.3091,
      "step": 8650
    },
    {
      "epoch": 1.74,
      "grad_norm": 3.028904676437378,
      "learning_rate": 1.652e-05,
      "loss": 2.3108,
      "step": 8700
    },
    {
      "epoch": 1.75,
      "grad_norm": 3.9699511528015137,
      "learning_rate": 1.65e-05,
      "loss": 2.3099,
      "step": 8750
    },
    {
      "epoch": 1.76,
      "grad_norm": 3.5045247077941895,
      "learning_rate": 1.648e-05,
      "loss": 2.3049,
      "step": 8800
    },
    {
      "epoch": 1.77,
      "grad_norm": 3.056018829345703,
      "learning_rate": 1.646e-05,
      "loss": 2.2953,
      "step": 8850
    },
    {
      "epoch": 1.78,
      "grad_norm": 4.373067855834961,
      "learning_rate": 1.6440000000000002e-05,
      "loss": 2.3115,
      "step": 8900
    },
    {
      "epoch": 1.79,
      "grad_norm": 3.7469584941864014,
      "learning_rate": 1.6420000000000002e-05,
      "loss": 2.3142,
      "step": 8950
    },
    {
      "epoch": 1.8,
      "grad_norm": 3.279076337814331,
      "learning_rate": 1.64e-05,
      "loss": 2.304,
      "step": 9000
    },
    {
      "epoch": 1.81,
      "grad_norm": 4.667911052703857,
      "learning_rate": 1.638e-05,
      "loss": 2.3054,
      "step": 9050
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 3.991116762161255,
      "learning_rate": 1.636e-05,
      "loss": 2.3073,
      "step": 9100
    },
    {
      "epoch": 1.83,
      "grad_norm": 4.821560859680176,
      "learning_rate": 1.634e-05,
      "loss": 2.3035,
      "step": 9150
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 3.1430280208587646,
      "learning_rate": 1.632e-05,
      "loss": 2.3035,
      "step": 9200
    },
    {
      "epoch": 1.85,
      "grad_norm": 3.152808666229248,
      "learning_rate": 1.63e-05,
      "loss": 2.315,
      "step": 9250
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 4.123400688171387,
      "learning_rate": 1.628e-05,
      "loss": 2.3049,
      "step": 9300
    },
    {
      "epoch": 1.87,
      "grad_norm": 3.377275228500366,
      "learning_rate": 1.626e-05,
      "loss": 2.322,
      "step": 9350
    },
    {
      "epoch": 1.88,
      "grad_norm": 4.559067249298096,
      "learning_rate": 1.6240000000000004e-05,
      "loss": 2.3047,
      "step": 9400
    },
    {
      "epoch": 1.8900000000000001,
      "grad_norm": 2.9303340911865234,
      "learning_rate": 1.6220000000000004e-05,
      "loss": 2.3062,
      "step": 9450
    },
    {
      "epoch": 1.9,
      "grad_norm": 5.951401233673096,
      "learning_rate": 1.62e-05,
      "loss": 2.3009,
      "step": 9500
    },
    {
      "epoch": 1.9100000000000001,
      "grad_norm": 5.913340091705322,
      "learning_rate": 1.618e-05,
      "loss": 2.309,
      "step": 9550
    },
    {
      "epoch": 1.92,
      "grad_norm": 4.982865333557129,
      "learning_rate": 1.616e-05,
      "loss": 2.3112,
      "step": 9600
    },
    {
      "epoch": 1.9300000000000002,
      "grad_norm": 6.089526176452637,
      "learning_rate": 1.614e-05,
      "loss": 2.3018,
      "step": 9650
    },
    {
      "epoch": 1.94,
      "grad_norm": 3.568571090698242,
      "learning_rate": 1.612e-05,
      "loss": 2.3121,
      "step": 9700
    },
    {
      "epoch": 1.95,
      "grad_norm": 3.9667110443115234,
      "learning_rate": 1.6100000000000002e-05,
      "loss": 2.3033,
      "step": 9750
    },
    {
      "epoch": 1.96,
      "grad_norm": 4.30645227432251,
      "learning_rate": 1.6080000000000002e-05,
      "loss": 2.3016,
      "step": 9800
    },
    {
      "epoch": 1.97,
      "grad_norm": 3.181745767593384,
      "learning_rate": 1.6060000000000002e-05,
      "loss": 2.3061,
      "step": 9850
    },
    {
      "epoch": 1.98,
      "grad_norm": 4.050427436828613,
      "learning_rate": 1.6040000000000002e-05,
      "loss": 2.3129,
      "step": 9900
    },
    {
      "epoch": 1.99,
      "grad_norm": 3.743104934692383,
      "learning_rate": 1.6020000000000002e-05,
      "loss": 2.3091,
      "step": 9950
    },
    {
      "epoch": 2.0,
      "grad_norm": 3.859184741973877,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 2.3055,
      "step": 10000
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.308318614959717,
      "eval_runtime": 72.2132,
      "eval_samples_per_second": 138.479,
      "eval_steps_per_second": 17.31,
      "step": 10000
    },
    {
      "epoch": 2.01,
      "grad_norm": 3.732830762863159,
      "learning_rate": 1.5980000000000003e-05,
      "loss": 2.3025,
      "step": 10050
    },
    {
      "epoch": 2.02,
      "grad_norm": 3.256619453430176,
      "learning_rate": 1.5960000000000003e-05,
      "loss": 2.3162,
      "step": 10100
    },
    {
      "epoch": 2.03,
      "grad_norm": 3.1862199306488037,
      "learning_rate": 1.5940000000000003e-05,
      "loss": 2.3181,
      "step": 10150
    },
    {
      "epoch": 2.04,
      "grad_norm": 3.585097551345825,
      "learning_rate": 1.5920000000000003e-05,
      "loss": 2.3088,
      "step": 10200
    },
    {
      "epoch": 2.05,
      "grad_norm": 4.080259323120117,
      "learning_rate": 1.5900000000000004e-05,
      "loss": 2.3058,
      "step": 10250
    },
    {
      "epoch": 2.06,
      "grad_norm": 4.132308006286621,
      "learning_rate": 1.588e-05,
      "loss": 2.304,
      "step": 10300
    },
    {
      "epoch": 2.07,
      "grad_norm": 3.743607997894287,
      "learning_rate": 1.586e-05,
      "loss": 2.3176,
      "step": 10350
    },
    {
      "epoch": 2.08,
      "grad_norm": 4.319986820220947,
      "learning_rate": 1.584e-05,
      "loss": 2.3079,
      "step": 10400
    },
    {
      "epoch": 2.09,
      "grad_norm": 2.9749534130096436,
      "learning_rate": 1.582e-05,
      "loss": 2.3164,
      "step": 10450
    },
    {
      "epoch": 2.1,
      "grad_norm": 3.565579652786255,
      "learning_rate": 1.58e-05,
      "loss": 2.3064,
      "step": 10500
    },
    {
      "epoch": 2.11,
      "grad_norm": 2.805232048034668,
      "learning_rate": 1.578e-05,
      "loss": 2.3124,
      "step": 10550
    },
    {
      "epoch": 2.12,
      "grad_norm": 3.415048599243164,
      "learning_rate": 1.576e-05,
      "loss": 2.3097,
      "step": 10600
    },
    {
      "epoch": 2.13,
      "grad_norm": 2.904900550842285,
      "learning_rate": 1.5740000000000002e-05,
      "loss": 2.3108,
      "step": 10650
    },
    {
      "epoch": 2.14,
      "grad_norm": 3.5956616401672363,
      "learning_rate": 1.5720000000000002e-05,
      "loss": 2.3022,
      "step": 10700
    },
    {
      "epoch": 2.15,
      "grad_norm": 2.6990110874176025,
      "learning_rate": 1.5700000000000002e-05,
      "loss": 2.3076,
      "step": 10750
    },
    {
      "epoch": 2.16,
      "grad_norm": 3.4828696250915527,
      "learning_rate": 1.5680000000000002e-05,
      "loss": 2.3131,
      "step": 10800
    },
    {
      "epoch": 2.17,
      "grad_norm": 3.941654682159424,
      "learning_rate": 1.5660000000000003e-05,
      "loss": 2.3019,
      "step": 10850
    },
    {
      "epoch": 2.18,
      "grad_norm": 2.979109764099121,
      "learning_rate": 1.5640000000000003e-05,
      "loss": 2.3059,
      "step": 10900
    },
    {
      "epoch": 2.19,
      "grad_norm": 3.3778417110443115,
      "learning_rate": 1.5620000000000003e-05,
      "loss": 2.3036,
      "step": 10950
    },
    {
      "epoch": 2.2,
      "grad_norm": 3.9732325077056885,
      "learning_rate": 1.5600000000000003e-05,
      "loss": 2.2983,
      "step": 11000
    },
    {
      "epoch": 2.21,
      "grad_norm": 3.5947468280792236,
      "learning_rate": 1.5580000000000003e-05,
      "loss": 2.3099,
      "step": 11050
    },
    {
      "epoch": 2.22,
      "grad_norm": 3.825817823410034,
      "learning_rate": 1.556e-05,
      "loss": 2.3103,
      "step": 11100
    },
    {
      "epoch": 2.23,
      "grad_norm": 3.7435054779052734,
      "learning_rate": 1.554e-05,
      "loss": 2.3078,
      "step": 11150
    },
    {
      "epoch": 2.24,
      "grad_norm": 3.1727333068847656,
      "learning_rate": 1.552e-05,
      "loss": 2.303,
      "step": 11200
    },
    {
      "epoch": 2.25,
      "grad_norm": 3.4667391777038574,
      "learning_rate": 1.55e-05,
      "loss": 2.3022,
      "step": 11250
    },
    {
      "epoch": 2.26,
      "grad_norm": 3.2346951961517334,
      "learning_rate": 1.548e-05,
      "loss": 2.3166,
      "step": 11300
    },
    {
      "epoch": 2.27,
      "grad_norm": 5.619704246520996,
      "learning_rate": 1.546e-05,
      "loss": 2.3104,
      "step": 11350
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 2.913992166519165,
      "learning_rate": 1.544e-05,
      "loss": 2.3018,
      "step": 11400
    },
    {
      "epoch": 2.29,
      "grad_norm": 4.6048760414123535,
      "learning_rate": 1.542e-05,
      "loss": 2.3157,
      "step": 11450
    },
    {
      "epoch": 2.3,
      "grad_norm": 4.351783752441406,
      "learning_rate": 1.54e-05,
      "loss": 2.3089,
      "step": 11500
    },
    {
      "epoch": 2.31,
      "grad_norm": 3.867129325866699,
      "learning_rate": 1.5380000000000002e-05,
      "loss": 2.3048,
      "step": 11550
    },
    {
      "epoch": 2.32,
      "grad_norm": 2.719106912612915,
      "learning_rate": 1.5360000000000002e-05,
      "loss": 2.3079,
      "step": 11600
    },
    {
      "epoch": 2.33,
      "grad_norm": 2.7233965396881104,
      "learning_rate": 1.5340000000000002e-05,
      "loss": 2.3039,
      "step": 11650
    },
    {
      "epoch": 2.34,
      "grad_norm": 3.708552122116089,
      "learning_rate": 1.5320000000000002e-05,
      "loss": 2.3012,
      "step": 11700
    },
    {
      "epoch": 2.35,
      "grad_norm": 2.773080587387085,
      "learning_rate": 1.5300000000000003e-05,
      "loss": 2.3058,
      "step": 11750
    },
    {
      "epoch": 2.36,
      "grad_norm": 2.9316444396972656,
      "learning_rate": 1.5280000000000003e-05,
      "loss": 2.312,
      "step": 11800
    },
    {
      "epoch": 2.37,
      "grad_norm": 3.0724332332611084,
      "learning_rate": 1.5260000000000003e-05,
      "loss": 2.3033,
      "step": 11850
    },
    {
      "epoch": 2.38,
      "grad_norm": 3.6343746185302734,
      "learning_rate": 1.5240000000000001e-05,
      "loss": 2.318,
      "step": 11900
    },
    {
      "epoch": 2.39,
      "grad_norm": 2.7692625522613525,
      "learning_rate": 1.5220000000000002e-05,
      "loss": 2.3042,
      "step": 11950
    },
    {
      "epoch": 2.4,
      "grad_norm": 4.3999128341674805,
      "learning_rate": 1.5200000000000002e-05,
      "loss": 2.3085,
      "step": 12000
    },
    {
      "epoch": 2.41,
      "grad_norm": 3.6880948543548584,
      "learning_rate": 1.5180000000000002e-05,
      "loss": 2.3019,
      "step": 12050
    },
    {
      "epoch": 2.42,
      "grad_norm": 3.0880253314971924,
      "learning_rate": 1.516e-05,
      "loss": 2.3172,
      "step": 12100
    },
    {
      "epoch": 2.43,
      "grad_norm": 3.6231324672698975,
      "learning_rate": 1.514e-05,
      "loss": 2.3022,
      "step": 12150
    },
    {
      "epoch": 2.44,
      "grad_norm": 2.9248688220977783,
      "learning_rate": 1.5120000000000001e-05,
      "loss": 2.3121,
      "step": 12200
    },
    {
      "epoch": 2.45,
      "grad_norm": 2.8870835304260254,
      "learning_rate": 1.5100000000000001e-05,
      "loss": 2.3111,
      "step": 12250
    },
    {
      "epoch": 2.46,
      "grad_norm": 3.042067527770996,
      "learning_rate": 1.5080000000000001e-05,
      "loss": 2.3031,
      "step": 12300
    },
    {
      "epoch": 2.4699999999999998,
      "grad_norm": 3.699312210083008,
      "learning_rate": 1.5060000000000001e-05,
      "loss": 2.3152,
      "step": 12350
    },
    {
      "epoch": 2.48,
      "grad_norm": 5.190868377685547,
      "learning_rate": 1.5040000000000002e-05,
      "loss": 2.3081,
      "step": 12400
    },
    {
      "epoch": 2.49,
      "grad_norm": 3.0931396484375,
      "learning_rate": 1.5020000000000002e-05,
      "loss": 2.3103,
      "step": 12450
    },
    {
      "epoch": 2.5,
      "grad_norm": 4.171586036682129,
      "learning_rate": 1.5000000000000002e-05,
      "loss": 2.3056,
      "step": 12500
    },
    {
      "epoch": 2.51,
      "grad_norm": 5.047077178955078,
      "learning_rate": 1.498e-05,
      "loss": 2.3044,
      "step": 12550
    },
    {
      "epoch": 2.52,
      "grad_norm": 4.485304832458496,
      "learning_rate": 1.496e-05,
      "loss": 2.3057,
      "step": 12600
    },
    {
      "epoch": 2.5300000000000002,
      "grad_norm": 3.783811569213867,
      "learning_rate": 1.4940000000000001e-05,
      "loss": 2.3068,
      "step": 12650
    },
    {
      "epoch": 2.54,
      "grad_norm": 2.9442059993743896,
      "learning_rate": 1.4920000000000001e-05,
      "loss": 2.2998,
      "step": 12700
    },
    {
      "epoch": 2.55,
      "grad_norm": 3.0852324962615967,
      "learning_rate": 1.4900000000000001e-05,
      "loss": 2.3102,
      "step": 12750
    },
    {
      "epoch": 2.56,
      "grad_norm": 3.90032696723938,
      "learning_rate": 1.4880000000000002e-05,
      "loss": 2.3067,
      "step": 12800
    },
    {
      "epoch": 2.57,
      "grad_norm": 2.693565845489502,
      "learning_rate": 1.4860000000000002e-05,
      "loss": 2.3039,
      "step": 12850
    },
    {
      "epoch": 2.58,
      "grad_norm": 3.5675225257873535,
      "learning_rate": 1.4840000000000002e-05,
      "loss": 2.3162,
      "step": 12900
    },
    {
      "epoch": 2.59,
      "grad_norm": 3.855067014694214,
      "learning_rate": 1.482e-05,
      "loss": 2.3097,
      "step": 12950
    },
    {
      "epoch": 2.6,
      "grad_norm": 2.658889055252075,
      "learning_rate": 1.48e-05,
      "loss": 2.3159,
      "step": 13000
    },
    {
      "epoch": 2.61,
      "grad_norm": 3.8396799564361572,
      "learning_rate": 1.478e-05,
      "loss": 2.3056,
      "step": 13050
    },
    {
      "epoch": 2.62,
      "grad_norm": 3.602550506591797,
      "learning_rate": 1.4760000000000001e-05,
      "loss": 2.3008,
      "step": 13100
    },
    {
      "epoch": 2.63,
      "grad_norm": 3.13809871673584,
      "learning_rate": 1.4740000000000001e-05,
      "loss": 2.3098,
      "step": 13150
    },
    {
      "epoch": 2.64,
      "grad_norm": 4.111541271209717,
      "learning_rate": 1.4720000000000001e-05,
      "loss": 2.2977,
      "step": 13200
    },
    {
      "epoch": 2.65,
      "grad_norm": 3.49513840675354,
      "learning_rate": 1.4700000000000002e-05,
      "loss": 2.3058,
      "step": 13250
    },
    {
      "epoch": 2.66,
      "grad_norm": 3.305954933166504,
      "learning_rate": 1.4680000000000002e-05,
      "loss": 2.3118,
      "step": 13300
    },
    {
      "epoch": 2.67,
      "grad_norm": 5.552613735198975,
      "learning_rate": 1.466e-05,
      "loss": 2.3004,
      "step": 13350
    },
    {
      "epoch": 2.68,
      "grad_norm": 3.931225538253784,
      "learning_rate": 1.464e-05,
      "loss": 2.3085,
      "step": 13400
    },
    {
      "epoch": 2.69,
      "grad_norm": 3.122083902359009,
      "learning_rate": 1.462e-05,
      "loss": 2.3075,
      "step": 13450
    },
    {
      "epoch": 2.7,
      "grad_norm": 2.972818613052368,
      "learning_rate": 1.46e-05,
      "loss": 2.2986,
      "step": 13500
    },
    {
      "epoch": 2.71,
      "grad_norm": 3.6672205924987793,
      "learning_rate": 1.4580000000000001e-05,
      "loss": 2.312,
      "step": 13550
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 3.7159721851348877,
      "learning_rate": 1.4560000000000001e-05,
      "loss": 2.3058,
      "step": 13600
    },
    {
      "epoch": 2.73,
      "grad_norm": 2.871330499649048,
      "learning_rate": 1.4540000000000001e-05,
      "loss": 2.3119,
      "step": 13650
    },
    {
      "epoch": 2.74,
      "grad_norm": 2.637660264968872,
      "learning_rate": 1.4520000000000002e-05,
      "loss": 2.308,
      "step": 13700
    },
    {
      "epoch": 2.75,
      "grad_norm": 3.606570243835449,
      "learning_rate": 1.45e-05,
      "loss": 2.3047,
      "step": 13750
    },
    {
      "epoch": 2.76,
      "grad_norm": 3.179940700531006,
      "learning_rate": 1.448e-05,
      "loss": 2.3134,
      "step": 13800
    },
    {
      "epoch": 2.77,
      "grad_norm": 3.240126848220825,
      "learning_rate": 1.446e-05,
      "loss": 2.309,
      "step": 13850
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 3.723541498184204,
      "learning_rate": 1.444e-05,
      "loss": 2.2957,
      "step": 13900
    },
    {
      "epoch": 2.79,
      "grad_norm": 4.540886878967285,
      "learning_rate": 1.4420000000000001e-05,
      "loss": 2.3012,
      "step": 13950
    },
    {
      "epoch": 2.8,
      "grad_norm": 3.756434679031372,
      "learning_rate": 1.4400000000000001e-05,
      "loss": 2.3132,
      "step": 14000
    },
    {
      "epoch": 2.81,
      "grad_norm": 3.620326042175293,
      "learning_rate": 1.4380000000000001e-05,
      "loss": 2.3,
      "step": 14050
    },
    {
      "epoch": 2.82,
      "grad_norm": 3.8561251163482666,
      "learning_rate": 1.4360000000000001e-05,
      "loss": 2.3071,
      "step": 14100
    },
    {
      "epoch": 2.83,
      "grad_norm": 4.281414031982422,
      "learning_rate": 1.434e-05,
      "loss": 2.3062,
      "step": 14150
    },
    {
      "epoch": 2.84,
      "grad_norm": 4.314666748046875,
      "learning_rate": 1.432e-05,
      "loss": 2.3066,
      "step": 14200
    },
    {
      "epoch": 2.85,
      "grad_norm": 4.546560764312744,
      "learning_rate": 1.43e-05,
      "loss": 2.3047,
      "step": 14250
    },
    {
      "epoch": 2.86,
      "grad_norm": 3.727492570877075,
      "learning_rate": 1.428e-05,
      "loss": 2.31,
      "step": 14300
    },
    {
      "epoch": 2.87,
      "grad_norm": 3.4926626682281494,
      "learning_rate": 1.426e-05,
      "loss": 2.3125,
      "step": 14350
    },
    {
      "epoch": 2.88,
      "grad_norm": 5.665472030639648,
      "learning_rate": 1.4240000000000001e-05,
      "loss": 2.2998,
      "step": 14400
    },
    {
      "epoch": 2.89,
      "grad_norm": 4.492315769195557,
      "learning_rate": 1.4220000000000001e-05,
      "loss": 2.3063,
      "step": 14450
    },
    {
      "epoch": 2.9,
      "grad_norm": 3.150766134262085,
      "learning_rate": 1.4200000000000001e-05,
      "loss": 2.3127,
      "step": 14500
    },
    {
      "epoch": 2.91,
      "grad_norm": 5.165378570556641,
      "learning_rate": 1.418e-05,
      "loss": 2.3092,
      "step": 14550
    },
    {
      "epoch": 2.92,
      "grad_norm": 4.600502967834473,
      "learning_rate": 1.416e-05,
      "loss": 2.2988,
      "step": 14600
    },
    {
      "epoch": 2.93,
      "grad_norm": 3.7427115440368652,
      "learning_rate": 1.414e-05,
      "loss": 2.3043,
      "step": 14650
    },
    {
      "epoch": 2.94,
      "grad_norm": 3.3843042850494385,
      "learning_rate": 1.412e-05,
      "loss": 2.3005,
      "step": 14700
    },
    {
      "epoch": 2.95,
      "grad_norm": 3.371196746826172,
      "learning_rate": 1.41e-05,
      "loss": 2.3103,
      "step": 14750
    },
    {
      "epoch": 2.96,
      "grad_norm": 3.3087053298950195,
      "learning_rate": 1.408e-05,
      "loss": 2.3152,
      "step": 14800
    },
    {
      "epoch": 2.9699999999999998,
      "grad_norm": 3.2721316814422607,
      "learning_rate": 1.4060000000000001e-05,
      "loss": 2.3102,
      "step": 14850
    },
    {
      "epoch": 2.98,
      "grad_norm": 4.1735029220581055,
      "learning_rate": 1.4040000000000001e-05,
      "loss": 2.3055,
      "step": 14900
    },
    {
      "epoch": 2.99,
      "grad_norm": 3.5431301593780518,
      "learning_rate": 1.402e-05,
      "loss": 2.3121,
      "step": 14950
    },
    {
      "epoch": 3.0,
      "grad_norm": 3.176959276199341,
      "learning_rate": 1.4e-05,
      "loss": 2.3005,
      "step": 15000
    },
    {
      "epoch": 3.0,
      "eval_loss": 2.3112215995788574,
      "eval_runtime": 76.3065,
      "eval_samples_per_second": 131.05,
      "eval_steps_per_second": 16.381,
      "step": 15000
    },
    {
      "epoch": 3.01,
      "grad_norm": 2.5361146926879883,
      "learning_rate": 1.398e-05,
      "loss": 2.3115,
      "step": 15050
    },
    {
      "epoch": 3.02,
      "grad_norm": 2.8698456287384033,
      "learning_rate": 1.396e-05,
      "loss": 2.3093,
      "step": 15100
    },
    {
      "epoch": 3.03,
      "grad_norm": 3.5496513843536377,
      "learning_rate": 1.394e-05,
      "loss": 2.3083,
      "step": 15150
    },
    {
      "epoch": 3.04,
      "grad_norm": 3.587686777114868,
      "learning_rate": 1.392e-05,
      "loss": 2.3071,
      "step": 15200
    },
    {
      "epoch": 3.05,
      "grad_norm": 3.120140314102173,
      "learning_rate": 1.39e-05,
      "loss": 2.3074,
      "step": 15250
    },
    {
      "epoch": 3.06,
      "grad_norm": 2.957615375518799,
      "learning_rate": 1.3880000000000001e-05,
      "loss": 2.3119,
      "step": 15300
    },
    {
      "epoch": 3.07,
      "grad_norm": 4.019079208374023,
      "learning_rate": 1.386e-05,
      "loss": 2.3116,
      "step": 15350
    },
    {
      "epoch": 3.08,
      "grad_norm": 4.531472682952881,
      "learning_rate": 1.384e-05,
      "loss": 2.3096,
      "step": 15400
    },
    {
      "epoch": 3.09,
      "grad_norm": 3.111880302429199,
      "learning_rate": 1.382e-05,
      "loss": 2.3043,
      "step": 15450
    },
    {
      "epoch": 3.1,
      "grad_norm": 3.715158700942993,
      "learning_rate": 1.38e-05,
      "loss": 2.3083,
      "step": 15500
    },
    {
      "epoch": 3.11,
      "grad_norm": 3.4800162315368652,
      "learning_rate": 1.378e-05,
      "loss": 2.3018,
      "step": 15550
    },
    {
      "epoch": 3.12,
      "grad_norm": 2.9092133045196533,
      "learning_rate": 1.376e-05,
      "loss": 2.3103,
      "step": 15600
    },
    {
      "epoch": 3.13,
      "grad_norm": 5.8560943603515625,
      "learning_rate": 1.3740000000000002e-05,
      "loss": 2.3031,
      "step": 15650
    },
    {
      "epoch": 3.14,
      "grad_norm": 2.6789023876190186,
      "learning_rate": 1.3720000000000002e-05,
      "loss": 2.3084,
      "step": 15700
    },
    {
      "epoch": 3.15,
      "grad_norm": 3.3706328868865967,
      "learning_rate": 1.3700000000000003e-05,
      "loss": 2.2967,
      "step": 15750
    },
    {
      "epoch": 3.16,
      "grad_norm": 3.279188632965088,
      "learning_rate": 1.3680000000000003e-05,
      "loss": 2.3108,
      "step": 15800
    },
    {
      "epoch": 3.17,
      "grad_norm": 2.390892267227173,
      "learning_rate": 1.3660000000000001e-05,
      "loss": 2.2896,
      "step": 15850
    },
    {
      "epoch": 3.18,
      "grad_norm": 6.412156105041504,
      "learning_rate": 1.3640000000000002e-05,
      "loss": 2.3079,
      "step": 15900
    },
    {
      "epoch": 3.19,
      "grad_norm": 2.9418134689331055,
      "learning_rate": 1.3620000000000002e-05,
      "loss": 2.3114,
      "step": 15950
    },
    {
      "epoch": 3.2,
      "grad_norm": 3.4472968578338623,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 2.3027,
      "step": 16000
    },
    {
      "epoch": 3.21,
      "grad_norm": 4.883509159088135,
      "learning_rate": 1.3580000000000002e-05,
      "loss": 2.3019,
      "step": 16050
    },
    {
      "epoch": 3.22,
      "grad_norm": 3.1951067447662354,
      "learning_rate": 1.3560000000000002e-05,
      "loss": 2.3131,
      "step": 16100
    },
    {
      "epoch": 3.23,
      "grad_norm": 2.646609306335449,
      "learning_rate": 1.3540000000000003e-05,
      "loss": 2.3059,
      "step": 16150
    },
    {
      "epoch": 3.24,
      "grad_norm": 3.0377821922302246,
      "learning_rate": 1.3520000000000003e-05,
      "loss": 2.3118,
      "step": 16200
    },
    {
      "epoch": 3.25,
      "grad_norm": 3.1002278327941895,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 2.312,
      "step": 16250
    },
    {
      "epoch": 3.26,
      "grad_norm": 3.1326303482055664,
      "learning_rate": 1.3480000000000001e-05,
      "loss": 2.3061,
      "step": 16300
    },
    {
      "epoch": 3.27,
      "grad_norm": 2.7257888317108154,
      "learning_rate": 1.3460000000000002e-05,
      "loss": 2.3083,
      "step": 16350
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 2.843252182006836,
      "learning_rate": 1.3440000000000002e-05,
      "loss": 2.3061,
      "step": 16400
    },
    {
      "epoch": 3.29,
      "grad_norm": 3.4829506874084473,
      "learning_rate": 1.3420000000000002e-05,
      "loss": 2.3133,
      "step": 16450
    },
    {
      "epoch": 3.3,
      "grad_norm": 3.389561414718628,
      "learning_rate": 1.3400000000000002e-05,
      "loss": 2.3079,
      "step": 16500
    },
    {
      "epoch": 3.31,
      "grad_norm": 3.862889289855957,
      "learning_rate": 1.3380000000000002e-05,
      "loss": 2.3074,
      "step": 16550
    },
    {
      "epoch": 3.32,
      "grad_norm": 3.4365620613098145,
      "learning_rate": 1.3360000000000003e-05,
      "loss": 2.3154,
      "step": 16600
    },
    {
      "epoch": 3.33,
      "grad_norm": 2.6510539054870605,
      "learning_rate": 1.3340000000000001e-05,
      "loss": 2.3119,
      "step": 16650
    },
    {
      "epoch": 3.34,
      "grad_norm": 3.0739150047302246,
      "learning_rate": 1.3320000000000001e-05,
      "loss": 2.3024,
      "step": 16700
    },
    {
      "epoch": 3.35,
      "grad_norm": 3.365506172180176,
      "learning_rate": 1.3300000000000001e-05,
      "loss": 2.3112,
      "step": 16750
    },
    {
      "epoch": 3.36,
      "grad_norm": 2.914586305618286,
      "learning_rate": 1.3280000000000002e-05,
      "loss": 2.3055,
      "step": 16800
    },
    {
      "epoch": 3.37,
      "grad_norm": 2.5783047676086426,
      "learning_rate": 1.3260000000000002e-05,
      "loss": 2.3069,
      "step": 16850
    },
    {
      "epoch": 3.38,
      "grad_norm": 3.1818525791168213,
      "learning_rate": 1.3240000000000002e-05,
      "loss": 2.3017,
      "step": 16900
    },
    {
      "epoch": 3.39,
      "grad_norm": 3.334033966064453,
      "learning_rate": 1.3220000000000002e-05,
      "loss": 2.3045,
      "step": 16950
    },
    {
      "epoch": 3.4,
      "grad_norm": 3.1316490173339844,
      "learning_rate": 1.3200000000000002e-05,
      "loss": 2.3041,
      "step": 17000
    },
    {
      "epoch": 3.41,
      "grad_norm": 3.422989845275879,
      "learning_rate": 1.3180000000000001e-05,
      "loss": 2.3154,
      "step": 17050
    },
    {
      "epoch": 3.42,
      "grad_norm": 2.5598626136779785,
      "learning_rate": 1.3160000000000001e-05,
      "loss": 2.3093,
      "step": 17100
    },
    {
      "epoch": 3.43,
      "grad_norm": 3.7407820224761963,
      "learning_rate": 1.3140000000000001e-05,
      "loss": 2.3014,
      "step": 17150
    },
    {
      "epoch": 3.44,
      "grad_norm": 2.7064902782440186,
      "learning_rate": 1.3120000000000001e-05,
      "loss": 2.3044,
      "step": 17200
    },
    {
      "epoch": 3.45,
      "grad_norm": 2.865593194961548,
      "learning_rate": 1.3100000000000002e-05,
      "loss": 2.3037,
      "step": 17250
    },
    {
      "epoch": 3.46,
      "grad_norm": 2.3507792949676514,
      "learning_rate": 1.3080000000000002e-05,
      "loss": 2.3041,
      "step": 17300
    },
    {
      "epoch": 3.4699999999999998,
      "grad_norm": 3.035489797592163,
      "learning_rate": 1.3060000000000002e-05,
      "loss": 2.3139,
      "step": 17350
    },
    {
      "epoch": 3.48,
      "grad_norm": 2.5806639194488525,
      "learning_rate": 1.3040000000000002e-05,
      "loss": 2.3096,
      "step": 17400
    },
    {
      "epoch": 3.49,
      "grad_norm": 4.087007999420166,
      "learning_rate": 1.302e-05,
      "loss": 2.3074,
      "step": 17450
    },
    {
      "epoch": 3.5,
      "grad_norm": 2.7406198978424072,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 2.3039,
      "step": 17500
    },
    {
      "epoch": 3.51,
      "grad_norm": 2.6955206394195557,
      "learning_rate": 1.2980000000000001e-05,
      "loss": 2.3007,
      "step": 17550
    },
    {
      "epoch": 3.52,
      "grad_norm": 2.320455312728882,
      "learning_rate": 1.2960000000000001e-05,
      "loss": 2.3058,
      "step": 17600
    },
    {
      "epoch": 3.5300000000000002,
      "grad_norm": 2.9957616329193115,
      "learning_rate": 1.2940000000000001e-05,
      "loss": 2.3055,
      "step": 17650
    },
    {
      "epoch": 3.54,
      "grad_norm": 3.898442029953003,
      "learning_rate": 1.2920000000000002e-05,
      "loss": 2.3026,
      "step": 17700
    },
    {
      "epoch": 3.55,
      "grad_norm": 2.7614247798919678,
      "learning_rate": 1.2900000000000002e-05,
      "loss": 2.3103,
      "step": 17750
    },
    {
      "epoch": 3.56,
      "grad_norm": 2.703737258911133,
      "learning_rate": 1.2880000000000002e-05,
      "loss": 2.2956,
      "step": 17800
    },
    {
      "epoch": 3.57,
      "grad_norm": 2.9392786026000977,
      "learning_rate": 1.286e-05,
      "loss": 2.3095,
      "step": 17850
    },
    {
      "epoch": 3.58,
      "grad_norm": 2.688724994659424,
      "learning_rate": 1.284e-05,
      "loss": 2.3033,
      "step": 17900
    },
    {
      "epoch": 3.59,
      "grad_norm": 3.2784616947174072,
      "learning_rate": 1.2820000000000001e-05,
      "loss": 2.3008,
      "step": 17950
    },
    {
      "epoch": 3.6,
      "grad_norm": 3.195438861846924,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 2.3045,
      "step": 18000
    },
    {
      "epoch": 3.61,
      "grad_norm": 3.234854221343994,
      "learning_rate": 1.2780000000000001e-05,
      "loss": 2.2986,
      "step": 18050
    },
    {
      "epoch": 3.62,
      "grad_norm": 3.3868660926818848,
      "learning_rate": 1.2760000000000001e-05,
      "loss": 2.3151,
      "step": 18100
    },
    {
      "epoch": 3.63,
      "grad_norm": 3.2249608039855957,
      "learning_rate": 1.2740000000000002e-05,
      "loss": 2.3114,
      "step": 18150
    },
    {
      "epoch": 3.64,
      "grad_norm": 2.40736985206604,
      "learning_rate": 1.2720000000000002e-05,
      "loss": 2.3016,
      "step": 18200
    },
    {
      "epoch": 3.65,
      "grad_norm": 2.814164400100708,
      "learning_rate": 1.27e-05,
      "loss": 2.3092,
      "step": 18250
    },
    {
      "epoch": 3.66,
      "grad_norm": 2.899949789047241,
      "learning_rate": 1.268e-05,
      "loss": 2.3113,
      "step": 18300
    },
    {
      "epoch": 3.67,
      "grad_norm": 2.7486042976379395,
      "learning_rate": 1.266e-05,
      "loss": 2.2999,
      "step": 18350
    },
    {
      "epoch": 3.68,
      "grad_norm": 2.7799060344696045,
      "learning_rate": 1.2640000000000001e-05,
      "loss": 2.3118,
      "step": 18400
    },
    {
      "epoch": 3.69,
      "grad_norm": 3.012212038040161,
      "learning_rate": 1.2620000000000001e-05,
      "loss": 2.304,
      "step": 18450
    },
    {
      "epoch": 3.7,
      "grad_norm": 3.184366464614868,
      "learning_rate": 1.2600000000000001e-05,
      "loss": 2.3122,
      "step": 18500
    },
    {
      "epoch": 3.71,
      "grad_norm": 3.5195910930633545,
      "learning_rate": 1.2580000000000002e-05,
      "loss": 2.3081,
      "step": 18550
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 3.5053794384002686,
      "learning_rate": 1.2560000000000002e-05,
      "loss": 2.3078,
      "step": 18600
    },
    {
      "epoch": 3.73,
      "grad_norm": 4.349270820617676,
      "learning_rate": 1.254e-05,
      "loss": 2.3076,
      "step": 18650
    },
    {
      "epoch": 3.74,
      "grad_norm": 3.548567771911621,
      "learning_rate": 1.252e-05,
      "loss": 2.314,
      "step": 18700
    },
    {
      "epoch": 3.75,
      "grad_norm": 2.480489492416382,
      "learning_rate": 1.25e-05,
      "loss": 2.3056,
      "step": 18750
    },
    {
      "epoch": 3.76,
      "grad_norm": 2.5821006298065186,
      "learning_rate": 1.248e-05,
      "loss": 2.3091,
      "step": 18800
    },
    {
      "epoch": 3.77,
      "grad_norm": 2.2772793769836426,
      "learning_rate": 1.2460000000000001e-05,
      "loss": 2.3091,
      "step": 18850
    },
    {
      "epoch": 3.7800000000000002,
      "grad_norm": 2.981762409210205,
      "learning_rate": 1.2440000000000001e-05,
      "loss": 2.3066,
      "step": 18900
    },
    {
      "epoch": 3.79,
      "grad_norm": 2.21083664894104,
      "learning_rate": 1.2420000000000001e-05,
      "loss": 2.3084,
      "step": 18950
    },
    {
      "epoch": 3.8,
      "grad_norm": 3.631108522415161,
      "learning_rate": 1.2400000000000002e-05,
      "loss": 2.307,
      "step": 19000
    },
    {
      "epoch": 3.81,
      "grad_norm": 3.1925878524780273,
      "learning_rate": 1.2380000000000002e-05,
      "loss": 2.3081,
      "step": 19050
    },
    {
      "epoch": 3.82,
      "grad_norm": 2.6622095108032227,
      "learning_rate": 1.236e-05,
      "loss": 2.307,
      "step": 19100
    },
    {
      "epoch": 3.83,
      "grad_norm": 2.9806296825408936,
      "learning_rate": 1.234e-05,
      "loss": 2.3056,
      "step": 19150
    },
    {
      "epoch": 3.84,
      "grad_norm": 2.269202947616577,
      "learning_rate": 1.232e-05,
      "loss": 2.3048,
      "step": 19200
    },
    {
      "epoch": 3.85,
      "grad_norm": 2.014336109161377,
      "learning_rate": 1.23e-05,
      "loss": 2.309,
      "step": 19250
    },
    {
      "epoch": 3.86,
      "grad_norm": 2.418225049972534,
      "learning_rate": 1.2280000000000001e-05,
      "loss": 2.31,
      "step": 19300
    },
    {
      "epoch": 3.87,
      "grad_norm": 2.5712099075317383,
      "learning_rate": 1.2260000000000001e-05,
      "loss": 2.305,
      "step": 19350
    },
    {
      "epoch": 3.88,
      "grad_norm": 2.8226959705352783,
      "learning_rate": 1.2240000000000001e-05,
      "loss": 2.3071,
      "step": 19400
    },
    {
      "epoch": 3.89,
      "grad_norm": 2.800567626953125,
      "learning_rate": 1.2220000000000002e-05,
      "loss": 2.305,
      "step": 19450
    },
    {
      "epoch": 3.9,
      "grad_norm": 2.868405342102051,
      "learning_rate": 1.22e-05,
      "loss": 2.3014,
      "step": 19500
    },
    {
      "epoch": 3.91,
      "grad_norm": 4.834807395935059,
      "learning_rate": 1.218e-05,
      "loss": 2.3143,
      "step": 19550
    },
    {
      "epoch": 3.92,
      "grad_norm": 3.7130794525146484,
      "learning_rate": 1.216e-05,
      "loss": 2.3025,
      "step": 19600
    },
    {
      "epoch": 3.93,
      "grad_norm": 2.601212978363037,
      "learning_rate": 1.214e-05,
      "loss": 2.3054,
      "step": 19650
    },
    {
      "epoch": 3.94,
      "grad_norm": 2.765312671661377,
      "learning_rate": 1.2120000000000001e-05,
      "loss": 2.3025,
      "step": 19700
    },
    {
      "epoch": 3.95,
      "grad_norm": 3.094862937927246,
      "learning_rate": 1.2100000000000001e-05,
      "loss": 2.3073,
      "step": 19750
    },
    {
      "epoch": 3.96,
      "grad_norm": 2.972184896469116,
      "learning_rate": 1.2080000000000001e-05,
      "loss": 2.3053,
      "step": 19800
    },
    {
      "epoch": 3.9699999999999998,
      "grad_norm": 2.6807777881622314,
      "learning_rate": 1.2060000000000001e-05,
      "loss": 2.3042,
      "step": 19850
    },
    {
      "epoch": 3.98,
      "grad_norm": 2.5225040912628174,
      "learning_rate": 1.204e-05,
      "loss": 2.306,
      "step": 19900
    },
    {
      "epoch": 3.99,
      "grad_norm": 2.9872488975524902,
      "learning_rate": 1.202e-05,
      "loss": 2.3111,
      "step": 19950
    },
    {
      "epoch": 4.0,
      "grad_norm": 2.582008123397827,
      "learning_rate": 1.2e-05,
      "loss": 2.3087,
      "step": 20000
    },
    {
      "epoch": 4.0,
      "eval_loss": 2.304513454437256,
      "eval_runtime": 80.1678,
      "eval_samples_per_second": 124.738,
      "eval_steps_per_second": 15.592,
      "step": 20000
    }
  ],
  "logging_steps": 50,
  "max_steps": 50000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 3,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2047712845440000.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
