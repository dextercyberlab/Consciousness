{
  "best_metric": 2.3035104274749756,
  "best_model_checkpoint": "./email_model/checkpoint-40000",
  "epoch": 8.0,
  "eval_steps": 500,
  "global_step": 40000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01,
      "grad_norm": 19.385080337524414,
      "learning_rate": 1.9980000000000002e-05,
      "loss": 2.3341,
      "step": 50
    },
    {
      "epoch": 0.02,
      "grad_norm": 13.161770820617676,
      "learning_rate": 1.9960000000000002e-05,
      "loss": 2.3542,
      "step": 100
    },
    {
      "epoch": 0.03,
      "grad_norm": 14.307844161987305,
      "learning_rate": 1.9940000000000002e-05,
      "loss": 2.3319,
      "step": 150
    },
    {
      "epoch": 0.04,
      "grad_norm": 14.867508888244629,
      "learning_rate": 1.9920000000000002e-05,
      "loss": 2.3312,
      "step": 200
    },
    {
      "epoch": 0.05,
      "grad_norm": 12.671855926513672,
      "learning_rate": 1.9900000000000003e-05,
      "loss": 2.3214,
      "step": 250
    },
    {
      "epoch": 0.06,
      "grad_norm": 15.80639362335205,
      "learning_rate": 1.9880000000000003e-05,
      "loss": 2.3178,
      "step": 300
    },
    {
      "epoch": 0.07,
      "grad_norm": 10.494821548461914,
      "learning_rate": 1.9860000000000003e-05,
      "loss": 2.3207,
      "step": 350
    },
    {
      "epoch": 0.08,
      "grad_norm": 14.765535354614258,
      "learning_rate": 1.9840000000000003e-05,
      "loss": 2.3178,
      "step": 400
    },
    {
      "epoch": 0.09,
      "grad_norm": 10.924908638000488,
      "learning_rate": 1.982e-05,
      "loss": 2.332,
      "step": 450
    },
    {
      "epoch": 0.1,
      "grad_norm": 12.74946403503418,
      "learning_rate": 1.98e-05,
      "loss": 2.3365,
      "step": 500
    },
    {
      "epoch": 0.11,
      "grad_norm": 15.624679565429688,
      "learning_rate": 1.978e-05,
      "loss": 2.3199,
      "step": 550
    },
    {
      "epoch": 0.12,
      "grad_norm": 12.352532386779785,
      "learning_rate": 1.976e-05,
      "loss": 2.3214,
      "step": 600
    },
    {
      "epoch": 0.13,
      "grad_norm": 11.360438346862793,
      "learning_rate": 1.974e-05,
      "loss": 2.3056,
      "step": 650
    },
    {
      "epoch": 0.14,
      "grad_norm": 15.717706680297852,
      "learning_rate": 1.972e-05,
      "loss": 2.3308,
      "step": 700
    },
    {
      "epoch": 0.15,
      "grad_norm": 13.675253868103027,
      "learning_rate": 1.97e-05,
      "loss": 2.3208,
      "step": 750
    },
    {
      "epoch": 0.16,
      "grad_norm": 13.025445938110352,
      "learning_rate": 1.968e-05,
      "loss": 2.3475,
      "step": 800
    },
    {
      "epoch": 0.17,
      "grad_norm": 11.246455192565918,
      "learning_rate": 1.966e-05,
      "loss": 2.3247,
      "step": 850
    },
    {
      "epoch": 0.18,
      "grad_norm": 10.59349250793457,
      "learning_rate": 1.9640000000000002e-05,
      "loss": 2.3338,
      "step": 900
    },
    {
      "epoch": 0.19,
      "grad_norm": 10.056619644165039,
      "learning_rate": 1.9620000000000002e-05,
      "loss": 2.3275,
      "step": 950
    },
    {
      "epoch": 0.2,
      "grad_norm": 7.479935646057129,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 2.3025,
      "step": 1000
    },
    {
      "epoch": 0.21,
      "grad_norm": 9.534283638000488,
      "learning_rate": 1.9580000000000002e-05,
      "loss": 2.3234,
      "step": 1050
    },
    {
      "epoch": 0.22,
      "grad_norm": 14.704282760620117,
      "learning_rate": 1.9560000000000002e-05,
      "loss": 2.3331,
      "step": 1100
    },
    {
      "epoch": 0.23,
      "grad_norm": 10.346384048461914,
      "learning_rate": 1.9540000000000003e-05,
      "loss": 2.3136,
      "step": 1150
    },
    {
      "epoch": 0.24,
      "grad_norm": 11.171239852905273,
      "learning_rate": 1.9520000000000003e-05,
      "loss": 2.331,
      "step": 1200
    },
    {
      "epoch": 0.25,
      "grad_norm": 10.557096481323242,
      "learning_rate": 1.95e-05,
      "loss": 2.3231,
      "step": 1250
    },
    {
      "epoch": 0.26,
      "grad_norm": 17.52431297302246,
      "learning_rate": 1.948e-05,
      "loss": 2.3308,
      "step": 1300
    },
    {
      "epoch": 0.27,
      "grad_norm": 17.11765480041504,
      "learning_rate": 1.946e-05,
      "loss": 2.3278,
      "step": 1350
    },
    {
      "epoch": 0.28,
      "grad_norm": 11.50122356414795,
      "learning_rate": 1.944e-05,
      "loss": 2.3105,
      "step": 1400
    },
    {
      "epoch": 0.29,
      "grad_norm": 9.483380317687988,
      "learning_rate": 1.942e-05,
      "loss": 2.31,
      "step": 1450
    },
    {
      "epoch": 0.3,
      "grad_norm": 11.612031936645508,
      "learning_rate": 1.94e-05,
      "loss": 2.3266,
      "step": 1500
    },
    {
      "epoch": 0.31,
      "grad_norm": 8.691301345825195,
      "learning_rate": 1.938e-05,
      "loss": 2.3374,
      "step": 1550
    },
    {
      "epoch": 0.32,
      "grad_norm": 14.132046699523926,
      "learning_rate": 1.936e-05,
      "loss": 2.3197,
      "step": 1600
    },
    {
      "epoch": 0.33,
      "grad_norm": 9.921758651733398,
      "learning_rate": 1.934e-05,
      "loss": 2.3284,
      "step": 1650
    },
    {
      "epoch": 0.34,
      "grad_norm": 8.381479263305664,
      "learning_rate": 1.932e-05,
      "loss": 2.3315,
      "step": 1700
    },
    {
      "epoch": 0.35,
      "grad_norm": 13.13210678100586,
      "learning_rate": 1.93e-05,
      "loss": 2.3151,
      "step": 1750
    },
    {
      "epoch": 0.36,
      "grad_norm": 11.878467559814453,
      "learning_rate": 1.9280000000000002e-05,
      "loss": 2.3179,
      "step": 1800
    },
    {
      "epoch": 0.37,
      "grad_norm": 10.282159805297852,
      "learning_rate": 1.9260000000000002e-05,
      "loss": 2.3165,
      "step": 1850
    },
    {
      "epoch": 0.38,
      "grad_norm": 6.800757884979248,
      "learning_rate": 1.9240000000000002e-05,
      "loss": 2.3115,
      "step": 1900
    },
    {
      "epoch": 0.39,
      "grad_norm": 8.903271675109863,
      "learning_rate": 1.9220000000000002e-05,
      "loss": 2.3193,
      "step": 1950
    },
    {
      "epoch": 0.4,
      "grad_norm": 9.88573169708252,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 2.326,
      "step": 2000
    },
    {
      "epoch": 0.41,
      "grad_norm": 11.043472290039062,
      "learning_rate": 1.918e-05,
      "loss": 2.3208,
      "step": 2050
    },
    {
      "epoch": 0.42,
      "grad_norm": 13.510946273803711,
      "learning_rate": 1.916e-05,
      "loss": 2.3239,
      "step": 2100
    },
    {
      "epoch": 0.43,
      "grad_norm": 14.747854232788086,
      "learning_rate": 1.914e-05,
      "loss": 2.3087,
      "step": 2150
    },
    {
      "epoch": 0.44,
      "grad_norm": 10.008430480957031,
      "learning_rate": 1.912e-05,
      "loss": 2.3128,
      "step": 2200
    },
    {
      "epoch": 0.45,
      "grad_norm": 14.800253868103027,
      "learning_rate": 1.91e-05,
      "loss": 2.3215,
      "step": 2250
    },
    {
      "epoch": 0.46,
      "grad_norm": 8.392619132995605,
      "learning_rate": 1.908e-05,
      "loss": 2.3174,
      "step": 2300
    },
    {
      "epoch": 0.47,
      "grad_norm": 9.418739318847656,
      "learning_rate": 1.906e-05,
      "loss": 2.3099,
      "step": 2350
    },
    {
      "epoch": 0.48,
      "grad_norm": 9.136834144592285,
      "learning_rate": 1.904e-05,
      "loss": 2.3334,
      "step": 2400
    },
    {
      "epoch": 0.49,
      "grad_norm": 11.362898826599121,
      "learning_rate": 1.902e-05,
      "loss": 2.316,
      "step": 2450
    },
    {
      "epoch": 0.5,
      "grad_norm": 8.661493301391602,
      "learning_rate": 1.9e-05,
      "loss": 2.3097,
      "step": 2500
    },
    {
      "epoch": 0.51,
      "grad_norm": 7.960152626037598,
      "learning_rate": 1.898e-05,
      "loss": 2.3271,
      "step": 2550
    },
    {
      "epoch": 0.52,
      "grad_norm": 12.086928367614746,
      "learning_rate": 1.896e-05,
      "loss": 2.308,
      "step": 2600
    },
    {
      "epoch": 0.53,
      "grad_norm": 10.56352710723877,
      "learning_rate": 1.894e-05,
      "loss": 2.2963,
      "step": 2650
    },
    {
      "epoch": 0.54,
      "grad_norm": 15.181665420532227,
      "learning_rate": 1.8920000000000002e-05,
      "loss": 2.3279,
      "step": 2700
    },
    {
      "epoch": 0.55,
      "grad_norm": 11.225553512573242,
      "learning_rate": 1.8900000000000002e-05,
      "loss": 2.3287,
      "step": 2750
    },
    {
      "epoch": 0.56,
      "grad_norm": 8.99458122253418,
      "learning_rate": 1.8880000000000002e-05,
      "loss": 2.3059,
      "step": 2800
    },
    {
      "epoch": 0.57,
      "grad_norm": 8.360197067260742,
      "learning_rate": 1.886e-05,
      "loss": 2.324,
      "step": 2850
    },
    {
      "epoch": 0.58,
      "grad_norm": 8.176400184631348,
      "learning_rate": 1.884e-05,
      "loss": 2.3054,
      "step": 2900
    },
    {
      "epoch": 0.59,
      "grad_norm": 9.855246543884277,
      "learning_rate": 1.882e-05,
      "loss": 2.3094,
      "step": 2950
    },
    {
      "epoch": 0.6,
      "grad_norm": 8.629568099975586,
      "learning_rate": 1.88e-05,
      "loss": 2.322,
      "step": 3000
    },
    {
      "epoch": 0.61,
      "grad_norm": 6.261651039123535,
      "learning_rate": 1.878e-05,
      "loss": 2.3263,
      "step": 3050
    },
    {
      "epoch": 0.62,
      "grad_norm": 9.921367645263672,
      "learning_rate": 1.876e-05,
      "loss": 2.3246,
      "step": 3100
    },
    {
      "epoch": 0.63,
      "grad_norm": 10.512733459472656,
      "learning_rate": 1.8740000000000004e-05,
      "loss": 2.3344,
      "step": 3150
    },
    {
      "epoch": 0.64,
      "grad_norm": 13.285110473632812,
      "learning_rate": 1.8720000000000004e-05,
      "loss": 2.3215,
      "step": 3200
    },
    {
      "epoch": 0.65,
      "grad_norm": 7.4466118812561035,
      "learning_rate": 1.8700000000000004e-05,
      "loss": 2.3159,
      "step": 3250
    },
    {
      "epoch": 0.66,
      "grad_norm": 9.474383354187012,
      "learning_rate": 1.8680000000000004e-05,
      "loss": 2.3157,
      "step": 3300
    },
    {
      "epoch": 0.67,
      "grad_norm": 10.68678092956543,
      "learning_rate": 1.866e-05,
      "loss": 2.3104,
      "step": 3350
    },
    {
      "epoch": 0.68,
      "grad_norm": 10.165528297424316,
      "learning_rate": 1.864e-05,
      "loss": 2.3329,
      "step": 3400
    },
    {
      "epoch": 0.69,
      "grad_norm": 6.324038505554199,
      "learning_rate": 1.862e-05,
      "loss": 2.3258,
      "step": 3450
    },
    {
      "epoch": 0.7,
      "grad_norm": 9.206775665283203,
      "learning_rate": 1.86e-05,
      "loss": 2.3007,
      "step": 3500
    },
    {
      "epoch": 0.71,
      "grad_norm": 7.852708339691162,
      "learning_rate": 1.858e-05,
      "loss": 2.3228,
      "step": 3550
    },
    {
      "epoch": 0.72,
      "grad_norm": 9.669377326965332,
      "learning_rate": 1.8560000000000002e-05,
      "loss": 2.3151,
      "step": 3600
    },
    {
      "epoch": 0.73,
      "grad_norm": 10.16727352142334,
      "learning_rate": 1.8540000000000002e-05,
      "loss": 2.3086,
      "step": 3650
    },
    {
      "epoch": 0.74,
      "grad_norm": 11.445466995239258,
      "learning_rate": 1.8520000000000002e-05,
      "loss": 2.3355,
      "step": 3700
    },
    {
      "epoch": 0.75,
      "grad_norm": 11.221419334411621,
      "learning_rate": 1.8500000000000002e-05,
      "loss": 2.3151,
      "step": 3750
    },
    {
      "epoch": 0.76,
      "grad_norm": 6.535282135009766,
      "learning_rate": 1.8480000000000003e-05,
      "loss": 2.3194,
      "step": 3800
    },
    {
      "epoch": 0.77,
      "grad_norm": 5.986711502075195,
      "learning_rate": 1.8460000000000003e-05,
      "loss": 2.3227,
      "step": 3850
    },
    {
      "epoch": 0.78,
      "grad_norm": 9.166412353515625,
      "learning_rate": 1.8440000000000003e-05,
      "loss": 2.3128,
      "step": 3900
    },
    {
      "epoch": 0.79,
      "grad_norm": 8.470988273620605,
      "learning_rate": 1.8420000000000003e-05,
      "loss": 2.3115,
      "step": 3950
    },
    {
      "epoch": 0.8,
      "grad_norm": 10.219285011291504,
      "learning_rate": 1.8400000000000003e-05,
      "loss": 2.306,
      "step": 4000
    },
    {
      "epoch": 0.81,
      "grad_norm": 9.251163482666016,
      "learning_rate": 1.8380000000000004e-05,
      "loss": 2.315,
      "step": 4050
    },
    {
      "epoch": 0.82,
      "grad_norm": 6.770888805389404,
      "learning_rate": 1.8360000000000004e-05,
      "loss": 2.3101,
      "step": 4100
    },
    {
      "epoch": 0.83,
      "grad_norm": 5.594916820526123,
      "learning_rate": 1.834e-05,
      "loss": 2.323,
      "step": 4150
    },
    {
      "epoch": 0.84,
      "grad_norm": 7.392215728759766,
      "learning_rate": 1.832e-05,
      "loss": 2.3068,
      "step": 4200
    },
    {
      "epoch": 0.85,
      "grad_norm": 5.590731143951416,
      "learning_rate": 1.83e-05,
      "loss": 2.326,
      "step": 4250
    },
    {
      "epoch": 0.86,
      "grad_norm": 5.061831951141357,
      "learning_rate": 1.828e-05,
      "loss": 2.3313,
      "step": 4300
    },
    {
      "epoch": 0.87,
      "grad_norm": 4.9249796867370605,
      "learning_rate": 1.826e-05,
      "loss": 2.327,
      "step": 4350
    },
    {
      "epoch": 0.88,
      "grad_norm": 5.29218053817749,
      "learning_rate": 1.824e-05,
      "loss": 2.3147,
      "step": 4400
    },
    {
      "epoch": 0.89,
      "grad_norm": 6.002513408660889,
      "learning_rate": 1.8220000000000002e-05,
      "loss": 2.3204,
      "step": 4450
    },
    {
      "epoch": 0.9,
      "grad_norm": 8.211124420166016,
      "learning_rate": 1.8200000000000002e-05,
      "loss": 2.3083,
      "step": 4500
    },
    {
      "epoch": 0.91,
      "grad_norm": 6.4096479415893555,
      "learning_rate": 1.8180000000000002e-05,
      "loss": 2.3113,
      "step": 4550
    },
    {
      "epoch": 0.92,
      "grad_norm": 5.323182106018066,
      "learning_rate": 1.8160000000000002e-05,
      "loss": 2.3137,
      "step": 4600
    },
    {
      "epoch": 0.93,
      "grad_norm": 5.445685863494873,
      "learning_rate": 1.8140000000000003e-05,
      "loss": 2.3183,
      "step": 4650
    },
    {
      "epoch": 0.94,
      "grad_norm": 6.645382404327393,
      "learning_rate": 1.8120000000000003e-05,
      "loss": 2.3155,
      "step": 4700
    },
    {
      "epoch": 0.95,
      "grad_norm": 5.411093235015869,
      "learning_rate": 1.8100000000000003e-05,
      "loss": 2.3093,
      "step": 4750
    },
    {
      "epoch": 0.96,
      "grad_norm": 5.59566593170166,
      "learning_rate": 1.8080000000000003e-05,
      "loss": 2.3181,
      "step": 4800
    },
    {
      "epoch": 0.97,
      "grad_norm": 4.804438591003418,
      "learning_rate": 1.8060000000000003e-05,
      "loss": 2.3102,
      "step": 4850
    },
    {
      "epoch": 0.98,
      "grad_norm": 5.247166633605957,
      "learning_rate": 1.8040000000000003e-05,
      "loss": 2.3136,
      "step": 4900
    },
    {
      "epoch": 0.99,
      "grad_norm": 6.232105731964111,
      "learning_rate": 1.802e-05,
      "loss": 2.3171,
      "step": 4950
    },
    {
      "epoch": 1.0,
      "grad_norm": 5.53768253326416,
      "learning_rate": 1.8e-05,
      "loss": 2.3126,
      "step": 5000
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.310702085494995,
      "eval_runtime": 64.8727,
      "eval_samples_per_second": 154.148,
      "eval_steps_per_second": 19.269,
      "step": 5000
    },
    {
      "epoch": 1.01,
      "grad_norm": 6.028878211975098,
      "learning_rate": 1.798e-05,
      "loss": 2.3181,
      "step": 5050
    },
    {
      "epoch": 1.02,
      "grad_norm": 5.792974472045898,
      "learning_rate": 1.796e-05,
      "loss": 2.3135,
      "step": 5100
    },
    {
      "epoch": 1.03,
      "grad_norm": 6.692221164703369,
      "learning_rate": 1.794e-05,
      "loss": 2.3046,
      "step": 5150
    },
    {
      "epoch": 1.04,
      "grad_norm": 7.3547682762146,
      "learning_rate": 1.792e-05,
      "loss": 2.3192,
      "step": 5200
    },
    {
      "epoch": 1.05,
      "grad_norm": 8.036142349243164,
      "learning_rate": 1.79e-05,
      "loss": 2.3056,
      "step": 5250
    },
    {
      "epoch": 1.06,
      "grad_norm": 5.976397514343262,
      "learning_rate": 1.788e-05,
      "loss": 2.3252,
      "step": 5300
    },
    {
      "epoch": 1.07,
      "grad_norm": 7.806346416473389,
      "learning_rate": 1.7860000000000002e-05,
      "loss": 2.3007,
      "step": 5350
    },
    {
      "epoch": 1.08,
      "grad_norm": 5.3514533042907715,
      "learning_rate": 1.7840000000000002e-05,
      "loss": 2.311,
      "step": 5400
    },
    {
      "epoch": 1.09,
      "grad_norm": 4.838935375213623,
      "learning_rate": 1.7820000000000002e-05,
      "loss": 2.3108,
      "step": 5450
    },
    {
      "epoch": 1.1,
      "grad_norm": 5.012179374694824,
      "learning_rate": 1.7800000000000002e-05,
      "loss": 2.3142,
      "step": 5500
    },
    {
      "epoch": 1.11,
      "grad_norm": 5.582799434661865,
      "learning_rate": 1.7780000000000003e-05,
      "loss": 2.3132,
      "step": 5550
    },
    {
      "epoch": 1.12,
      "grad_norm": 4.960293769836426,
      "learning_rate": 1.7760000000000003e-05,
      "loss": 2.3124,
      "step": 5600
    },
    {
      "epoch": 1.13,
      "grad_norm": 5.920013904571533,
      "learning_rate": 1.7740000000000003e-05,
      "loss": 2.3131,
      "step": 5650
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 3.6646342277526855,
      "learning_rate": 1.7720000000000003e-05,
      "loss": 2.3116,
      "step": 5700
    },
    {
      "epoch": 1.15,
      "grad_norm": 6.484598636627197,
      "learning_rate": 1.77e-05,
      "loss": 2.307,
      "step": 5750
    },
    {
      "epoch": 1.16,
      "grad_norm": 6.540764808654785,
      "learning_rate": 1.768e-05,
      "loss": 2.3083,
      "step": 5800
    },
    {
      "epoch": 1.17,
      "grad_norm": 6.659757137298584,
      "learning_rate": 1.766e-05,
      "loss": 2.3097,
      "step": 5850
    },
    {
      "epoch": 1.18,
      "grad_norm": 4.790577411651611,
      "learning_rate": 1.764e-05,
      "loss": 2.3095,
      "step": 5900
    },
    {
      "epoch": 1.19,
      "grad_norm": 4.686943531036377,
      "learning_rate": 1.762e-05,
      "loss": 2.3197,
      "step": 5950
    },
    {
      "epoch": 1.2,
      "grad_norm": 4.200990200042725,
      "learning_rate": 1.76e-05,
      "loss": 2.3046,
      "step": 6000
    },
    {
      "epoch": 1.21,
      "grad_norm": 6.180019378662109,
      "learning_rate": 1.758e-05,
      "loss": 2.3233,
      "step": 6050
    },
    {
      "epoch": 1.22,
      "grad_norm": 7.037835597991943,
      "learning_rate": 1.756e-05,
      "loss": 2.3151,
      "step": 6100
    },
    {
      "epoch": 1.23,
      "grad_norm": 5.64480447769165,
      "learning_rate": 1.754e-05,
      "loss": 2.3171,
      "step": 6150
    },
    {
      "epoch": 1.24,
      "grad_norm": 8.175661087036133,
      "learning_rate": 1.752e-05,
      "loss": 2.311,
      "step": 6200
    },
    {
      "epoch": 1.25,
      "grad_norm": 8.660012245178223,
      "learning_rate": 1.7500000000000002e-05,
      "loss": 2.3052,
      "step": 6250
    },
    {
      "epoch": 1.26,
      "grad_norm": 5.315845489501953,
      "learning_rate": 1.7480000000000002e-05,
      "loss": 2.3325,
      "step": 6300
    },
    {
      "epoch": 1.27,
      "grad_norm": 8.086030006408691,
      "learning_rate": 1.7460000000000002e-05,
      "loss": 2.3076,
      "step": 6350
    },
    {
      "epoch": 1.28,
      "grad_norm": 7.876940727233887,
      "learning_rate": 1.7440000000000002e-05,
      "loss": 2.3174,
      "step": 6400
    },
    {
      "epoch": 1.29,
      "grad_norm": 9.061759948730469,
      "learning_rate": 1.7420000000000003e-05,
      "loss": 2.3178,
      "step": 6450
    },
    {
      "epoch": 1.3,
      "grad_norm": 9.268248558044434,
      "learning_rate": 1.7400000000000003e-05,
      "loss": 2.3111,
      "step": 6500
    },
    {
      "epoch": 1.31,
      "grad_norm": 5.535234451293945,
      "learning_rate": 1.7380000000000003e-05,
      "loss": 2.3073,
      "step": 6550
    },
    {
      "epoch": 1.32,
      "grad_norm": 4.651727199554443,
      "learning_rate": 1.736e-05,
      "loss": 2.3107,
      "step": 6600
    },
    {
      "epoch": 1.33,
      "grad_norm": 4.36002779006958,
      "learning_rate": 1.734e-05,
      "loss": 2.3155,
      "step": 6650
    },
    {
      "epoch": 1.34,
      "grad_norm": 4.653507709503174,
      "learning_rate": 1.732e-05,
      "loss": 2.3124,
      "step": 6700
    },
    {
      "epoch": 1.35,
      "grad_norm": 5.896796703338623,
      "learning_rate": 1.73e-05,
      "loss": 2.3163,
      "step": 6750
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 4.974400043487549,
      "learning_rate": 1.728e-05,
      "loss": 2.3062,
      "step": 6800
    },
    {
      "epoch": 1.37,
      "grad_norm": 5.26999044418335,
      "learning_rate": 1.726e-05,
      "loss": 2.3127,
      "step": 6850
    },
    {
      "epoch": 1.38,
      "grad_norm": 5.759142875671387,
      "learning_rate": 1.724e-05,
      "loss": 2.3029,
      "step": 6900
    },
    {
      "epoch": 1.3900000000000001,
      "grad_norm": 3.6459169387817383,
      "learning_rate": 1.722e-05,
      "loss": 2.3058,
      "step": 6950
    },
    {
      "epoch": 1.4,
      "grad_norm": 5.068429946899414,
      "learning_rate": 1.72e-05,
      "loss": 2.3188,
      "step": 7000
    },
    {
      "epoch": 1.41,
      "grad_norm": 3.3782472610473633,
      "learning_rate": 1.718e-05,
      "loss": 2.3142,
      "step": 7050
    },
    {
      "epoch": 1.42,
      "grad_norm": 4.531368732452393,
      "learning_rate": 1.7160000000000002e-05,
      "loss": 2.3046,
      "step": 7100
    },
    {
      "epoch": 1.43,
      "grad_norm": 3.916994094848633,
      "learning_rate": 1.7140000000000002e-05,
      "loss": 2.3147,
      "step": 7150
    },
    {
      "epoch": 1.44,
      "grad_norm": 5.143563270568848,
      "learning_rate": 1.7120000000000002e-05,
      "loss": 2.3073,
      "step": 7200
    },
    {
      "epoch": 1.45,
      "grad_norm": 3.9445550441741943,
      "learning_rate": 1.7100000000000002e-05,
      "loss": 2.3104,
      "step": 7250
    },
    {
      "epoch": 1.46,
      "grad_norm": 4.125280857086182,
      "learning_rate": 1.7080000000000002e-05,
      "loss": 2.3053,
      "step": 7300
    },
    {
      "epoch": 1.47,
      "grad_norm": 4.985342979431152,
      "learning_rate": 1.7060000000000003e-05,
      "loss": 2.3125,
      "step": 7350
    },
    {
      "epoch": 1.48,
      "grad_norm": 4.524167537689209,
      "learning_rate": 1.704e-05,
      "loss": 2.3163,
      "step": 7400
    },
    {
      "epoch": 1.49,
      "grad_norm": 4.5826239585876465,
      "learning_rate": 1.702e-05,
      "loss": 2.3091,
      "step": 7450
    },
    {
      "epoch": 1.5,
      "grad_norm": 4.576885223388672,
      "learning_rate": 1.7e-05,
      "loss": 2.3043,
      "step": 7500
    },
    {
      "epoch": 1.51,
      "grad_norm": 4.996545791625977,
      "learning_rate": 1.698e-05,
      "loss": 2.3058,
      "step": 7550
    },
    {
      "epoch": 1.52,
      "grad_norm": 5.7647600173950195,
      "learning_rate": 1.696e-05,
      "loss": 2.3154,
      "step": 7600
    },
    {
      "epoch": 1.53,
      "grad_norm": 5.069006443023682,
      "learning_rate": 1.694e-05,
      "loss": 2.3127,
      "step": 7650
    },
    {
      "epoch": 1.54,
      "grad_norm": 5.627108097076416,
      "learning_rate": 1.692e-05,
      "loss": 2.303,
      "step": 7700
    },
    {
      "epoch": 1.55,
      "grad_norm": 6.677248001098633,
      "learning_rate": 1.69e-05,
      "loss": 2.3002,
      "step": 7750
    },
    {
      "epoch": 1.56,
      "grad_norm": 3.887768268585205,
      "learning_rate": 1.688e-05,
      "loss": 2.3152,
      "step": 7800
    },
    {
      "epoch": 1.5699999999999998,
      "grad_norm": 3.9421706199645996,
      "learning_rate": 1.686e-05,
      "loss": 2.3127,
      "step": 7850
    },
    {
      "epoch": 1.58,
      "grad_norm": 6.665843486785889,
      "learning_rate": 1.684e-05,
      "loss": 2.3074,
      "step": 7900
    },
    {
      "epoch": 1.5899999999999999,
      "grad_norm": 4.134314060211182,
      "learning_rate": 1.682e-05,
      "loss": 2.3126,
      "step": 7950
    },
    {
      "epoch": 1.6,
      "grad_norm": 3.595874071121216,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 2.3133,
      "step": 8000
    },
    {
      "epoch": 1.6099999999999999,
      "grad_norm": 6.913798809051514,
      "learning_rate": 1.6780000000000002e-05,
      "loss": 2.306,
      "step": 8050
    },
    {
      "epoch": 1.62,
      "grad_norm": 5.354004383087158,
      "learning_rate": 1.6760000000000002e-05,
      "loss": 2.3092,
      "step": 8100
    },
    {
      "epoch": 1.63,
      "grad_norm": 5.480829238891602,
      "learning_rate": 1.6740000000000002e-05,
      "loss": 2.3188,
      "step": 8150
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 4.864090919494629,
      "learning_rate": 1.672e-05,
      "loss": 2.3124,
      "step": 8200
    },
    {
      "epoch": 1.65,
      "grad_norm": 4.787199020385742,
      "learning_rate": 1.67e-05,
      "loss": 2.3053,
      "step": 8250
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 4.4177069664001465,
      "learning_rate": 1.668e-05,
      "loss": 2.3142,
      "step": 8300
    },
    {
      "epoch": 1.67,
      "grad_norm": 3.4972310066223145,
      "learning_rate": 1.666e-05,
      "loss": 2.3112,
      "step": 8350
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 3.415234327316284,
      "learning_rate": 1.664e-05,
      "loss": 2.2986,
      "step": 8400
    },
    {
      "epoch": 1.69,
      "grad_norm": 5.758885383605957,
      "learning_rate": 1.662e-05,
      "loss": 2.3214,
      "step": 8450
    },
    {
      "epoch": 1.7,
      "grad_norm": 3.8563232421875,
      "learning_rate": 1.66e-05,
      "loss": 2.3013,
      "step": 8500
    },
    {
      "epoch": 1.71,
      "grad_norm": 4.324464797973633,
      "learning_rate": 1.658e-05,
      "loss": 2.316,
      "step": 8550
    },
    {
      "epoch": 1.72,
      "grad_norm": 3.387227773666382,
      "learning_rate": 1.656e-05,
      "loss": 2.3108,
      "step": 8600
    },
    {
      "epoch": 1.73,
      "grad_norm": 4.130061149597168,
      "learning_rate": 1.654e-05,
      "loss": 2.3091,
      "step": 8650
    },
    {
      "epoch": 1.74,
      "grad_norm": 3.028904676437378,
      "learning_rate": 1.652e-05,
      "loss": 2.3108,
      "step": 8700
    },
    {
      "epoch": 1.75,
      "grad_norm": 3.9699511528015137,
      "learning_rate": 1.65e-05,
      "loss": 2.3099,
      "step": 8750
    },
    {
      "epoch": 1.76,
      "grad_norm": 3.5045247077941895,
      "learning_rate": 1.648e-05,
      "loss": 2.3049,
      "step": 8800
    },
    {
      "epoch": 1.77,
      "grad_norm": 3.056018829345703,
      "learning_rate": 1.646e-05,
      "loss": 2.2953,
      "step": 8850
    },
    {
      "epoch": 1.78,
      "grad_norm": 4.373067855834961,
      "learning_rate": 1.6440000000000002e-05,
      "loss": 2.3115,
      "step": 8900
    },
    {
      "epoch": 1.79,
      "grad_norm": 3.7469584941864014,
      "learning_rate": 1.6420000000000002e-05,
      "loss": 2.3142,
      "step": 8950
    },
    {
      "epoch": 1.8,
      "grad_norm": 3.279076337814331,
      "learning_rate": 1.64e-05,
      "loss": 2.304,
      "step": 9000
    },
    {
      "epoch": 1.81,
      "grad_norm": 4.667911052703857,
      "learning_rate": 1.638e-05,
      "loss": 2.3054,
      "step": 9050
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 3.991116762161255,
      "learning_rate": 1.636e-05,
      "loss": 2.3073,
      "step": 9100
    },
    {
      "epoch": 1.83,
      "grad_norm": 4.821560859680176,
      "learning_rate": 1.634e-05,
      "loss": 2.3035,
      "step": 9150
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 3.1430280208587646,
      "learning_rate": 1.632e-05,
      "loss": 2.3035,
      "step": 9200
    },
    {
      "epoch": 1.85,
      "grad_norm": 3.152808666229248,
      "learning_rate": 1.63e-05,
      "loss": 2.315,
      "step": 9250
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 4.123400688171387,
      "learning_rate": 1.628e-05,
      "loss": 2.3049,
      "step": 9300
    },
    {
      "epoch": 1.87,
      "grad_norm": 3.377275228500366,
      "learning_rate": 1.626e-05,
      "loss": 2.322,
      "step": 9350
    },
    {
      "epoch": 1.88,
      "grad_norm": 4.559067249298096,
      "learning_rate": 1.6240000000000004e-05,
      "loss": 2.3047,
      "step": 9400
    },
    {
      "epoch": 1.8900000000000001,
      "grad_norm": 2.9303340911865234,
      "learning_rate": 1.6220000000000004e-05,
      "loss": 2.3062,
      "step": 9450
    },
    {
      "epoch": 1.9,
      "grad_norm": 5.951401233673096,
      "learning_rate": 1.62e-05,
      "loss": 2.3009,
      "step": 9500
    },
    {
      "epoch": 1.9100000000000001,
      "grad_norm": 5.913340091705322,
      "learning_rate": 1.618e-05,
      "loss": 2.309,
      "step": 9550
    },
    {
      "epoch": 1.92,
      "grad_norm": 4.982865333557129,
      "learning_rate": 1.616e-05,
      "loss": 2.3112,
      "step": 9600
    },
    {
      "epoch": 1.9300000000000002,
      "grad_norm": 6.089526176452637,
      "learning_rate": 1.614e-05,
      "loss": 2.3018,
      "step": 9650
    },
    {
      "epoch": 1.94,
      "grad_norm": 3.568571090698242,
      "learning_rate": 1.612e-05,
      "loss": 2.3121,
      "step": 9700
    },
    {
      "epoch": 1.95,
      "grad_norm": 3.9667110443115234,
      "learning_rate": 1.6100000000000002e-05,
      "loss": 2.3033,
      "step": 9750
    },
    {
      "epoch": 1.96,
      "grad_norm": 4.30645227432251,
      "learning_rate": 1.6080000000000002e-05,
      "loss": 2.3016,
      "step": 9800
    },
    {
      "epoch": 1.97,
      "grad_norm": 3.181745767593384,
      "learning_rate": 1.6060000000000002e-05,
      "loss": 2.3061,
      "step": 9850
    },
    {
      "epoch": 1.98,
      "grad_norm": 4.050427436828613,
      "learning_rate": 1.6040000000000002e-05,
      "loss": 2.3129,
      "step": 9900
    },
    {
      "epoch": 1.99,
      "grad_norm": 3.743104934692383,
      "learning_rate": 1.6020000000000002e-05,
      "loss": 2.3091,
      "step": 9950
    },
    {
      "epoch": 2.0,
      "grad_norm": 3.859184741973877,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 2.3055,
      "step": 10000
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.308318614959717,
      "eval_runtime": 72.2132,
      "eval_samples_per_second": 138.479,
      "eval_steps_per_second": 17.31,
      "step": 10000
    },
    {
      "epoch": 2.01,
      "grad_norm": 3.732830762863159,
      "learning_rate": 1.5980000000000003e-05,
      "loss": 2.3025,
      "step": 10050
    },
    {
      "epoch": 2.02,
      "grad_norm": 3.256619453430176,
      "learning_rate": 1.5960000000000003e-05,
      "loss": 2.3162,
      "step": 10100
    },
    {
      "epoch": 2.03,
      "grad_norm": 3.1862199306488037,
      "learning_rate": 1.5940000000000003e-05,
      "loss": 2.3181,
      "step": 10150
    },
    {
      "epoch": 2.04,
      "grad_norm": 3.585097551345825,
      "learning_rate": 1.5920000000000003e-05,
      "loss": 2.3088,
      "step": 10200
    },
    {
      "epoch": 2.05,
      "grad_norm": 4.080259323120117,
      "learning_rate": 1.5900000000000004e-05,
      "loss": 2.3058,
      "step": 10250
    },
    {
      "epoch": 2.06,
      "grad_norm": 4.132308006286621,
      "learning_rate": 1.588e-05,
      "loss": 2.304,
      "step": 10300
    },
    {
      "epoch": 2.07,
      "grad_norm": 3.743607997894287,
      "learning_rate": 1.586e-05,
      "loss": 2.3176,
      "step": 10350
    },
    {
      "epoch": 2.08,
      "grad_norm": 4.319986820220947,
      "learning_rate": 1.584e-05,
      "loss": 2.3079,
      "step": 10400
    },
    {
      "epoch": 2.09,
      "grad_norm": 2.9749534130096436,
      "learning_rate": 1.582e-05,
      "loss": 2.3164,
      "step": 10450
    },
    {
      "epoch": 2.1,
      "grad_norm": 3.565579652786255,
      "learning_rate": 1.58e-05,
      "loss": 2.3064,
      "step": 10500
    },
    {
      "epoch": 2.11,
      "grad_norm": 2.805232048034668,
      "learning_rate": 1.578e-05,
      "loss": 2.3124,
      "step": 10550
    },
    {
      "epoch": 2.12,
      "grad_norm": 3.415048599243164,
      "learning_rate": 1.576e-05,
      "loss": 2.3097,
      "step": 10600
    },
    {
      "epoch": 2.13,
      "grad_norm": 2.904900550842285,
      "learning_rate": 1.5740000000000002e-05,
      "loss": 2.3108,
      "step": 10650
    },
    {
      "epoch": 2.14,
      "grad_norm": 3.5956616401672363,
      "learning_rate": 1.5720000000000002e-05,
      "loss": 2.3022,
      "step": 10700
    },
    {
      "epoch": 2.15,
      "grad_norm": 2.6990110874176025,
      "learning_rate": 1.5700000000000002e-05,
      "loss": 2.3076,
      "step": 10750
    },
    {
      "epoch": 2.16,
      "grad_norm": 3.4828696250915527,
      "learning_rate": 1.5680000000000002e-05,
      "loss": 2.3131,
      "step": 10800
    },
    {
      "epoch": 2.17,
      "grad_norm": 3.941654682159424,
      "learning_rate": 1.5660000000000003e-05,
      "loss": 2.3019,
      "step": 10850
    },
    {
      "epoch": 2.18,
      "grad_norm": 2.979109764099121,
      "learning_rate": 1.5640000000000003e-05,
      "loss": 2.3059,
      "step": 10900
    },
    {
      "epoch": 2.19,
      "grad_norm": 3.3778417110443115,
      "learning_rate": 1.5620000000000003e-05,
      "loss": 2.3036,
      "step": 10950
    },
    {
      "epoch": 2.2,
      "grad_norm": 3.9732325077056885,
      "learning_rate": 1.5600000000000003e-05,
      "loss": 2.2983,
      "step": 11000
    },
    {
      "epoch": 2.21,
      "grad_norm": 3.5947468280792236,
      "learning_rate": 1.5580000000000003e-05,
      "loss": 2.3099,
      "step": 11050
    },
    {
      "epoch": 2.22,
      "grad_norm": 3.825817823410034,
      "learning_rate": 1.556e-05,
      "loss": 2.3103,
      "step": 11100
    },
    {
      "epoch": 2.23,
      "grad_norm": 3.7435054779052734,
      "learning_rate": 1.554e-05,
      "loss": 2.3078,
      "step": 11150
    },
    {
      "epoch": 2.24,
      "grad_norm": 3.1727333068847656,
      "learning_rate": 1.552e-05,
      "loss": 2.303,
      "step": 11200
    },
    {
      "epoch": 2.25,
      "grad_norm": 3.4667391777038574,
      "learning_rate": 1.55e-05,
      "loss": 2.3022,
      "step": 11250
    },
    {
      "epoch": 2.26,
      "grad_norm": 3.2346951961517334,
      "learning_rate": 1.548e-05,
      "loss": 2.3166,
      "step": 11300
    },
    {
      "epoch": 2.27,
      "grad_norm": 5.619704246520996,
      "learning_rate": 1.546e-05,
      "loss": 2.3104,
      "step": 11350
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 2.913992166519165,
      "learning_rate": 1.544e-05,
      "loss": 2.3018,
      "step": 11400
    },
    {
      "epoch": 2.29,
      "grad_norm": 4.6048760414123535,
      "learning_rate": 1.542e-05,
      "loss": 2.3157,
      "step": 11450
    },
    {
      "epoch": 2.3,
      "grad_norm": 4.351783752441406,
      "learning_rate": 1.54e-05,
      "loss": 2.3089,
      "step": 11500
    },
    {
      "epoch": 2.31,
      "grad_norm": 3.867129325866699,
      "learning_rate": 1.5380000000000002e-05,
      "loss": 2.3048,
      "step": 11550
    },
    {
      "epoch": 2.32,
      "grad_norm": 2.719106912612915,
      "learning_rate": 1.5360000000000002e-05,
      "loss": 2.3079,
      "step": 11600
    },
    {
      "epoch": 2.33,
      "grad_norm": 2.7233965396881104,
      "learning_rate": 1.5340000000000002e-05,
      "loss": 2.3039,
      "step": 11650
    },
    {
      "epoch": 2.34,
      "grad_norm": 3.708552122116089,
      "learning_rate": 1.5320000000000002e-05,
      "loss": 2.3012,
      "step": 11700
    },
    {
      "epoch": 2.35,
      "grad_norm": 2.773080587387085,
      "learning_rate": 1.5300000000000003e-05,
      "loss": 2.3058,
      "step": 11750
    },
    {
      "epoch": 2.36,
      "grad_norm": 2.9316444396972656,
      "learning_rate": 1.5280000000000003e-05,
      "loss": 2.312,
      "step": 11800
    },
    {
      "epoch": 2.37,
      "grad_norm": 3.0724332332611084,
      "learning_rate": 1.5260000000000003e-05,
      "loss": 2.3033,
      "step": 11850
    },
    {
      "epoch": 2.38,
      "grad_norm": 3.6343746185302734,
      "learning_rate": 1.5240000000000001e-05,
      "loss": 2.318,
      "step": 11900
    },
    {
      "epoch": 2.39,
      "grad_norm": 2.7692625522613525,
      "learning_rate": 1.5220000000000002e-05,
      "loss": 2.3042,
      "step": 11950
    },
    {
      "epoch": 2.4,
      "grad_norm": 4.3999128341674805,
      "learning_rate": 1.5200000000000002e-05,
      "loss": 2.3085,
      "step": 12000
    },
    {
      "epoch": 2.41,
      "grad_norm": 3.6880948543548584,
      "learning_rate": 1.5180000000000002e-05,
      "loss": 2.3019,
      "step": 12050
    },
    {
      "epoch": 2.42,
      "grad_norm": 3.0880253314971924,
      "learning_rate": 1.516e-05,
      "loss": 2.3172,
      "step": 12100
    },
    {
      "epoch": 2.43,
      "grad_norm": 3.6231324672698975,
      "learning_rate": 1.514e-05,
      "loss": 2.3022,
      "step": 12150
    },
    {
      "epoch": 2.44,
      "grad_norm": 2.9248688220977783,
      "learning_rate": 1.5120000000000001e-05,
      "loss": 2.3121,
      "step": 12200
    },
    {
      "epoch": 2.45,
      "grad_norm": 2.8870835304260254,
      "learning_rate": 1.5100000000000001e-05,
      "loss": 2.3111,
      "step": 12250
    },
    {
      "epoch": 2.46,
      "grad_norm": 3.042067527770996,
      "learning_rate": 1.5080000000000001e-05,
      "loss": 2.3031,
      "step": 12300
    },
    {
      "epoch": 2.4699999999999998,
      "grad_norm": 3.699312210083008,
      "learning_rate": 1.5060000000000001e-05,
      "loss": 2.3152,
      "step": 12350
    },
    {
      "epoch": 2.48,
      "grad_norm": 5.190868377685547,
      "learning_rate": 1.5040000000000002e-05,
      "loss": 2.3081,
      "step": 12400
    },
    {
      "epoch": 2.49,
      "grad_norm": 3.0931396484375,
      "learning_rate": 1.5020000000000002e-05,
      "loss": 2.3103,
      "step": 12450
    },
    {
      "epoch": 2.5,
      "grad_norm": 4.171586036682129,
      "learning_rate": 1.5000000000000002e-05,
      "loss": 2.3056,
      "step": 12500
    },
    {
      "epoch": 2.51,
      "grad_norm": 5.047077178955078,
      "learning_rate": 1.498e-05,
      "loss": 2.3044,
      "step": 12550
    },
    {
      "epoch": 2.52,
      "grad_norm": 4.485304832458496,
      "learning_rate": 1.496e-05,
      "loss": 2.3057,
      "step": 12600
    },
    {
      "epoch": 2.5300000000000002,
      "grad_norm": 3.783811569213867,
      "learning_rate": 1.4940000000000001e-05,
      "loss": 2.3068,
      "step": 12650
    },
    {
      "epoch": 2.54,
      "grad_norm": 2.9442059993743896,
      "learning_rate": 1.4920000000000001e-05,
      "loss": 2.2998,
      "step": 12700
    },
    {
      "epoch": 2.55,
      "grad_norm": 3.0852324962615967,
      "learning_rate": 1.4900000000000001e-05,
      "loss": 2.3102,
      "step": 12750
    },
    {
      "epoch": 2.56,
      "grad_norm": 3.90032696723938,
      "learning_rate": 1.4880000000000002e-05,
      "loss": 2.3067,
      "step": 12800
    },
    {
      "epoch": 2.57,
      "grad_norm": 2.693565845489502,
      "learning_rate": 1.4860000000000002e-05,
      "loss": 2.3039,
      "step": 12850
    },
    {
      "epoch": 2.58,
      "grad_norm": 3.5675225257873535,
      "learning_rate": 1.4840000000000002e-05,
      "loss": 2.3162,
      "step": 12900
    },
    {
      "epoch": 2.59,
      "grad_norm": 3.855067014694214,
      "learning_rate": 1.482e-05,
      "loss": 2.3097,
      "step": 12950
    },
    {
      "epoch": 2.6,
      "grad_norm": 2.658889055252075,
      "learning_rate": 1.48e-05,
      "loss": 2.3159,
      "step": 13000
    },
    {
      "epoch": 2.61,
      "grad_norm": 3.8396799564361572,
      "learning_rate": 1.478e-05,
      "loss": 2.3056,
      "step": 13050
    },
    {
      "epoch": 2.62,
      "grad_norm": 3.602550506591797,
      "learning_rate": 1.4760000000000001e-05,
      "loss": 2.3008,
      "step": 13100
    },
    {
      "epoch": 2.63,
      "grad_norm": 3.13809871673584,
      "learning_rate": 1.4740000000000001e-05,
      "loss": 2.3098,
      "step": 13150
    },
    {
      "epoch": 2.64,
      "grad_norm": 4.111541271209717,
      "learning_rate": 1.4720000000000001e-05,
      "loss": 2.2977,
      "step": 13200
    },
    {
      "epoch": 2.65,
      "grad_norm": 3.49513840675354,
      "learning_rate": 1.4700000000000002e-05,
      "loss": 2.3058,
      "step": 13250
    },
    {
      "epoch": 2.66,
      "grad_norm": 3.305954933166504,
      "learning_rate": 1.4680000000000002e-05,
      "loss": 2.3118,
      "step": 13300
    },
    {
      "epoch": 2.67,
      "grad_norm": 5.552613735198975,
      "learning_rate": 1.466e-05,
      "loss": 2.3004,
      "step": 13350
    },
    {
      "epoch": 2.68,
      "grad_norm": 3.931225538253784,
      "learning_rate": 1.464e-05,
      "loss": 2.3085,
      "step": 13400
    },
    {
      "epoch": 2.69,
      "grad_norm": 3.122083902359009,
      "learning_rate": 1.462e-05,
      "loss": 2.3075,
      "step": 13450
    },
    {
      "epoch": 2.7,
      "grad_norm": 2.972818613052368,
      "learning_rate": 1.46e-05,
      "loss": 2.2986,
      "step": 13500
    },
    {
      "epoch": 2.71,
      "grad_norm": 3.6672205924987793,
      "learning_rate": 1.4580000000000001e-05,
      "loss": 2.312,
      "step": 13550
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 3.7159721851348877,
      "learning_rate": 1.4560000000000001e-05,
      "loss": 2.3058,
      "step": 13600
    },
    {
      "epoch": 2.73,
      "grad_norm": 2.871330499649048,
      "learning_rate": 1.4540000000000001e-05,
      "loss": 2.3119,
      "step": 13650
    },
    {
      "epoch": 2.74,
      "grad_norm": 2.637660264968872,
      "learning_rate": 1.4520000000000002e-05,
      "loss": 2.308,
      "step": 13700
    },
    {
      "epoch": 2.75,
      "grad_norm": 3.606570243835449,
      "learning_rate": 1.45e-05,
      "loss": 2.3047,
      "step": 13750
    },
    {
      "epoch": 2.76,
      "grad_norm": 3.179940700531006,
      "learning_rate": 1.448e-05,
      "loss": 2.3134,
      "step": 13800
    },
    {
      "epoch": 2.77,
      "grad_norm": 3.240126848220825,
      "learning_rate": 1.446e-05,
      "loss": 2.309,
      "step": 13850
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 3.723541498184204,
      "learning_rate": 1.444e-05,
      "loss": 2.2957,
      "step": 13900
    },
    {
      "epoch": 2.79,
      "grad_norm": 4.540886878967285,
      "learning_rate": 1.4420000000000001e-05,
      "loss": 2.3012,
      "step": 13950
    },
    {
      "epoch": 2.8,
      "grad_norm": 3.756434679031372,
      "learning_rate": 1.4400000000000001e-05,
      "loss": 2.3132,
      "step": 14000
    },
    {
      "epoch": 2.81,
      "grad_norm": 3.620326042175293,
      "learning_rate": 1.4380000000000001e-05,
      "loss": 2.3,
      "step": 14050
    },
    {
      "epoch": 2.82,
      "grad_norm": 3.8561251163482666,
      "learning_rate": 1.4360000000000001e-05,
      "loss": 2.3071,
      "step": 14100
    },
    {
      "epoch": 2.83,
      "grad_norm": 4.281414031982422,
      "learning_rate": 1.434e-05,
      "loss": 2.3062,
      "step": 14150
    },
    {
      "epoch": 2.84,
      "grad_norm": 4.314666748046875,
      "learning_rate": 1.432e-05,
      "loss": 2.3066,
      "step": 14200
    },
    {
      "epoch": 2.85,
      "grad_norm": 4.546560764312744,
      "learning_rate": 1.43e-05,
      "loss": 2.3047,
      "step": 14250
    },
    {
      "epoch": 2.86,
      "grad_norm": 3.727492570877075,
      "learning_rate": 1.428e-05,
      "loss": 2.31,
      "step": 14300
    },
    {
      "epoch": 2.87,
      "grad_norm": 3.4926626682281494,
      "learning_rate": 1.426e-05,
      "loss": 2.3125,
      "step": 14350
    },
    {
      "epoch": 2.88,
      "grad_norm": 5.665472030639648,
      "learning_rate": 1.4240000000000001e-05,
      "loss": 2.2998,
      "step": 14400
    },
    {
      "epoch": 2.89,
      "grad_norm": 4.492315769195557,
      "learning_rate": 1.4220000000000001e-05,
      "loss": 2.3063,
      "step": 14450
    },
    {
      "epoch": 2.9,
      "grad_norm": 3.150766134262085,
      "learning_rate": 1.4200000000000001e-05,
      "loss": 2.3127,
      "step": 14500
    },
    {
      "epoch": 2.91,
      "grad_norm": 5.165378570556641,
      "learning_rate": 1.418e-05,
      "loss": 2.3092,
      "step": 14550
    },
    {
      "epoch": 2.92,
      "grad_norm": 4.600502967834473,
      "learning_rate": 1.416e-05,
      "loss": 2.2988,
      "step": 14600
    },
    {
      "epoch": 2.93,
      "grad_norm": 3.7427115440368652,
      "learning_rate": 1.414e-05,
      "loss": 2.3043,
      "step": 14650
    },
    {
      "epoch": 2.94,
      "grad_norm": 3.3843042850494385,
      "learning_rate": 1.412e-05,
      "loss": 2.3005,
      "step": 14700
    },
    {
      "epoch": 2.95,
      "grad_norm": 3.371196746826172,
      "learning_rate": 1.41e-05,
      "loss": 2.3103,
      "step": 14750
    },
    {
      "epoch": 2.96,
      "grad_norm": 3.3087053298950195,
      "learning_rate": 1.408e-05,
      "loss": 2.3152,
      "step": 14800
    },
    {
      "epoch": 2.9699999999999998,
      "grad_norm": 3.2721316814422607,
      "learning_rate": 1.4060000000000001e-05,
      "loss": 2.3102,
      "step": 14850
    },
    {
      "epoch": 2.98,
      "grad_norm": 4.1735029220581055,
      "learning_rate": 1.4040000000000001e-05,
      "loss": 2.3055,
      "step": 14900
    },
    {
      "epoch": 2.99,
      "grad_norm": 3.5431301593780518,
      "learning_rate": 1.402e-05,
      "loss": 2.3121,
      "step": 14950
    },
    {
      "epoch": 3.0,
      "grad_norm": 3.176959276199341,
      "learning_rate": 1.4e-05,
      "loss": 2.3005,
      "step": 15000
    },
    {
      "epoch": 3.0,
      "eval_loss": 2.3112215995788574,
      "eval_runtime": 76.3065,
      "eval_samples_per_second": 131.05,
      "eval_steps_per_second": 16.381,
      "step": 15000
    },
    {
      "epoch": 3.01,
      "grad_norm": 2.5361146926879883,
      "learning_rate": 1.398e-05,
      "loss": 2.3115,
      "step": 15050
    },
    {
      "epoch": 3.02,
      "grad_norm": 2.8698456287384033,
      "learning_rate": 1.396e-05,
      "loss": 2.3093,
      "step": 15100
    },
    {
      "epoch": 3.03,
      "grad_norm": 3.5496513843536377,
      "learning_rate": 1.394e-05,
      "loss": 2.3083,
      "step": 15150
    },
    {
      "epoch": 3.04,
      "grad_norm": 3.587686777114868,
      "learning_rate": 1.392e-05,
      "loss": 2.3071,
      "step": 15200
    },
    {
      "epoch": 3.05,
      "grad_norm": 3.120140314102173,
      "learning_rate": 1.39e-05,
      "loss": 2.3074,
      "step": 15250
    },
    {
      "epoch": 3.06,
      "grad_norm": 2.957615375518799,
      "learning_rate": 1.3880000000000001e-05,
      "loss": 2.3119,
      "step": 15300
    },
    {
      "epoch": 3.07,
      "grad_norm": 4.019079208374023,
      "learning_rate": 1.386e-05,
      "loss": 2.3116,
      "step": 15350
    },
    {
      "epoch": 3.08,
      "grad_norm": 4.531472682952881,
      "learning_rate": 1.384e-05,
      "loss": 2.3096,
      "step": 15400
    },
    {
      "epoch": 3.09,
      "grad_norm": 3.111880302429199,
      "learning_rate": 1.382e-05,
      "loss": 2.3043,
      "step": 15450
    },
    {
      "epoch": 3.1,
      "grad_norm": 3.715158700942993,
      "learning_rate": 1.38e-05,
      "loss": 2.3083,
      "step": 15500
    },
    {
      "epoch": 3.11,
      "grad_norm": 3.4800162315368652,
      "learning_rate": 1.378e-05,
      "loss": 2.3018,
      "step": 15550
    },
    {
      "epoch": 3.12,
      "grad_norm": 2.9092133045196533,
      "learning_rate": 1.376e-05,
      "loss": 2.3103,
      "step": 15600
    },
    {
      "epoch": 3.13,
      "grad_norm": 5.8560943603515625,
      "learning_rate": 1.3740000000000002e-05,
      "loss": 2.3031,
      "step": 15650
    },
    {
      "epoch": 3.14,
      "grad_norm": 2.6789023876190186,
      "learning_rate": 1.3720000000000002e-05,
      "loss": 2.3084,
      "step": 15700
    },
    {
      "epoch": 3.15,
      "grad_norm": 3.3706328868865967,
      "learning_rate": 1.3700000000000003e-05,
      "loss": 2.2967,
      "step": 15750
    },
    {
      "epoch": 3.16,
      "grad_norm": 3.279188632965088,
      "learning_rate": 1.3680000000000003e-05,
      "loss": 2.3108,
      "step": 15800
    },
    {
      "epoch": 3.17,
      "grad_norm": 2.390892267227173,
      "learning_rate": 1.3660000000000001e-05,
      "loss": 2.2896,
      "step": 15850
    },
    {
      "epoch": 3.18,
      "grad_norm": 6.412156105041504,
      "learning_rate": 1.3640000000000002e-05,
      "loss": 2.3079,
      "step": 15900
    },
    {
      "epoch": 3.19,
      "grad_norm": 2.9418134689331055,
      "learning_rate": 1.3620000000000002e-05,
      "loss": 2.3114,
      "step": 15950
    },
    {
      "epoch": 3.2,
      "grad_norm": 3.4472968578338623,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 2.3027,
      "step": 16000
    },
    {
      "epoch": 3.21,
      "grad_norm": 4.883509159088135,
      "learning_rate": 1.3580000000000002e-05,
      "loss": 2.3019,
      "step": 16050
    },
    {
      "epoch": 3.22,
      "grad_norm": 3.1951067447662354,
      "learning_rate": 1.3560000000000002e-05,
      "loss": 2.3131,
      "step": 16100
    },
    {
      "epoch": 3.23,
      "grad_norm": 2.646609306335449,
      "learning_rate": 1.3540000000000003e-05,
      "loss": 2.3059,
      "step": 16150
    },
    {
      "epoch": 3.24,
      "grad_norm": 3.0377821922302246,
      "learning_rate": 1.3520000000000003e-05,
      "loss": 2.3118,
      "step": 16200
    },
    {
      "epoch": 3.25,
      "grad_norm": 3.1002278327941895,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 2.312,
      "step": 16250
    },
    {
      "epoch": 3.26,
      "grad_norm": 3.1326303482055664,
      "learning_rate": 1.3480000000000001e-05,
      "loss": 2.3061,
      "step": 16300
    },
    {
      "epoch": 3.27,
      "grad_norm": 2.7257888317108154,
      "learning_rate": 1.3460000000000002e-05,
      "loss": 2.3083,
      "step": 16350
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 2.843252182006836,
      "learning_rate": 1.3440000000000002e-05,
      "loss": 2.3061,
      "step": 16400
    },
    {
      "epoch": 3.29,
      "grad_norm": 3.4829506874084473,
      "learning_rate": 1.3420000000000002e-05,
      "loss": 2.3133,
      "step": 16450
    },
    {
      "epoch": 3.3,
      "grad_norm": 3.389561414718628,
      "learning_rate": 1.3400000000000002e-05,
      "loss": 2.3079,
      "step": 16500
    },
    {
      "epoch": 3.31,
      "grad_norm": 3.862889289855957,
      "learning_rate": 1.3380000000000002e-05,
      "loss": 2.3074,
      "step": 16550
    },
    {
      "epoch": 3.32,
      "grad_norm": 3.4365620613098145,
      "learning_rate": 1.3360000000000003e-05,
      "loss": 2.3154,
      "step": 16600
    },
    {
      "epoch": 3.33,
      "grad_norm": 2.6510539054870605,
      "learning_rate": 1.3340000000000001e-05,
      "loss": 2.3119,
      "step": 16650
    },
    {
      "epoch": 3.34,
      "grad_norm": 3.0739150047302246,
      "learning_rate": 1.3320000000000001e-05,
      "loss": 2.3024,
      "step": 16700
    },
    {
      "epoch": 3.35,
      "grad_norm": 3.365506172180176,
      "learning_rate": 1.3300000000000001e-05,
      "loss": 2.3112,
      "step": 16750
    },
    {
      "epoch": 3.36,
      "grad_norm": 2.914586305618286,
      "learning_rate": 1.3280000000000002e-05,
      "loss": 2.3055,
      "step": 16800
    },
    {
      "epoch": 3.37,
      "grad_norm": 2.5783047676086426,
      "learning_rate": 1.3260000000000002e-05,
      "loss": 2.3069,
      "step": 16850
    },
    {
      "epoch": 3.38,
      "grad_norm": 3.1818525791168213,
      "learning_rate": 1.3240000000000002e-05,
      "loss": 2.3017,
      "step": 16900
    },
    {
      "epoch": 3.39,
      "grad_norm": 3.334033966064453,
      "learning_rate": 1.3220000000000002e-05,
      "loss": 2.3045,
      "step": 16950
    },
    {
      "epoch": 3.4,
      "grad_norm": 3.1316490173339844,
      "learning_rate": 1.3200000000000002e-05,
      "loss": 2.3041,
      "step": 17000
    },
    {
      "epoch": 3.41,
      "grad_norm": 3.422989845275879,
      "learning_rate": 1.3180000000000001e-05,
      "loss": 2.3154,
      "step": 17050
    },
    {
      "epoch": 3.42,
      "grad_norm": 2.5598626136779785,
      "learning_rate": 1.3160000000000001e-05,
      "loss": 2.3093,
      "step": 17100
    },
    {
      "epoch": 3.43,
      "grad_norm": 3.7407820224761963,
      "learning_rate": 1.3140000000000001e-05,
      "loss": 2.3014,
      "step": 17150
    },
    {
      "epoch": 3.44,
      "grad_norm": 2.7064902782440186,
      "learning_rate": 1.3120000000000001e-05,
      "loss": 2.3044,
      "step": 17200
    },
    {
      "epoch": 3.45,
      "grad_norm": 2.865593194961548,
      "learning_rate": 1.3100000000000002e-05,
      "loss": 2.3037,
      "step": 17250
    },
    {
      "epoch": 3.46,
      "grad_norm": 2.3507792949676514,
      "learning_rate": 1.3080000000000002e-05,
      "loss": 2.3041,
      "step": 17300
    },
    {
      "epoch": 3.4699999999999998,
      "grad_norm": 3.035489797592163,
      "learning_rate": 1.3060000000000002e-05,
      "loss": 2.3139,
      "step": 17350
    },
    {
      "epoch": 3.48,
      "grad_norm": 2.5806639194488525,
      "learning_rate": 1.3040000000000002e-05,
      "loss": 2.3096,
      "step": 17400
    },
    {
      "epoch": 3.49,
      "grad_norm": 4.087007999420166,
      "learning_rate": 1.302e-05,
      "loss": 2.3074,
      "step": 17450
    },
    {
      "epoch": 3.5,
      "grad_norm": 2.7406198978424072,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 2.3039,
      "step": 17500
    },
    {
      "epoch": 3.51,
      "grad_norm": 2.6955206394195557,
      "learning_rate": 1.2980000000000001e-05,
      "loss": 2.3007,
      "step": 17550
    },
    {
      "epoch": 3.52,
      "grad_norm": 2.320455312728882,
      "learning_rate": 1.2960000000000001e-05,
      "loss": 2.3058,
      "step": 17600
    },
    {
      "epoch": 3.5300000000000002,
      "grad_norm": 2.9957616329193115,
      "learning_rate": 1.2940000000000001e-05,
      "loss": 2.3055,
      "step": 17650
    },
    {
      "epoch": 3.54,
      "grad_norm": 3.898442029953003,
      "learning_rate": 1.2920000000000002e-05,
      "loss": 2.3026,
      "step": 17700
    },
    {
      "epoch": 3.55,
      "grad_norm": 2.7614247798919678,
      "learning_rate": 1.2900000000000002e-05,
      "loss": 2.3103,
      "step": 17750
    },
    {
      "epoch": 3.56,
      "grad_norm": 2.703737258911133,
      "learning_rate": 1.2880000000000002e-05,
      "loss": 2.2956,
      "step": 17800
    },
    {
      "epoch": 3.57,
      "grad_norm": 2.9392786026000977,
      "learning_rate": 1.286e-05,
      "loss": 2.3095,
      "step": 17850
    },
    {
      "epoch": 3.58,
      "grad_norm": 2.688724994659424,
      "learning_rate": 1.284e-05,
      "loss": 2.3033,
      "step": 17900
    },
    {
      "epoch": 3.59,
      "grad_norm": 3.2784616947174072,
      "learning_rate": 1.2820000000000001e-05,
      "loss": 2.3008,
      "step": 17950
    },
    {
      "epoch": 3.6,
      "grad_norm": 3.195438861846924,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 2.3045,
      "step": 18000
    },
    {
      "epoch": 3.61,
      "grad_norm": 3.234854221343994,
      "learning_rate": 1.2780000000000001e-05,
      "loss": 2.2986,
      "step": 18050
    },
    {
      "epoch": 3.62,
      "grad_norm": 3.3868660926818848,
      "learning_rate": 1.2760000000000001e-05,
      "loss": 2.3151,
      "step": 18100
    },
    {
      "epoch": 3.63,
      "grad_norm": 3.2249608039855957,
      "learning_rate": 1.2740000000000002e-05,
      "loss": 2.3114,
      "step": 18150
    },
    {
      "epoch": 3.64,
      "grad_norm": 2.40736985206604,
      "learning_rate": 1.2720000000000002e-05,
      "loss": 2.3016,
      "step": 18200
    },
    {
      "epoch": 3.65,
      "grad_norm": 2.814164400100708,
      "learning_rate": 1.27e-05,
      "loss": 2.3092,
      "step": 18250
    },
    {
      "epoch": 3.66,
      "grad_norm": 2.899949789047241,
      "learning_rate": 1.268e-05,
      "loss": 2.3113,
      "step": 18300
    },
    {
      "epoch": 3.67,
      "grad_norm": 2.7486042976379395,
      "learning_rate": 1.266e-05,
      "loss": 2.2999,
      "step": 18350
    },
    {
      "epoch": 3.68,
      "grad_norm": 2.7799060344696045,
      "learning_rate": 1.2640000000000001e-05,
      "loss": 2.3118,
      "step": 18400
    },
    {
      "epoch": 3.69,
      "grad_norm": 3.012212038040161,
      "learning_rate": 1.2620000000000001e-05,
      "loss": 2.304,
      "step": 18450
    },
    {
      "epoch": 3.7,
      "grad_norm": 3.184366464614868,
      "learning_rate": 1.2600000000000001e-05,
      "loss": 2.3122,
      "step": 18500
    },
    {
      "epoch": 3.71,
      "grad_norm": 3.5195910930633545,
      "learning_rate": 1.2580000000000002e-05,
      "loss": 2.3081,
      "step": 18550
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 3.5053794384002686,
      "learning_rate": 1.2560000000000002e-05,
      "loss": 2.3078,
      "step": 18600
    },
    {
      "epoch": 3.73,
      "grad_norm": 4.349270820617676,
      "learning_rate": 1.254e-05,
      "loss": 2.3076,
      "step": 18650
    },
    {
      "epoch": 3.74,
      "grad_norm": 3.548567771911621,
      "learning_rate": 1.252e-05,
      "loss": 2.314,
      "step": 18700
    },
    {
      "epoch": 3.75,
      "grad_norm": 2.480489492416382,
      "learning_rate": 1.25e-05,
      "loss": 2.3056,
      "step": 18750
    },
    {
      "epoch": 3.76,
      "grad_norm": 2.5821006298065186,
      "learning_rate": 1.248e-05,
      "loss": 2.3091,
      "step": 18800
    },
    {
      "epoch": 3.77,
      "grad_norm": 2.2772793769836426,
      "learning_rate": 1.2460000000000001e-05,
      "loss": 2.3091,
      "step": 18850
    },
    {
      "epoch": 3.7800000000000002,
      "grad_norm": 2.981762409210205,
      "learning_rate": 1.2440000000000001e-05,
      "loss": 2.3066,
      "step": 18900
    },
    {
      "epoch": 3.79,
      "grad_norm": 2.21083664894104,
      "learning_rate": 1.2420000000000001e-05,
      "loss": 2.3084,
      "step": 18950
    },
    {
      "epoch": 3.8,
      "grad_norm": 3.631108522415161,
      "learning_rate": 1.2400000000000002e-05,
      "loss": 2.307,
      "step": 19000
    },
    {
      "epoch": 3.81,
      "grad_norm": 3.1925878524780273,
      "learning_rate": 1.2380000000000002e-05,
      "loss": 2.3081,
      "step": 19050
    },
    {
      "epoch": 3.82,
      "grad_norm": 2.6622095108032227,
      "learning_rate": 1.236e-05,
      "loss": 2.307,
      "step": 19100
    },
    {
      "epoch": 3.83,
      "grad_norm": 2.9806296825408936,
      "learning_rate": 1.234e-05,
      "loss": 2.3056,
      "step": 19150
    },
    {
      "epoch": 3.84,
      "grad_norm": 2.269202947616577,
      "learning_rate": 1.232e-05,
      "loss": 2.3048,
      "step": 19200
    },
    {
      "epoch": 3.85,
      "grad_norm": 2.014336109161377,
      "learning_rate": 1.23e-05,
      "loss": 2.309,
      "step": 19250
    },
    {
      "epoch": 3.86,
      "grad_norm": 2.418225049972534,
      "learning_rate": 1.2280000000000001e-05,
      "loss": 2.31,
      "step": 19300
    },
    {
      "epoch": 3.87,
      "grad_norm": 2.5712099075317383,
      "learning_rate": 1.2260000000000001e-05,
      "loss": 2.305,
      "step": 19350
    },
    {
      "epoch": 3.88,
      "grad_norm": 2.8226959705352783,
      "learning_rate": 1.2240000000000001e-05,
      "loss": 2.3071,
      "step": 19400
    },
    {
      "epoch": 3.89,
      "grad_norm": 2.800567626953125,
      "learning_rate": 1.2220000000000002e-05,
      "loss": 2.305,
      "step": 19450
    },
    {
      "epoch": 3.9,
      "grad_norm": 2.868405342102051,
      "learning_rate": 1.22e-05,
      "loss": 2.3014,
      "step": 19500
    },
    {
      "epoch": 3.91,
      "grad_norm": 4.834807395935059,
      "learning_rate": 1.218e-05,
      "loss": 2.3143,
      "step": 19550
    },
    {
      "epoch": 3.92,
      "grad_norm": 3.7130794525146484,
      "learning_rate": 1.216e-05,
      "loss": 2.3025,
      "step": 19600
    },
    {
      "epoch": 3.93,
      "grad_norm": 2.601212978363037,
      "learning_rate": 1.214e-05,
      "loss": 2.3054,
      "step": 19650
    },
    {
      "epoch": 3.94,
      "grad_norm": 2.765312671661377,
      "learning_rate": 1.2120000000000001e-05,
      "loss": 2.3025,
      "step": 19700
    },
    {
      "epoch": 3.95,
      "grad_norm": 3.094862937927246,
      "learning_rate": 1.2100000000000001e-05,
      "loss": 2.3073,
      "step": 19750
    },
    {
      "epoch": 3.96,
      "grad_norm": 2.972184896469116,
      "learning_rate": 1.2080000000000001e-05,
      "loss": 2.3053,
      "step": 19800
    },
    {
      "epoch": 3.9699999999999998,
      "grad_norm": 2.6807777881622314,
      "learning_rate": 1.2060000000000001e-05,
      "loss": 2.3042,
      "step": 19850
    },
    {
      "epoch": 3.98,
      "grad_norm": 2.5225040912628174,
      "learning_rate": 1.204e-05,
      "loss": 2.306,
      "step": 19900
    },
    {
      "epoch": 3.99,
      "grad_norm": 2.9872488975524902,
      "learning_rate": 1.202e-05,
      "loss": 2.3111,
      "step": 19950
    },
    {
      "epoch": 4.0,
      "grad_norm": 2.582008123397827,
      "learning_rate": 1.2e-05,
      "loss": 2.3087,
      "step": 20000
    },
    {
      "epoch": 4.0,
      "eval_loss": 2.304513454437256,
      "eval_runtime": 80.1678,
      "eval_samples_per_second": 124.738,
      "eval_steps_per_second": 15.592,
      "step": 20000
    },
    {
      "epoch": 4.01,
      "grad_norm": 2.248522996902466,
      "learning_rate": 1.198e-05,
      "loss": 2.3067,
      "step": 20050
    },
    {
      "epoch": 4.02,
      "grad_norm": 2.711115837097168,
      "learning_rate": 1.196e-05,
      "loss": 2.307,
      "step": 20100
    },
    {
      "epoch": 4.03,
      "grad_norm": 3.754503011703491,
      "learning_rate": 1.1940000000000001e-05,
      "loss": 2.2967,
      "step": 20150
    },
    {
      "epoch": 4.04,
      "grad_norm": 4.337978839874268,
      "learning_rate": 1.1920000000000001e-05,
      "loss": 2.2987,
      "step": 20200
    },
    {
      "epoch": 4.05,
      "grad_norm": 3.3110780715942383,
      "learning_rate": 1.1900000000000001e-05,
      "loss": 2.3072,
      "step": 20250
    },
    {
      "epoch": 4.06,
      "grad_norm": 2.9314937591552734,
      "learning_rate": 1.188e-05,
      "loss": 2.3144,
      "step": 20300
    },
    {
      "epoch": 4.07,
      "grad_norm": 2.692697286605835,
      "learning_rate": 1.186e-05,
      "loss": 2.3084,
      "step": 20350
    },
    {
      "epoch": 4.08,
      "grad_norm": 2.8851006031036377,
      "learning_rate": 1.184e-05,
      "loss": 2.2972,
      "step": 20400
    },
    {
      "epoch": 4.09,
      "grad_norm": 2.9137256145477295,
      "learning_rate": 1.182e-05,
      "loss": 2.3093,
      "step": 20450
    },
    {
      "epoch": 4.1,
      "grad_norm": 3.2910900115966797,
      "learning_rate": 1.18e-05,
      "loss": 2.3045,
      "step": 20500
    },
    {
      "epoch": 4.11,
      "grad_norm": 2.729559898376465,
      "learning_rate": 1.178e-05,
      "loss": 2.3072,
      "step": 20550
    },
    {
      "epoch": 4.12,
      "grad_norm": 2.9849092960357666,
      "learning_rate": 1.1760000000000001e-05,
      "loss": 2.3085,
      "step": 20600
    },
    {
      "epoch": 4.13,
      "grad_norm": 2.4654793739318848,
      "learning_rate": 1.1740000000000001e-05,
      "loss": 2.308,
      "step": 20650
    },
    {
      "epoch": 4.14,
      "grad_norm": 2.687467336654663,
      "learning_rate": 1.172e-05,
      "loss": 2.3023,
      "step": 20700
    },
    {
      "epoch": 4.15,
      "grad_norm": 2.108617067337036,
      "learning_rate": 1.17e-05,
      "loss": 2.3098,
      "step": 20750
    },
    {
      "epoch": 4.16,
      "grad_norm": 3.079068660736084,
      "learning_rate": 1.168e-05,
      "loss": 2.3015,
      "step": 20800
    },
    {
      "epoch": 4.17,
      "grad_norm": 3.28670334815979,
      "learning_rate": 1.166e-05,
      "loss": 2.309,
      "step": 20850
    },
    {
      "epoch": 4.18,
      "grad_norm": 3.055734872817993,
      "learning_rate": 1.164e-05,
      "loss": 2.3033,
      "step": 20900
    },
    {
      "epoch": 4.19,
      "grad_norm": 3.273629665374756,
      "learning_rate": 1.162e-05,
      "loss": 2.2985,
      "step": 20950
    },
    {
      "epoch": 4.2,
      "grad_norm": 3.185546875,
      "learning_rate": 1.16e-05,
      "loss": 2.3159,
      "step": 21000
    },
    {
      "epoch": 4.21,
      "grad_norm": 2.666844606399536,
      "learning_rate": 1.1580000000000001e-05,
      "loss": 2.3061,
      "step": 21050
    },
    {
      "epoch": 4.22,
      "grad_norm": 2.4846129417419434,
      "learning_rate": 1.156e-05,
      "loss": 2.3045,
      "step": 21100
    },
    {
      "epoch": 4.23,
      "grad_norm": 2.7236857414245605,
      "learning_rate": 1.154e-05,
      "loss": 2.3079,
      "step": 21150
    },
    {
      "epoch": 4.24,
      "grad_norm": 3.1290407180786133,
      "learning_rate": 1.152e-05,
      "loss": 2.3082,
      "step": 21200
    },
    {
      "epoch": 4.25,
      "grad_norm": 2.7664196491241455,
      "learning_rate": 1.15e-05,
      "loss": 2.3096,
      "step": 21250
    },
    {
      "epoch": 4.26,
      "grad_norm": 2.636988639831543,
      "learning_rate": 1.148e-05,
      "loss": 2.306,
      "step": 21300
    },
    {
      "epoch": 4.27,
      "grad_norm": 2.0895309448242188,
      "learning_rate": 1.146e-05,
      "loss": 2.3083,
      "step": 21350
    },
    {
      "epoch": 4.28,
      "grad_norm": 2.4309513568878174,
      "learning_rate": 1.144e-05,
      "loss": 2.3056,
      "step": 21400
    },
    {
      "epoch": 4.29,
      "grad_norm": 2.495255470275879,
      "learning_rate": 1.142e-05,
      "loss": 2.3044,
      "step": 21450
    },
    {
      "epoch": 4.3,
      "grad_norm": 3.149142026901245,
      "learning_rate": 1.14e-05,
      "loss": 2.31,
      "step": 21500
    },
    {
      "epoch": 4.31,
      "grad_norm": 2.3730762004852295,
      "learning_rate": 1.138e-05,
      "loss": 2.3046,
      "step": 21550
    },
    {
      "epoch": 4.32,
      "grad_norm": 1.9491711854934692,
      "learning_rate": 1.136e-05,
      "loss": 2.3071,
      "step": 21600
    },
    {
      "epoch": 4.33,
      "grad_norm": 3.051607608795166,
      "learning_rate": 1.134e-05,
      "loss": 2.3065,
      "step": 21650
    },
    {
      "epoch": 4.34,
      "grad_norm": 3.0851597785949707,
      "learning_rate": 1.132e-05,
      "loss": 2.3083,
      "step": 21700
    },
    {
      "epoch": 4.35,
      "grad_norm": 2.8589558601379395,
      "learning_rate": 1.13e-05,
      "loss": 2.3026,
      "step": 21750
    },
    {
      "epoch": 4.36,
      "grad_norm": 3.5821173191070557,
      "learning_rate": 1.128e-05,
      "loss": 2.3041,
      "step": 21800
    },
    {
      "epoch": 4.37,
      "grad_norm": 2.641977310180664,
      "learning_rate": 1.126e-05,
      "loss": 2.3075,
      "step": 21850
    },
    {
      "epoch": 4.38,
      "grad_norm": 2.7269623279571533,
      "learning_rate": 1.1240000000000002e-05,
      "loss": 2.302,
      "step": 21900
    },
    {
      "epoch": 4.39,
      "grad_norm": 3.2832956314086914,
      "learning_rate": 1.1220000000000003e-05,
      "loss": 2.2965,
      "step": 21950
    },
    {
      "epoch": 4.4,
      "grad_norm": 2.7536699771881104,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 2.3078,
      "step": 22000
    },
    {
      "epoch": 4.41,
      "grad_norm": 3.149033546447754,
      "learning_rate": 1.1180000000000001e-05,
      "loss": 2.3048,
      "step": 22050
    },
    {
      "epoch": 4.42,
      "grad_norm": 2.178201913833618,
      "learning_rate": 1.1160000000000002e-05,
      "loss": 2.3038,
      "step": 22100
    },
    {
      "epoch": 4.43,
      "grad_norm": 2.8356592655181885,
      "learning_rate": 1.1140000000000002e-05,
      "loss": 2.3109,
      "step": 22150
    },
    {
      "epoch": 4.44,
      "grad_norm": 2.5951313972473145,
      "learning_rate": 1.1120000000000002e-05,
      "loss": 2.3075,
      "step": 22200
    },
    {
      "epoch": 4.45,
      "grad_norm": 3.1130530834198,
      "learning_rate": 1.1100000000000002e-05,
      "loss": 2.3026,
      "step": 22250
    },
    {
      "epoch": 4.46,
      "grad_norm": 3.437817335128784,
      "learning_rate": 1.1080000000000002e-05,
      "loss": 2.304,
      "step": 22300
    },
    {
      "epoch": 4.47,
      "grad_norm": 2.766355514526367,
      "learning_rate": 1.1060000000000003e-05,
      "loss": 2.3037,
      "step": 22350
    },
    {
      "epoch": 4.48,
      "grad_norm": 3.4103446006774902,
      "learning_rate": 1.1040000000000001e-05,
      "loss": 2.3028,
      "step": 22400
    },
    {
      "epoch": 4.49,
      "grad_norm": 2.2621617317199707,
      "learning_rate": 1.1020000000000001e-05,
      "loss": 2.3073,
      "step": 22450
    },
    {
      "epoch": 4.5,
      "grad_norm": 2.8432118892669678,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 2.3032,
      "step": 22500
    },
    {
      "epoch": 4.51,
      "grad_norm": 2.987271308898926,
      "learning_rate": 1.0980000000000002e-05,
      "loss": 2.3126,
      "step": 22550
    },
    {
      "epoch": 4.52,
      "grad_norm": 2.6659600734710693,
      "learning_rate": 1.0960000000000002e-05,
      "loss": 2.3101,
      "step": 22600
    },
    {
      "epoch": 4.53,
      "grad_norm": 3.1232240200042725,
      "learning_rate": 1.0940000000000002e-05,
      "loss": 2.306,
      "step": 22650
    },
    {
      "epoch": 4.54,
      "grad_norm": 2.237391710281372,
      "learning_rate": 1.0920000000000002e-05,
      "loss": 2.307,
      "step": 22700
    },
    {
      "epoch": 4.55,
      "grad_norm": 2.447011709213257,
      "learning_rate": 1.0900000000000002e-05,
      "loss": 2.3049,
      "step": 22750
    },
    {
      "epoch": 4.5600000000000005,
      "grad_norm": 2.3561415672302246,
      "learning_rate": 1.0880000000000001e-05,
      "loss": 2.303,
      "step": 22800
    },
    {
      "epoch": 4.57,
      "grad_norm": 2.596703290939331,
      "learning_rate": 1.0860000000000001e-05,
      "loss": 2.3088,
      "step": 22850
    },
    {
      "epoch": 4.58,
      "grad_norm": 3.0583691596984863,
      "learning_rate": 1.0840000000000001e-05,
      "loss": 2.3102,
      "step": 22900
    },
    {
      "epoch": 4.59,
      "grad_norm": 2.4712886810302734,
      "learning_rate": 1.0820000000000001e-05,
      "loss": 2.3094,
      "step": 22950
    },
    {
      "epoch": 4.6,
      "grad_norm": 2.4367997646331787,
      "learning_rate": 1.0800000000000002e-05,
      "loss": 2.3053,
      "step": 23000
    },
    {
      "epoch": 4.61,
      "grad_norm": 2.8364923000335693,
      "learning_rate": 1.0780000000000002e-05,
      "loss": 2.3051,
      "step": 23050
    },
    {
      "epoch": 4.62,
      "grad_norm": 2.7456135749816895,
      "learning_rate": 1.0760000000000002e-05,
      "loss": 2.3077,
      "step": 23100
    },
    {
      "epoch": 4.63,
      "grad_norm": 4.068730354309082,
      "learning_rate": 1.0740000000000002e-05,
      "loss": 2.3077,
      "step": 23150
    },
    {
      "epoch": 4.64,
      "grad_norm": 3.001342535018921,
      "learning_rate": 1.072e-05,
      "loss": 2.3089,
      "step": 23200
    },
    {
      "epoch": 4.65,
      "grad_norm": 3.4423065185546875,
      "learning_rate": 1.0700000000000001e-05,
      "loss": 2.3051,
      "step": 23250
    },
    {
      "epoch": 4.66,
      "grad_norm": 2.428626298904419,
      "learning_rate": 1.0680000000000001e-05,
      "loss": 2.3046,
      "step": 23300
    },
    {
      "epoch": 4.67,
      "grad_norm": 2.441276788711548,
      "learning_rate": 1.0660000000000001e-05,
      "loss": 2.3059,
      "step": 23350
    },
    {
      "epoch": 4.68,
      "grad_norm": 1.9387147426605225,
      "learning_rate": 1.0640000000000001e-05,
      "loss": 2.3011,
      "step": 23400
    },
    {
      "epoch": 4.6899999999999995,
      "grad_norm": 2.9489777088165283,
      "learning_rate": 1.0620000000000002e-05,
      "loss": 2.2997,
      "step": 23450
    },
    {
      "epoch": 4.7,
      "grad_norm": 2.839346408843994,
      "learning_rate": 1.0600000000000002e-05,
      "loss": 2.3118,
      "step": 23500
    },
    {
      "epoch": 4.71,
      "grad_norm": 2.465365171432495,
      "learning_rate": 1.0580000000000002e-05,
      "loss": 2.3041,
      "step": 23550
    },
    {
      "epoch": 4.72,
      "grad_norm": 2.8413262367248535,
      "learning_rate": 1.056e-05,
      "loss": 2.3061,
      "step": 23600
    },
    {
      "epoch": 4.73,
      "grad_norm": 2.7019758224487305,
      "learning_rate": 1.054e-05,
      "loss": 2.3087,
      "step": 23650
    },
    {
      "epoch": 4.74,
      "grad_norm": 3.4093728065490723,
      "learning_rate": 1.0520000000000001e-05,
      "loss": 2.3045,
      "step": 23700
    },
    {
      "epoch": 4.75,
      "grad_norm": 2.784804582595825,
      "learning_rate": 1.0500000000000001e-05,
      "loss": 2.3066,
      "step": 23750
    },
    {
      "epoch": 4.76,
      "grad_norm": 2.1512787342071533,
      "learning_rate": 1.0480000000000001e-05,
      "loss": 2.3028,
      "step": 23800
    },
    {
      "epoch": 4.77,
      "grad_norm": 2.749594211578369,
      "learning_rate": 1.0460000000000001e-05,
      "loss": 2.3013,
      "step": 23850
    },
    {
      "epoch": 4.78,
      "grad_norm": 3.2891950607299805,
      "learning_rate": 1.0440000000000002e-05,
      "loss": 2.3092,
      "step": 23900
    },
    {
      "epoch": 4.79,
      "grad_norm": 2.587604284286499,
      "learning_rate": 1.0420000000000002e-05,
      "loss": 2.3021,
      "step": 23950
    },
    {
      "epoch": 4.8,
      "grad_norm": 2.185086488723755,
      "learning_rate": 1.04e-05,
      "loss": 2.3028,
      "step": 24000
    },
    {
      "epoch": 4.8100000000000005,
      "grad_norm": 3.263176202774048,
      "learning_rate": 1.038e-05,
      "loss": 2.3089,
      "step": 24050
    },
    {
      "epoch": 4.82,
      "grad_norm": 2.337491512298584,
      "learning_rate": 1.036e-05,
      "loss": 2.3078,
      "step": 24100
    },
    {
      "epoch": 4.83,
      "grad_norm": 2.770355224609375,
      "learning_rate": 1.0340000000000001e-05,
      "loss": 2.3126,
      "step": 24150
    },
    {
      "epoch": 4.84,
      "grad_norm": 3.221604824066162,
      "learning_rate": 1.0320000000000001e-05,
      "loss": 2.3048,
      "step": 24200
    },
    {
      "epoch": 4.85,
      "grad_norm": 2.725904703140259,
      "learning_rate": 1.0300000000000001e-05,
      "loss": 2.3026,
      "step": 24250
    },
    {
      "epoch": 4.86,
      "grad_norm": 2.3120908737182617,
      "learning_rate": 1.0280000000000002e-05,
      "loss": 2.3024,
      "step": 24300
    },
    {
      "epoch": 4.87,
      "grad_norm": 2.347205400466919,
      "learning_rate": 1.0260000000000002e-05,
      "loss": 2.3058,
      "step": 24350
    },
    {
      "epoch": 4.88,
      "grad_norm": 2.7632265090942383,
      "learning_rate": 1.024e-05,
      "loss": 2.3099,
      "step": 24400
    },
    {
      "epoch": 4.89,
      "grad_norm": 2.95652437210083,
      "learning_rate": 1.022e-05,
      "loss": 2.303,
      "step": 24450
    },
    {
      "epoch": 4.9,
      "grad_norm": 1.980648398399353,
      "learning_rate": 1.02e-05,
      "loss": 2.308,
      "step": 24500
    },
    {
      "epoch": 4.91,
      "grad_norm": 2.5096709728240967,
      "learning_rate": 1.018e-05,
      "loss": 2.3091,
      "step": 24550
    },
    {
      "epoch": 4.92,
      "grad_norm": 3.6924164295196533,
      "learning_rate": 1.0160000000000001e-05,
      "loss": 2.3036,
      "step": 24600
    },
    {
      "epoch": 4.93,
      "grad_norm": 2.30193829536438,
      "learning_rate": 1.0140000000000001e-05,
      "loss": 2.304,
      "step": 24650
    },
    {
      "epoch": 4.9399999999999995,
      "grad_norm": 2.312033176422119,
      "learning_rate": 1.0120000000000001e-05,
      "loss": 2.3073,
      "step": 24700
    },
    {
      "epoch": 4.95,
      "grad_norm": 2.493994951248169,
      "learning_rate": 1.0100000000000002e-05,
      "loss": 2.3103,
      "step": 24750
    },
    {
      "epoch": 4.96,
      "grad_norm": 2.1135478019714355,
      "learning_rate": 1.008e-05,
      "loss": 2.3007,
      "step": 24800
    },
    {
      "epoch": 4.97,
      "grad_norm": 2.3266663551330566,
      "learning_rate": 1.006e-05,
      "loss": 2.3087,
      "step": 24850
    },
    {
      "epoch": 4.98,
      "grad_norm": 2.7491393089294434,
      "learning_rate": 1.004e-05,
      "loss": 2.3053,
      "step": 24900
    },
    {
      "epoch": 4.99,
      "grad_norm": 3.0552401542663574,
      "learning_rate": 1.002e-05,
      "loss": 2.3039,
      "step": 24950
    },
    {
      "epoch": 5.0,
      "grad_norm": 2.0856590270996094,
      "learning_rate": 1e-05,
      "loss": 2.2994,
      "step": 25000
    },
    {
      "epoch": 5.0,
      "eval_loss": 2.306124210357666,
      "eval_runtime": 81.0752,
      "eval_samples_per_second": 123.342,
      "eval_steps_per_second": 15.418,
      "step": 25000
    },
    {
      "epoch": 5.01,
      "grad_norm": 2.318976402282715,
      "learning_rate": 9.980000000000001e-06,
      "loss": 2.3109,
      "step": 25050
    },
    {
      "epoch": 5.02,
      "grad_norm": 2.972386598587036,
      "learning_rate": 9.960000000000001e-06,
      "loss": 2.3065,
      "step": 25100
    },
    {
      "epoch": 5.03,
      "grad_norm": 4.287505626678467,
      "learning_rate": 9.940000000000001e-06,
      "loss": 2.307,
      "step": 25150
    },
    {
      "epoch": 5.04,
      "grad_norm": 2.1588892936706543,
      "learning_rate": 9.920000000000002e-06,
      "loss": 2.3076,
      "step": 25200
    },
    {
      "epoch": 5.05,
      "grad_norm": 2.759894847869873,
      "learning_rate": 9.9e-06,
      "loss": 2.3071,
      "step": 25250
    },
    {
      "epoch": 5.06,
      "grad_norm": 2.4955785274505615,
      "learning_rate": 9.88e-06,
      "loss": 2.3041,
      "step": 25300
    },
    {
      "epoch": 5.07,
      "grad_norm": 2.239356756210327,
      "learning_rate": 9.86e-06,
      "loss": 2.299,
      "step": 25350
    },
    {
      "epoch": 5.08,
      "grad_norm": 2.5481116771698,
      "learning_rate": 9.84e-06,
      "loss": 2.3111,
      "step": 25400
    },
    {
      "epoch": 5.09,
      "grad_norm": 2.5271284580230713,
      "learning_rate": 9.820000000000001e-06,
      "loss": 2.3076,
      "step": 25450
    },
    {
      "epoch": 5.1,
      "grad_norm": 2.7213964462280273,
      "learning_rate": 9.800000000000001e-06,
      "loss": 2.3068,
      "step": 25500
    },
    {
      "epoch": 5.11,
      "grad_norm": 2.214102268218994,
      "learning_rate": 9.780000000000001e-06,
      "loss": 2.3042,
      "step": 25550
    },
    {
      "epoch": 5.12,
      "grad_norm": 2.2783401012420654,
      "learning_rate": 9.760000000000001e-06,
      "loss": 2.312,
      "step": 25600
    },
    {
      "epoch": 5.13,
      "grad_norm": 2.7349472045898438,
      "learning_rate": 9.74e-06,
      "loss": 2.307,
      "step": 25650
    },
    {
      "epoch": 5.14,
      "grad_norm": 2.712162494659424,
      "learning_rate": 9.72e-06,
      "loss": 2.3043,
      "step": 25700
    },
    {
      "epoch": 5.15,
      "grad_norm": 2.591907501220703,
      "learning_rate": 9.7e-06,
      "loss": 2.2991,
      "step": 25750
    },
    {
      "epoch": 5.16,
      "grad_norm": 2.1237616539001465,
      "learning_rate": 9.68e-06,
      "loss": 2.3084,
      "step": 25800
    },
    {
      "epoch": 5.17,
      "grad_norm": 2.8030035495758057,
      "learning_rate": 9.66e-06,
      "loss": 2.3068,
      "step": 25850
    },
    {
      "epoch": 5.18,
      "grad_norm": 3.0479795932769775,
      "learning_rate": 9.640000000000001e-06,
      "loss": 2.3136,
      "step": 25900
    },
    {
      "epoch": 5.19,
      "grad_norm": 2.085261821746826,
      "learning_rate": 9.620000000000001e-06,
      "loss": 2.3015,
      "step": 25950
    },
    {
      "epoch": 5.2,
      "grad_norm": 2.1493735313415527,
      "learning_rate": 9.600000000000001e-06,
      "loss": 2.306,
      "step": 26000
    },
    {
      "epoch": 5.21,
      "grad_norm": 1.7415798902511597,
      "learning_rate": 9.58e-06,
      "loss": 2.2987,
      "step": 26050
    },
    {
      "epoch": 5.22,
      "grad_norm": 3.0313212871551514,
      "learning_rate": 9.56e-06,
      "loss": 2.3082,
      "step": 26100
    },
    {
      "epoch": 5.23,
      "grad_norm": 3.2365176677703857,
      "learning_rate": 9.54e-06,
      "loss": 2.3055,
      "step": 26150
    },
    {
      "epoch": 5.24,
      "grad_norm": 1.988623857498169,
      "learning_rate": 9.52e-06,
      "loss": 2.3035,
      "step": 26200
    },
    {
      "epoch": 5.25,
      "grad_norm": 2.0103254318237305,
      "learning_rate": 9.5e-06,
      "loss": 2.3085,
      "step": 26250
    },
    {
      "epoch": 5.26,
      "grad_norm": 2.2323555946350098,
      "learning_rate": 9.48e-06,
      "loss": 2.3063,
      "step": 26300
    },
    {
      "epoch": 5.27,
      "grad_norm": 1.9059062004089355,
      "learning_rate": 9.460000000000001e-06,
      "loss": 2.3039,
      "step": 26350
    },
    {
      "epoch": 5.28,
      "grad_norm": 2.634831428527832,
      "learning_rate": 9.440000000000001e-06,
      "loss": 2.3075,
      "step": 26400
    },
    {
      "epoch": 5.29,
      "grad_norm": 3.371645927429199,
      "learning_rate": 9.42e-06,
      "loss": 2.3084,
      "step": 26450
    },
    {
      "epoch": 5.3,
      "grad_norm": 2.2664854526519775,
      "learning_rate": 9.4e-06,
      "loss": 2.3061,
      "step": 26500
    },
    {
      "epoch": 5.31,
      "grad_norm": 2.990535259246826,
      "learning_rate": 9.38e-06,
      "loss": 2.3026,
      "step": 26550
    },
    {
      "epoch": 5.32,
      "grad_norm": 1.9023431539535522,
      "learning_rate": 9.360000000000002e-06,
      "loss": 2.3017,
      "step": 26600
    },
    {
      "epoch": 5.33,
      "grad_norm": 2.421755790710449,
      "learning_rate": 9.340000000000002e-06,
      "loss": 2.3036,
      "step": 26650
    },
    {
      "epoch": 5.34,
      "grad_norm": 3.825237989425659,
      "learning_rate": 9.32e-06,
      "loss": 2.2987,
      "step": 26700
    },
    {
      "epoch": 5.35,
      "grad_norm": 1.9496073722839355,
      "learning_rate": 9.3e-06,
      "loss": 2.3106,
      "step": 26750
    },
    {
      "epoch": 5.36,
      "grad_norm": 2.381946325302124,
      "learning_rate": 9.280000000000001e-06,
      "loss": 2.3005,
      "step": 26800
    },
    {
      "epoch": 5.37,
      "grad_norm": 3.486485719680786,
      "learning_rate": 9.260000000000001e-06,
      "loss": 2.3037,
      "step": 26850
    },
    {
      "epoch": 5.38,
      "grad_norm": 2.7597367763519287,
      "learning_rate": 9.240000000000001e-06,
      "loss": 2.3082,
      "step": 26900
    },
    {
      "epoch": 5.39,
      "grad_norm": 3.189903497695923,
      "learning_rate": 9.220000000000002e-06,
      "loss": 2.3089,
      "step": 26950
    },
    {
      "epoch": 5.4,
      "grad_norm": 2.647944450378418,
      "learning_rate": 9.200000000000002e-06,
      "loss": 2.306,
      "step": 27000
    },
    {
      "epoch": 5.41,
      "grad_norm": 2.259282112121582,
      "learning_rate": 9.180000000000002e-06,
      "loss": 2.303,
      "step": 27050
    },
    {
      "epoch": 5.42,
      "grad_norm": 3.2376906871795654,
      "learning_rate": 9.16e-06,
      "loss": 2.3042,
      "step": 27100
    },
    {
      "epoch": 5.43,
      "grad_norm": 2.5395426750183105,
      "learning_rate": 9.14e-06,
      "loss": 2.3044,
      "step": 27150
    },
    {
      "epoch": 5.44,
      "grad_norm": 1.9608588218688965,
      "learning_rate": 9.12e-06,
      "loss": 2.302,
      "step": 27200
    },
    {
      "epoch": 5.45,
      "grad_norm": 2.4313740730285645,
      "learning_rate": 9.100000000000001e-06,
      "loss": 2.2957,
      "step": 27250
    },
    {
      "epoch": 5.46,
      "grad_norm": 2.297826051712036,
      "learning_rate": 9.080000000000001e-06,
      "loss": 2.3041,
      "step": 27300
    },
    {
      "epoch": 5.47,
      "grad_norm": 2.6716995239257812,
      "learning_rate": 9.060000000000001e-06,
      "loss": 2.3013,
      "step": 27350
    },
    {
      "epoch": 5.48,
      "grad_norm": 2.457958459854126,
      "learning_rate": 9.040000000000002e-06,
      "loss": 2.309,
      "step": 27400
    },
    {
      "epoch": 5.49,
      "grad_norm": 3.1113502979278564,
      "learning_rate": 9.020000000000002e-06,
      "loss": 2.3061,
      "step": 27450
    },
    {
      "epoch": 5.5,
      "grad_norm": 2.1396238803863525,
      "learning_rate": 9e-06,
      "loss": 2.3072,
      "step": 27500
    },
    {
      "epoch": 5.51,
      "grad_norm": 2.974846839904785,
      "learning_rate": 8.98e-06,
      "loss": 2.3095,
      "step": 27550
    },
    {
      "epoch": 5.52,
      "grad_norm": 2.6742169857025146,
      "learning_rate": 8.96e-06,
      "loss": 2.3059,
      "step": 27600
    },
    {
      "epoch": 5.53,
      "grad_norm": 2.01405930519104,
      "learning_rate": 8.94e-06,
      "loss": 2.3059,
      "step": 27650
    },
    {
      "epoch": 5.54,
      "grad_norm": 2.721114158630371,
      "learning_rate": 8.920000000000001e-06,
      "loss": 2.3089,
      "step": 27700
    },
    {
      "epoch": 5.55,
      "grad_norm": 3.1808860301971436,
      "learning_rate": 8.900000000000001e-06,
      "loss": 2.3067,
      "step": 27750
    },
    {
      "epoch": 5.5600000000000005,
      "grad_norm": 2.120537519454956,
      "learning_rate": 8.880000000000001e-06,
      "loss": 2.3037,
      "step": 27800
    },
    {
      "epoch": 5.57,
      "grad_norm": 2.9770376682281494,
      "learning_rate": 8.860000000000002e-06,
      "loss": 2.3024,
      "step": 27850
    },
    {
      "epoch": 5.58,
      "grad_norm": 2.221165895462036,
      "learning_rate": 8.84e-06,
      "loss": 2.3006,
      "step": 27900
    },
    {
      "epoch": 5.59,
      "grad_norm": 3.1120948791503906,
      "learning_rate": 8.82e-06,
      "loss": 2.3029,
      "step": 27950
    },
    {
      "epoch": 5.6,
      "grad_norm": 2.3510406017303467,
      "learning_rate": 8.8e-06,
      "loss": 2.3079,
      "step": 28000
    },
    {
      "epoch": 5.61,
      "grad_norm": 2.5917530059814453,
      "learning_rate": 8.78e-06,
      "loss": 2.311,
      "step": 28050
    },
    {
      "epoch": 5.62,
      "grad_norm": 2.9226512908935547,
      "learning_rate": 8.76e-06,
      "loss": 2.3065,
      "step": 28100
    },
    {
      "epoch": 5.63,
      "grad_norm": 2.3445510864257812,
      "learning_rate": 8.740000000000001e-06,
      "loss": 2.3047,
      "step": 28150
    },
    {
      "epoch": 5.64,
      "grad_norm": 2.3747942447662354,
      "learning_rate": 8.720000000000001e-06,
      "loss": 2.3038,
      "step": 28200
    },
    {
      "epoch": 5.65,
      "grad_norm": 2.806694507598877,
      "learning_rate": 8.700000000000001e-06,
      "loss": 2.3051,
      "step": 28250
    },
    {
      "epoch": 5.66,
      "grad_norm": 2.047832727432251,
      "learning_rate": 8.68e-06,
      "loss": 2.3052,
      "step": 28300
    },
    {
      "epoch": 5.67,
      "grad_norm": 2.5870745182037354,
      "learning_rate": 8.66e-06,
      "loss": 2.3046,
      "step": 28350
    },
    {
      "epoch": 5.68,
      "grad_norm": 2.3797497749328613,
      "learning_rate": 8.64e-06,
      "loss": 2.3099,
      "step": 28400
    },
    {
      "epoch": 5.6899999999999995,
      "grad_norm": 3.1197400093078613,
      "learning_rate": 8.62e-06,
      "loss": 2.2988,
      "step": 28450
    },
    {
      "epoch": 5.7,
      "grad_norm": 2.427788496017456,
      "learning_rate": 8.6e-06,
      "loss": 2.3106,
      "step": 28500
    },
    {
      "epoch": 5.71,
      "grad_norm": 2.5976176261901855,
      "learning_rate": 8.580000000000001e-06,
      "loss": 2.3091,
      "step": 28550
    },
    {
      "epoch": 5.72,
      "grad_norm": 3.698845863342285,
      "learning_rate": 8.560000000000001e-06,
      "loss": 2.3076,
      "step": 28600
    },
    {
      "epoch": 5.73,
      "grad_norm": 3.294133186340332,
      "learning_rate": 8.540000000000001e-06,
      "loss": 2.3032,
      "step": 28650
    },
    {
      "epoch": 5.74,
      "grad_norm": 2.968776226043701,
      "learning_rate": 8.52e-06,
      "loss": 2.3091,
      "step": 28700
    },
    {
      "epoch": 5.75,
      "grad_norm": 2.7293224334716797,
      "learning_rate": 8.5e-06,
      "loss": 2.3123,
      "step": 28750
    },
    {
      "epoch": 5.76,
      "grad_norm": 2.6169276237487793,
      "learning_rate": 8.48e-06,
      "loss": 2.2962,
      "step": 28800
    },
    {
      "epoch": 5.77,
      "grad_norm": 2.2692346572875977,
      "learning_rate": 8.46e-06,
      "loss": 2.2975,
      "step": 28850
    },
    {
      "epoch": 5.78,
      "grad_norm": 2.2397406101226807,
      "learning_rate": 8.44e-06,
      "loss": 2.3068,
      "step": 28900
    },
    {
      "epoch": 5.79,
      "grad_norm": 2.9125561714172363,
      "learning_rate": 8.42e-06,
      "loss": 2.3024,
      "step": 28950
    },
    {
      "epoch": 5.8,
      "grad_norm": 2.292748212814331,
      "learning_rate": 8.400000000000001e-06,
      "loss": 2.2967,
      "step": 29000
    },
    {
      "epoch": 5.8100000000000005,
      "grad_norm": 2.3284175395965576,
      "learning_rate": 8.380000000000001e-06,
      "loss": 2.3119,
      "step": 29050
    },
    {
      "epoch": 5.82,
      "grad_norm": 2.4995622634887695,
      "learning_rate": 8.36e-06,
      "loss": 2.3098,
      "step": 29100
    },
    {
      "epoch": 5.83,
      "grad_norm": 2.3327507972717285,
      "learning_rate": 8.34e-06,
      "loss": 2.3031,
      "step": 29150
    },
    {
      "epoch": 5.84,
      "grad_norm": 2.5056779384613037,
      "learning_rate": 8.32e-06,
      "loss": 2.3024,
      "step": 29200
    },
    {
      "epoch": 5.85,
      "grad_norm": 2.441554069519043,
      "learning_rate": 8.3e-06,
      "loss": 2.3065,
      "step": 29250
    },
    {
      "epoch": 5.86,
      "grad_norm": 2.8062963485717773,
      "learning_rate": 8.28e-06,
      "loss": 2.3059,
      "step": 29300
    },
    {
      "epoch": 5.87,
      "grad_norm": 2.4145143032073975,
      "learning_rate": 8.26e-06,
      "loss": 2.3078,
      "step": 29350
    },
    {
      "epoch": 5.88,
      "grad_norm": 2.258784294128418,
      "learning_rate": 8.24e-06,
      "loss": 2.3012,
      "step": 29400
    },
    {
      "epoch": 5.89,
      "grad_norm": 2.016547679901123,
      "learning_rate": 8.220000000000001e-06,
      "loss": 2.3088,
      "step": 29450
    },
    {
      "epoch": 5.9,
      "grad_norm": 2.6244308948516846,
      "learning_rate": 8.2e-06,
      "loss": 2.2984,
      "step": 29500
    },
    {
      "epoch": 5.91,
      "grad_norm": 2.3264875411987305,
      "learning_rate": 8.18e-06,
      "loss": 2.3028,
      "step": 29550
    },
    {
      "epoch": 5.92,
      "grad_norm": 3.425694227218628,
      "learning_rate": 8.16e-06,
      "loss": 2.3026,
      "step": 29600
    },
    {
      "epoch": 5.93,
      "grad_norm": 3.337432384490967,
      "learning_rate": 8.14e-06,
      "loss": 2.3083,
      "step": 29650
    },
    {
      "epoch": 5.9399999999999995,
      "grad_norm": 2.280160427093506,
      "learning_rate": 8.120000000000002e-06,
      "loss": 2.3009,
      "step": 29700
    },
    {
      "epoch": 5.95,
      "grad_norm": 2.8757858276367188,
      "learning_rate": 8.1e-06,
      "loss": 2.3009,
      "step": 29750
    },
    {
      "epoch": 5.96,
      "grad_norm": 2.285874128341675,
      "learning_rate": 8.08e-06,
      "loss": 2.3036,
      "step": 29800
    },
    {
      "epoch": 5.97,
      "grad_norm": 3.303217649459839,
      "learning_rate": 8.06e-06,
      "loss": 2.3066,
      "step": 29850
    },
    {
      "epoch": 5.98,
      "grad_norm": 3.713111639022827,
      "learning_rate": 8.040000000000001e-06,
      "loss": 2.3075,
      "step": 29900
    },
    {
      "epoch": 5.99,
      "grad_norm": 2.7750110626220703,
      "learning_rate": 8.020000000000001e-06,
      "loss": 2.3048,
      "step": 29950
    },
    {
      "epoch": 6.0,
      "grad_norm": 3.264353036880493,
      "learning_rate": 8.000000000000001e-06,
      "loss": 2.3052,
      "step": 30000
    },
    {
      "epoch": 6.0,
      "eval_loss": 2.3044745922088623,
      "eval_runtime": 86.5392,
      "eval_samples_per_second": 115.555,
      "eval_steps_per_second": 14.444,
      "step": 30000
    },
    {
      "epoch": 6.01,
      "grad_norm": 2.3522140979766846,
      "learning_rate": 7.980000000000002e-06,
      "loss": 2.3033,
      "step": 30050
    },
    {
      "epoch": 6.02,
      "grad_norm": 2.577932119369507,
      "learning_rate": 7.960000000000002e-06,
      "loss": 2.305,
      "step": 30100
    },
    {
      "epoch": 6.03,
      "grad_norm": 3.478081226348877,
      "learning_rate": 7.94e-06,
      "loss": 2.3054,
      "step": 30150
    },
    {
      "epoch": 6.04,
      "grad_norm": 1.9758713245391846,
      "learning_rate": 7.92e-06,
      "loss": 2.3001,
      "step": 30200
    },
    {
      "epoch": 6.05,
      "grad_norm": 1.9999338388442993,
      "learning_rate": 7.9e-06,
      "loss": 2.3068,
      "step": 30250
    },
    {
      "epoch": 6.06,
      "grad_norm": 2.1782093048095703,
      "learning_rate": 7.88e-06,
      "loss": 2.301,
      "step": 30300
    },
    {
      "epoch": 6.07,
      "grad_norm": 2.3675873279571533,
      "learning_rate": 7.860000000000001e-06,
      "loss": 2.302,
      "step": 30350
    },
    {
      "epoch": 6.08,
      "grad_norm": 2.570207118988037,
      "learning_rate": 7.840000000000001e-06,
      "loss": 2.3027,
      "step": 30400
    },
    {
      "epoch": 6.09,
      "grad_norm": 2.886136293411255,
      "learning_rate": 7.820000000000001e-06,
      "loss": 2.3118,
      "step": 30450
    },
    {
      "epoch": 6.1,
      "grad_norm": 2.4668965339660645,
      "learning_rate": 7.800000000000002e-06,
      "loss": 2.3111,
      "step": 30500
    },
    {
      "epoch": 6.11,
      "grad_norm": 2.995600938796997,
      "learning_rate": 7.78e-06,
      "loss": 2.3034,
      "step": 30550
    },
    {
      "epoch": 6.12,
      "grad_norm": 2.5221993923187256,
      "learning_rate": 7.76e-06,
      "loss": 2.3036,
      "step": 30600
    },
    {
      "epoch": 6.13,
      "grad_norm": 1.9779077768325806,
      "learning_rate": 7.74e-06,
      "loss": 2.3019,
      "step": 30650
    },
    {
      "epoch": 6.14,
      "grad_norm": 2.9039082527160645,
      "learning_rate": 7.72e-06,
      "loss": 2.299,
      "step": 30700
    },
    {
      "epoch": 6.15,
      "grad_norm": 2.2708005905151367,
      "learning_rate": 7.7e-06,
      "loss": 2.3059,
      "step": 30750
    },
    {
      "epoch": 6.16,
      "grad_norm": 2.5029242038726807,
      "learning_rate": 7.680000000000001e-06,
      "loss": 2.3067,
      "step": 30800
    },
    {
      "epoch": 6.17,
      "grad_norm": 3.192117214202881,
      "learning_rate": 7.660000000000001e-06,
      "loss": 2.3066,
      "step": 30850
    },
    {
      "epoch": 6.18,
      "grad_norm": 2.5358357429504395,
      "learning_rate": 7.640000000000001e-06,
      "loss": 2.3048,
      "step": 30900
    },
    {
      "epoch": 6.19,
      "grad_norm": 1.8782929182052612,
      "learning_rate": 7.620000000000001e-06,
      "loss": 2.3026,
      "step": 30950
    },
    {
      "epoch": 6.2,
      "grad_norm": 2.477928400039673,
      "learning_rate": 7.600000000000001e-06,
      "loss": 2.3019,
      "step": 31000
    },
    {
      "epoch": 6.21,
      "grad_norm": 2.2901718616485596,
      "learning_rate": 7.58e-06,
      "loss": 2.311,
      "step": 31050
    },
    {
      "epoch": 6.22,
      "grad_norm": 3.8739049434661865,
      "learning_rate": 7.5600000000000005e-06,
      "loss": 2.3066,
      "step": 31100
    },
    {
      "epoch": 6.23,
      "grad_norm": 2.2428205013275146,
      "learning_rate": 7.540000000000001e-06,
      "loss": 2.2999,
      "step": 31150
    },
    {
      "epoch": 6.24,
      "grad_norm": 2.7602334022521973,
      "learning_rate": 7.520000000000001e-06,
      "loss": 2.3039,
      "step": 31200
    },
    {
      "epoch": 6.25,
      "grad_norm": 2.04937481880188,
      "learning_rate": 7.500000000000001e-06,
      "loss": 2.3098,
      "step": 31250
    },
    {
      "epoch": 6.26,
      "grad_norm": 2.2785089015960693,
      "learning_rate": 7.48e-06,
      "loss": 2.303,
      "step": 31300
    },
    {
      "epoch": 6.27,
      "grad_norm": 2.1144957542419434,
      "learning_rate": 7.4600000000000006e-06,
      "loss": 2.3045,
      "step": 31350
    },
    {
      "epoch": 6.28,
      "grad_norm": 2.134369134902954,
      "learning_rate": 7.440000000000001e-06,
      "loss": 2.304,
      "step": 31400
    },
    {
      "epoch": 6.29,
      "grad_norm": 3.0026516914367676,
      "learning_rate": 7.420000000000001e-06,
      "loss": 2.3052,
      "step": 31450
    },
    {
      "epoch": 6.3,
      "grad_norm": 2.3099398612976074,
      "learning_rate": 7.4e-06,
      "loss": 2.3034,
      "step": 31500
    },
    {
      "epoch": 6.31,
      "grad_norm": 3.200728178024292,
      "learning_rate": 7.3800000000000005e-06,
      "loss": 2.3077,
      "step": 31550
    },
    {
      "epoch": 6.32,
      "grad_norm": 2.426515817642212,
      "learning_rate": 7.360000000000001e-06,
      "loss": 2.3044,
      "step": 31600
    },
    {
      "epoch": 6.33,
      "grad_norm": 2.4297890663146973,
      "learning_rate": 7.340000000000001e-06,
      "loss": 2.3037,
      "step": 31650
    },
    {
      "epoch": 6.34,
      "grad_norm": 2.3624539375305176,
      "learning_rate": 7.32e-06,
      "loss": 2.301,
      "step": 31700
    },
    {
      "epoch": 6.35,
      "grad_norm": 2.4993793964385986,
      "learning_rate": 7.3e-06,
      "loss": 2.3046,
      "step": 31750
    },
    {
      "epoch": 6.36,
      "grad_norm": 2.178433656692505,
      "learning_rate": 7.280000000000001e-06,
      "loss": 2.3053,
      "step": 31800
    },
    {
      "epoch": 6.37,
      "grad_norm": 2.0053470134735107,
      "learning_rate": 7.260000000000001e-06,
      "loss": 2.3053,
      "step": 31850
    },
    {
      "epoch": 6.38,
      "grad_norm": 2.879831552505493,
      "learning_rate": 7.24e-06,
      "loss": 2.3051,
      "step": 31900
    },
    {
      "epoch": 6.39,
      "grad_norm": 1.9376016855239868,
      "learning_rate": 7.22e-06,
      "loss": 2.3052,
      "step": 31950
    },
    {
      "epoch": 6.4,
      "grad_norm": 1.8997433185577393,
      "learning_rate": 7.2000000000000005e-06,
      "loss": 2.3065,
      "step": 32000
    },
    {
      "epoch": 6.41,
      "grad_norm": 1.9970670938491821,
      "learning_rate": 7.180000000000001e-06,
      "loss": 2.311,
      "step": 32050
    },
    {
      "epoch": 6.42,
      "grad_norm": 2.2365782260894775,
      "learning_rate": 7.16e-06,
      "loss": 2.3057,
      "step": 32100
    },
    {
      "epoch": 6.43,
      "grad_norm": 2.6769206523895264,
      "learning_rate": 7.14e-06,
      "loss": 2.305,
      "step": 32150
    },
    {
      "epoch": 6.44,
      "grad_norm": 2.0942811965942383,
      "learning_rate": 7.1200000000000004e-06,
      "loss": 2.2998,
      "step": 32200
    },
    {
      "epoch": 6.45,
      "grad_norm": 2.1550447940826416,
      "learning_rate": 7.100000000000001e-06,
      "loss": 2.3064,
      "step": 32250
    },
    {
      "epoch": 6.46,
      "grad_norm": 2.4392762184143066,
      "learning_rate": 7.08e-06,
      "loss": 2.3004,
      "step": 32300
    },
    {
      "epoch": 6.47,
      "grad_norm": 2.952755928039551,
      "learning_rate": 7.06e-06,
      "loss": 2.2966,
      "step": 32350
    },
    {
      "epoch": 6.48,
      "grad_norm": 5.091666221618652,
      "learning_rate": 7.04e-06,
      "loss": 2.3063,
      "step": 32400
    },
    {
      "epoch": 6.49,
      "grad_norm": 1.862101435661316,
      "learning_rate": 7.0200000000000006e-06,
      "loss": 2.3035,
      "step": 32450
    },
    {
      "epoch": 6.5,
      "grad_norm": 2.0266740322113037,
      "learning_rate": 7e-06,
      "loss": 2.3085,
      "step": 32500
    },
    {
      "epoch": 6.51,
      "grad_norm": 1.7137279510498047,
      "learning_rate": 6.98e-06,
      "loss": 2.3027,
      "step": 32550
    },
    {
      "epoch": 6.52,
      "grad_norm": 2.1133687496185303,
      "learning_rate": 6.96e-06,
      "loss": 2.306,
      "step": 32600
    },
    {
      "epoch": 6.53,
      "grad_norm": 1.9467312097549438,
      "learning_rate": 6.9400000000000005e-06,
      "loss": 2.3086,
      "step": 32650
    },
    {
      "epoch": 6.54,
      "grad_norm": 2.319089412689209,
      "learning_rate": 6.92e-06,
      "loss": 2.3035,
      "step": 32700
    },
    {
      "epoch": 6.55,
      "grad_norm": 2.1411666870117188,
      "learning_rate": 6.9e-06,
      "loss": 2.3065,
      "step": 32750
    },
    {
      "epoch": 6.5600000000000005,
      "grad_norm": 2.6837639808654785,
      "learning_rate": 6.88e-06,
      "loss": 2.3037,
      "step": 32800
    },
    {
      "epoch": 6.57,
      "grad_norm": 2.948906898498535,
      "learning_rate": 6.860000000000001e-06,
      "loss": 2.3074,
      "step": 32850
    },
    {
      "epoch": 6.58,
      "grad_norm": 2.547851324081421,
      "learning_rate": 6.8400000000000014e-06,
      "loss": 2.3062,
      "step": 32900
    },
    {
      "epoch": 6.59,
      "grad_norm": 2.8760628700256348,
      "learning_rate": 6.820000000000001e-06,
      "loss": 2.3017,
      "step": 32950
    },
    {
      "epoch": 6.6,
      "grad_norm": 3.3706600666046143,
      "learning_rate": 6.800000000000001e-06,
      "loss": 2.3023,
      "step": 33000
    },
    {
      "epoch": 6.61,
      "grad_norm": 2.2948153018951416,
      "learning_rate": 6.780000000000001e-06,
      "loss": 2.311,
      "step": 33050
    },
    {
      "epoch": 6.62,
      "grad_norm": 2.245314598083496,
      "learning_rate": 6.760000000000001e-06,
      "loss": 2.2976,
      "step": 33100
    },
    {
      "epoch": 6.63,
      "grad_norm": 2.8290858268737793,
      "learning_rate": 6.740000000000001e-06,
      "loss": 2.3074,
      "step": 33150
    },
    {
      "epoch": 6.64,
      "grad_norm": 2.2169928550720215,
      "learning_rate": 6.720000000000001e-06,
      "loss": 2.3019,
      "step": 33200
    },
    {
      "epoch": 6.65,
      "grad_norm": 3.004506826400757,
      "learning_rate": 6.700000000000001e-06,
      "loss": 2.3042,
      "step": 33250
    },
    {
      "epoch": 6.66,
      "grad_norm": 2.0989058017730713,
      "learning_rate": 6.680000000000001e-06,
      "loss": 2.3035,
      "step": 33300
    },
    {
      "epoch": 6.67,
      "grad_norm": 2.144847869873047,
      "learning_rate": 6.660000000000001e-06,
      "loss": 2.3075,
      "step": 33350
    },
    {
      "epoch": 6.68,
      "grad_norm": 2.2945380210876465,
      "learning_rate": 6.640000000000001e-06,
      "loss": 2.3029,
      "step": 33400
    },
    {
      "epoch": 6.6899999999999995,
      "grad_norm": 2.6350276470184326,
      "learning_rate": 6.620000000000001e-06,
      "loss": 2.3098,
      "step": 33450
    },
    {
      "epoch": 6.7,
      "grad_norm": 2.615997791290283,
      "learning_rate": 6.600000000000001e-06,
      "loss": 2.3007,
      "step": 33500
    },
    {
      "epoch": 6.71,
      "grad_norm": 2.046395778656006,
      "learning_rate": 6.5800000000000005e-06,
      "loss": 2.3071,
      "step": 33550
    },
    {
      "epoch": 6.72,
      "grad_norm": 2.047410726547241,
      "learning_rate": 6.560000000000001e-06,
      "loss": 2.3077,
      "step": 33600
    },
    {
      "epoch": 6.73,
      "grad_norm": 2.5634918212890625,
      "learning_rate": 6.540000000000001e-06,
      "loss": 2.3032,
      "step": 33650
    },
    {
      "epoch": 6.74,
      "grad_norm": 2.637038469314575,
      "learning_rate": 6.520000000000001e-06,
      "loss": 2.3047,
      "step": 33700
    },
    {
      "epoch": 6.75,
      "grad_norm": 4.023037433624268,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 2.2977,
      "step": 33750
    },
    {
      "epoch": 6.76,
      "grad_norm": 3.0403988361358643,
      "learning_rate": 6.480000000000001e-06,
      "loss": 2.3007,
      "step": 33800
    },
    {
      "epoch": 6.77,
      "grad_norm": 4.127804279327393,
      "learning_rate": 6.460000000000001e-06,
      "loss": 2.3035,
      "step": 33850
    },
    {
      "epoch": 6.78,
      "grad_norm": 2.6000776290893555,
      "learning_rate": 6.440000000000001e-06,
      "loss": 2.3011,
      "step": 33900
    },
    {
      "epoch": 6.79,
      "grad_norm": 2.8481061458587646,
      "learning_rate": 6.42e-06,
      "loss": 2.3013,
      "step": 33950
    },
    {
      "epoch": 6.8,
      "grad_norm": 2.6258039474487305,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 2.3007,
      "step": 34000
    },
    {
      "epoch": 6.8100000000000005,
      "grad_norm": 2.7604949474334717,
      "learning_rate": 6.380000000000001e-06,
      "loss": 2.3015,
      "step": 34050
    },
    {
      "epoch": 6.82,
      "grad_norm": 2.177971601486206,
      "learning_rate": 6.360000000000001e-06,
      "loss": 2.3096,
      "step": 34100
    },
    {
      "epoch": 6.83,
      "grad_norm": 2.251556396484375,
      "learning_rate": 6.34e-06,
      "loss": 2.3052,
      "step": 34150
    },
    {
      "epoch": 6.84,
      "grad_norm": 1.9665220975875854,
      "learning_rate": 6.3200000000000005e-06,
      "loss": 2.3042,
      "step": 34200
    },
    {
      "epoch": 6.85,
      "grad_norm": 2.589402675628662,
      "learning_rate": 6.300000000000001e-06,
      "loss": 2.3051,
      "step": 34250
    },
    {
      "epoch": 6.86,
      "grad_norm": 3.0564310550689697,
      "learning_rate": 6.280000000000001e-06,
      "loss": 2.307,
      "step": 34300
    },
    {
      "epoch": 6.87,
      "grad_norm": 2.287426471710205,
      "learning_rate": 6.26e-06,
      "loss": 2.3042,
      "step": 34350
    },
    {
      "epoch": 6.88,
      "grad_norm": 3.524685859680176,
      "learning_rate": 6.24e-06,
      "loss": 2.3082,
      "step": 34400
    },
    {
      "epoch": 6.89,
      "grad_norm": 2.3281915187835693,
      "learning_rate": 6.220000000000001e-06,
      "loss": 2.3082,
      "step": 34450
    },
    {
      "epoch": 6.9,
      "grad_norm": 2.3597936630249023,
      "learning_rate": 6.200000000000001e-06,
      "loss": 2.3042,
      "step": 34500
    },
    {
      "epoch": 6.91,
      "grad_norm": 2.1398651599884033,
      "learning_rate": 6.18e-06,
      "loss": 2.3024,
      "step": 34550
    },
    {
      "epoch": 6.92,
      "grad_norm": 1.9671604633331299,
      "learning_rate": 6.16e-06,
      "loss": 2.3037,
      "step": 34600
    },
    {
      "epoch": 6.93,
      "grad_norm": 2.4621076583862305,
      "learning_rate": 6.1400000000000005e-06,
      "loss": 2.3021,
      "step": 34650
    },
    {
      "epoch": 6.9399999999999995,
      "grad_norm": 2.5528910160064697,
      "learning_rate": 6.120000000000001e-06,
      "loss": 2.3064,
      "step": 34700
    },
    {
      "epoch": 6.95,
      "grad_norm": 2.007253408432007,
      "learning_rate": 6.1e-06,
      "loss": 2.3024,
      "step": 34750
    },
    {
      "epoch": 6.96,
      "grad_norm": 2.258230686187744,
      "learning_rate": 6.08e-06,
      "loss": 2.3042,
      "step": 34800
    },
    {
      "epoch": 6.97,
      "grad_norm": 2.2780120372772217,
      "learning_rate": 6.0600000000000004e-06,
      "loss": 2.3071,
      "step": 34850
    },
    {
      "epoch": 6.98,
      "grad_norm": 3.2872049808502197,
      "learning_rate": 6.040000000000001e-06,
      "loss": 2.3063,
      "step": 34900
    },
    {
      "epoch": 6.99,
      "grad_norm": 2.5522584915161133,
      "learning_rate": 6.02e-06,
      "loss": 2.3076,
      "step": 34950
    },
    {
      "epoch": 7.0,
      "grad_norm": 2.266815423965454,
      "learning_rate": 6e-06,
      "loss": 2.3061,
      "step": 35000
    },
    {
      "epoch": 7.0,
      "eval_loss": 2.304119825363159,
      "eval_runtime": 442.729,
      "eval_samples_per_second": 22.587,
      "eval_steps_per_second": 2.823,
      "step": 35000
    },
    {
      "epoch": 7.01,
      "grad_norm": 3.2899036407470703,
      "learning_rate": 5.98e-06,
      "loss": 2.3025,
      "step": 35050
    },
    {
      "epoch": 7.02,
      "grad_norm": 2.3409135341644287,
      "learning_rate": 5.9600000000000005e-06,
      "loss": 2.3018,
      "step": 35100
    },
    {
      "epoch": 7.03,
      "grad_norm": 2.712967872619629,
      "learning_rate": 5.94e-06,
      "loss": 2.3043,
      "step": 35150
    },
    {
      "epoch": 7.04,
      "grad_norm": 2.1725616455078125,
      "learning_rate": 5.92e-06,
      "loss": 2.3038,
      "step": 35200
    },
    {
      "epoch": 7.05,
      "grad_norm": 2.6533186435699463,
      "learning_rate": 5.9e-06,
      "loss": 2.305,
      "step": 35250
    },
    {
      "epoch": 7.06,
      "grad_norm": 2.7729525566101074,
      "learning_rate": 5.8800000000000005e-06,
      "loss": 2.3035,
      "step": 35300
    },
    {
      "epoch": 7.07,
      "grad_norm": 2.1037802696228027,
      "learning_rate": 5.86e-06,
      "loss": 2.2985,
      "step": 35350
    },
    {
      "epoch": 7.08,
      "grad_norm": 2.0776784420013428,
      "learning_rate": 5.84e-06,
      "loss": 2.3013,
      "step": 35400
    },
    {
      "epoch": 7.09,
      "grad_norm": 2.5960536003112793,
      "learning_rate": 5.82e-06,
      "loss": 2.3054,
      "step": 35450
    },
    {
      "epoch": 7.1,
      "grad_norm": 1.8401299715042114,
      "learning_rate": 5.8e-06,
      "loss": 2.3009,
      "step": 35500
    },
    {
      "epoch": 7.11,
      "grad_norm": 2.307668447494507,
      "learning_rate": 5.78e-06,
      "loss": 2.3064,
      "step": 35550
    },
    {
      "epoch": 7.12,
      "grad_norm": 2.4265129566192627,
      "learning_rate": 5.76e-06,
      "loss": 2.3046,
      "step": 35600
    },
    {
      "epoch": 7.13,
      "grad_norm": 2.987403392791748,
      "learning_rate": 5.74e-06,
      "loss": 2.3093,
      "step": 35650
    },
    {
      "epoch": 7.14,
      "grad_norm": 2.0816218852996826,
      "learning_rate": 5.72e-06,
      "loss": 2.3119,
      "step": 35700
    },
    {
      "epoch": 7.15,
      "grad_norm": 2.558690071105957,
      "learning_rate": 5.7e-06,
      "loss": 2.306,
      "step": 35750
    },
    {
      "epoch": 7.16,
      "grad_norm": 2.543161153793335,
      "learning_rate": 5.68e-06,
      "loss": 2.3012,
      "step": 35800
    },
    {
      "epoch": 7.17,
      "grad_norm": 4.017397403717041,
      "learning_rate": 5.66e-06,
      "loss": 2.3049,
      "step": 35850
    },
    {
      "epoch": 7.18,
      "grad_norm": 2.3107452392578125,
      "learning_rate": 5.64e-06,
      "loss": 2.3059,
      "step": 35900
    },
    {
      "epoch": 7.19,
      "grad_norm": 2.180063486099243,
      "learning_rate": 5.620000000000001e-06,
      "loss": 2.3051,
      "step": 35950
    },
    {
      "epoch": 7.2,
      "grad_norm": 2.1365787982940674,
      "learning_rate": 5.600000000000001e-06,
      "loss": 2.3048,
      "step": 36000
    },
    {
      "epoch": 7.21,
      "grad_norm": 3.2742791175842285,
      "learning_rate": 5.580000000000001e-06,
      "loss": 2.305,
      "step": 36050
    },
    {
      "epoch": 7.22,
      "grad_norm": 1.7527769804000854,
      "learning_rate": 5.560000000000001e-06,
      "loss": 2.3042,
      "step": 36100
    },
    {
      "epoch": 7.23,
      "grad_norm": 1.8976123332977295,
      "learning_rate": 5.540000000000001e-06,
      "loss": 2.3054,
      "step": 36150
    },
    {
      "epoch": 7.24,
      "grad_norm": 3.463707685470581,
      "learning_rate": 5.5200000000000005e-06,
      "loss": 2.3065,
      "step": 36200
    },
    {
      "epoch": 7.25,
      "grad_norm": 3.0130269527435303,
      "learning_rate": 5.500000000000001e-06,
      "loss": 2.298,
      "step": 36250
    },
    {
      "epoch": 7.26,
      "grad_norm": 2.559272527694702,
      "learning_rate": 5.480000000000001e-06,
      "loss": 2.3036,
      "step": 36300
    },
    {
      "epoch": 7.27,
      "grad_norm": 2.0999836921691895,
      "learning_rate": 5.460000000000001e-06,
      "loss": 2.3001,
      "step": 36350
    },
    {
      "epoch": 7.28,
      "grad_norm": 3.115741729736328,
      "learning_rate": 5.4400000000000004e-06,
      "loss": 2.3119,
      "step": 36400
    },
    {
      "epoch": 7.29,
      "grad_norm": 2.4876675605773926,
      "learning_rate": 5.420000000000001e-06,
      "loss": 2.3031,
      "step": 36450
    },
    {
      "epoch": 7.3,
      "grad_norm": 2.3593485355377197,
      "learning_rate": 5.400000000000001e-06,
      "loss": 2.3026,
      "step": 36500
    },
    {
      "epoch": 7.31,
      "grad_norm": 3.7975456714630127,
      "learning_rate": 5.380000000000001e-06,
      "loss": 2.3073,
      "step": 36550
    },
    {
      "epoch": 7.32,
      "grad_norm": 2.028317451477051,
      "learning_rate": 5.36e-06,
      "loss": 2.3009,
      "step": 36600
    },
    {
      "epoch": 7.33,
      "grad_norm": 2.3509016036987305,
      "learning_rate": 5.3400000000000005e-06,
      "loss": 2.3002,
      "step": 36650
    },
    {
      "epoch": 7.34,
      "grad_norm": 2.2186968326568604,
      "learning_rate": 5.320000000000001e-06,
      "loss": 2.2982,
      "step": 36700
    },
    {
      "epoch": 7.35,
      "grad_norm": 2.426612138748169,
      "learning_rate": 5.300000000000001e-06,
      "loss": 2.3078,
      "step": 36750
    },
    {
      "epoch": 7.36,
      "grad_norm": 2.207942247390747,
      "learning_rate": 5.28e-06,
      "loss": 2.3054,
      "step": 36800
    },
    {
      "epoch": 7.37,
      "grad_norm": 2.682003974914551,
      "learning_rate": 5.2600000000000005e-06,
      "loss": 2.3089,
      "step": 36850
    },
    {
      "epoch": 7.38,
      "grad_norm": 2.1589038372039795,
      "learning_rate": 5.240000000000001e-06,
      "loss": 2.3053,
      "step": 36900
    },
    {
      "epoch": 7.39,
      "grad_norm": 2.2472774982452393,
      "learning_rate": 5.220000000000001e-06,
      "loss": 2.3034,
      "step": 36950
    },
    {
      "epoch": 7.4,
      "grad_norm": 2.22072172164917,
      "learning_rate": 5.2e-06,
      "loss": 2.3078,
      "step": 37000
    },
    {
      "epoch": 7.41,
      "grad_norm": 2.3471105098724365,
      "learning_rate": 5.18e-06,
      "loss": 2.3046,
      "step": 37050
    },
    {
      "epoch": 7.42,
      "grad_norm": 3.6866579055786133,
      "learning_rate": 5.1600000000000006e-06,
      "loss": 2.2997,
      "step": 37100
    },
    {
      "epoch": 7.43,
      "grad_norm": 2.6076552867889404,
      "learning_rate": 5.140000000000001e-06,
      "loss": 2.3057,
      "step": 37150
    },
    {
      "epoch": 7.44,
      "grad_norm": 2.47062087059021,
      "learning_rate": 5.12e-06,
      "loss": 2.3028,
      "step": 37200
    },
    {
      "epoch": 7.45,
      "grad_norm": 2.3720738887786865,
      "learning_rate": 5.1e-06,
      "loss": 2.3082,
      "step": 37250
    },
    {
      "epoch": 7.46,
      "grad_norm": 2.574333667755127,
      "learning_rate": 5.0800000000000005e-06,
      "loss": 2.3064,
      "step": 37300
    },
    {
      "epoch": 7.47,
      "grad_norm": 2.5315659046173096,
      "learning_rate": 5.060000000000001e-06,
      "loss": 2.3055,
      "step": 37350
    },
    {
      "epoch": 7.48,
      "grad_norm": 2.239516496658325,
      "learning_rate": 5.04e-06,
      "loss": 2.3036,
      "step": 37400
    },
    {
      "epoch": 7.49,
      "grad_norm": 2.6684000492095947,
      "learning_rate": 5.02e-06,
      "loss": 2.3091,
      "step": 37450
    },
    {
      "epoch": 7.5,
      "grad_norm": 1.8432410955429077,
      "learning_rate": 5e-06,
      "loss": 2.3033,
      "step": 37500
    },
    {
      "epoch": 7.51,
      "grad_norm": 1.8388959169387817,
      "learning_rate": 4.980000000000001e-06,
      "loss": 2.3052,
      "step": 37550
    },
    {
      "epoch": 7.52,
      "grad_norm": 2.653771162033081,
      "learning_rate": 4.960000000000001e-06,
      "loss": 2.3041,
      "step": 37600
    },
    {
      "epoch": 7.53,
      "grad_norm": 2.385406255722046,
      "learning_rate": 4.94e-06,
      "loss": 2.3054,
      "step": 37650
    },
    {
      "epoch": 7.54,
      "grad_norm": 2.607447862625122,
      "learning_rate": 4.92e-06,
      "loss": 2.3065,
      "step": 37700
    },
    {
      "epoch": 7.55,
      "grad_norm": 3.5289883613586426,
      "learning_rate": 4.9000000000000005e-06,
      "loss": 2.3076,
      "step": 37750
    },
    {
      "epoch": 7.5600000000000005,
      "grad_norm": 1.776977777481079,
      "learning_rate": 4.880000000000001e-06,
      "loss": 2.3027,
      "step": 37800
    },
    {
      "epoch": 7.57,
      "grad_norm": 2.6221132278442383,
      "learning_rate": 4.86e-06,
      "loss": 2.3026,
      "step": 37850
    },
    {
      "epoch": 7.58,
      "grad_norm": 2.4726200103759766,
      "learning_rate": 4.84e-06,
      "loss": 2.3015,
      "step": 37900
    },
    {
      "epoch": 7.59,
      "grad_norm": 2.7415761947631836,
      "learning_rate": 4.8200000000000004e-06,
      "loss": 2.302,
      "step": 37950
    },
    {
      "epoch": 7.6,
      "grad_norm": 2.2210965156555176,
      "learning_rate": 4.800000000000001e-06,
      "loss": 2.308,
      "step": 38000
    },
    {
      "epoch": 7.61,
      "grad_norm": 2.252146005630493,
      "learning_rate": 4.78e-06,
      "loss": 2.301,
      "step": 38050
    },
    {
      "epoch": 7.62,
      "grad_norm": 2.341628074645996,
      "learning_rate": 4.76e-06,
      "loss": 2.3019,
      "step": 38100
    },
    {
      "epoch": 7.63,
      "grad_norm": 2.1367619037628174,
      "learning_rate": 4.74e-06,
      "loss": 2.308,
      "step": 38150
    },
    {
      "epoch": 7.64,
      "grad_norm": 3.3694493770599365,
      "learning_rate": 4.7200000000000005e-06,
      "loss": 2.3011,
      "step": 38200
    },
    {
      "epoch": 7.65,
      "grad_norm": 3.1735997200012207,
      "learning_rate": 4.7e-06,
      "loss": 2.3032,
      "step": 38250
    },
    {
      "epoch": 7.66,
      "grad_norm": 2.475865125656128,
      "learning_rate": 4.680000000000001e-06,
      "loss": 2.3046,
      "step": 38300
    },
    {
      "epoch": 7.67,
      "grad_norm": 2.793370008468628,
      "learning_rate": 4.66e-06,
      "loss": 2.3037,
      "step": 38350
    },
    {
      "epoch": 7.68,
      "grad_norm": 2.3774783611297607,
      "learning_rate": 4.6400000000000005e-06,
      "loss": 2.3009,
      "step": 38400
    },
    {
      "epoch": 7.6899999999999995,
      "grad_norm": 3.1572189331054688,
      "learning_rate": 4.620000000000001e-06,
      "loss": 2.3052,
      "step": 38450
    },
    {
      "epoch": 7.7,
      "grad_norm": 3.079153060913086,
      "learning_rate": 4.600000000000001e-06,
      "loss": 2.302,
      "step": 38500
    },
    {
      "epoch": 7.71,
      "grad_norm": 1.9374126195907593,
      "learning_rate": 4.58e-06,
      "loss": 2.3009,
      "step": 38550
    },
    {
      "epoch": 7.72,
      "grad_norm": 2.8650317192077637,
      "learning_rate": 4.56e-06,
      "loss": 2.3033,
      "step": 38600
    },
    {
      "epoch": 7.73,
      "grad_norm": 2.343463182449341,
      "learning_rate": 4.540000000000001e-06,
      "loss": 2.2981,
      "step": 38650
    },
    {
      "epoch": 7.74,
      "grad_norm": 2.6057772636413574,
      "learning_rate": 4.520000000000001e-06,
      "loss": 2.3069,
      "step": 38700
    },
    {
      "epoch": 7.75,
      "grad_norm": 3.091970920562744,
      "learning_rate": 4.5e-06,
      "loss": 2.3026,
      "step": 38750
    },
    {
      "epoch": 7.76,
      "grad_norm": 2.408308267593384,
      "learning_rate": 4.48e-06,
      "loss": 2.3095,
      "step": 38800
    },
    {
      "epoch": 7.77,
      "grad_norm": 2.586900472640991,
      "learning_rate": 4.4600000000000005e-06,
      "loss": 2.3068,
      "step": 38850
    },
    {
      "epoch": 7.78,
      "grad_norm": 2.279012441635132,
      "learning_rate": 4.440000000000001e-06,
      "loss": 2.3073,
      "step": 38900
    },
    {
      "epoch": 7.79,
      "grad_norm": 2.4237916469573975,
      "learning_rate": 4.42e-06,
      "loss": 2.3031,
      "step": 38950
    },
    {
      "epoch": 7.8,
      "grad_norm": 3.282724618911743,
      "learning_rate": 4.4e-06,
      "loss": 2.3066,
      "step": 39000
    },
    {
      "epoch": 7.8100000000000005,
      "grad_norm": 2.570235252380371,
      "learning_rate": 4.38e-06,
      "loss": 2.3061,
      "step": 39050
    },
    {
      "epoch": 7.82,
      "grad_norm": 2.461674690246582,
      "learning_rate": 4.360000000000001e-06,
      "loss": 2.3034,
      "step": 39100
    },
    {
      "epoch": 7.83,
      "grad_norm": 2.122076988220215,
      "learning_rate": 4.34e-06,
      "loss": 2.3023,
      "step": 39150
    },
    {
      "epoch": 7.84,
      "grad_norm": 2.0791239738464355,
      "learning_rate": 4.32e-06,
      "loss": 2.3061,
      "step": 39200
    },
    {
      "epoch": 7.85,
      "grad_norm": 3.7101690769195557,
      "learning_rate": 4.3e-06,
      "loss": 2.3024,
      "step": 39250
    },
    {
      "epoch": 7.86,
      "grad_norm": 2.131781816482544,
      "learning_rate": 4.2800000000000005e-06,
      "loss": 2.3063,
      "step": 39300
    },
    {
      "epoch": 7.87,
      "grad_norm": 2.051103115081787,
      "learning_rate": 4.26e-06,
      "loss": 2.3077,
      "step": 39350
    },
    {
      "epoch": 7.88,
      "grad_norm": 2.513620376586914,
      "learning_rate": 4.24e-06,
      "loss": 2.3057,
      "step": 39400
    },
    {
      "epoch": 7.89,
      "grad_norm": 2.4692487716674805,
      "learning_rate": 4.22e-06,
      "loss": 2.302,
      "step": 39450
    },
    {
      "epoch": 7.9,
      "grad_norm": 2.502405881881714,
      "learning_rate": 4.2000000000000004e-06,
      "loss": 2.3051,
      "step": 39500
    },
    {
      "epoch": 7.91,
      "grad_norm": 2.534421682357788,
      "learning_rate": 4.18e-06,
      "loss": 2.3033,
      "step": 39550
    },
    {
      "epoch": 7.92,
      "grad_norm": 2.3864552974700928,
      "learning_rate": 4.16e-06,
      "loss": 2.3065,
      "step": 39600
    },
    {
      "epoch": 7.93,
      "grad_norm": 2.7624361515045166,
      "learning_rate": 4.14e-06,
      "loss": 2.3029,
      "step": 39650
    },
    {
      "epoch": 7.9399999999999995,
      "grad_norm": 4.089295387268066,
      "learning_rate": 4.12e-06,
      "loss": 2.3039,
      "step": 39700
    },
    {
      "epoch": 7.95,
      "grad_norm": 2.865680456161499,
      "learning_rate": 4.1e-06,
      "loss": 2.3047,
      "step": 39750
    },
    {
      "epoch": 7.96,
      "grad_norm": 2.374423027038574,
      "learning_rate": 4.08e-06,
      "loss": 2.306,
      "step": 39800
    },
    {
      "epoch": 7.97,
      "grad_norm": 2.5309531688690186,
      "learning_rate": 4.060000000000001e-06,
      "loss": 2.3035,
      "step": 39850
    },
    {
      "epoch": 7.98,
      "grad_norm": 2.3371551036834717,
      "learning_rate": 4.04e-06,
      "loss": 2.2999,
      "step": 39900
    },
    {
      "epoch": 7.99,
      "grad_norm": 2.7729134559631348,
      "learning_rate": 4.0200000000000005e-06,
      "loss": 2.3049,
      "step": 39950
    },
    {
      "epoch": 8.0,
      "grad_norm": 2.066315174102783,
      "learning_rate": 4.000000000000001e-06,
      "loss": 2.3056,
      "step": 40000
    },
    {
      "epoch": 8.0,
      "eval_loss": 2.3035104274749756,
      "eval_runtime": 87.7211,
      "eval_samples_per_second": 113.998,
      "eval_steps_per_second": 14.25,
      "step": 40000
    }
  ],
  "logging_steps": 50,
  "max_steps": 50000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 3,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4095425690880000.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
