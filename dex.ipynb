
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load the tokenizer and model
model_name = "ModelCloud/DeepSeek-R1-Distill-Qwen-7B-gptqmodel-4bit-vortex-v1"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto")

# Move the model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[2], line 7
      5 model_name = "ModelCloud/DeepSeek-R1-Distill-Qwen-7B-gptqmodel-4bit-vortex-v1"
      6 tokenizer = AutoTokenizer.from_pretrained(model_name)
----> 7 model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto")
      9 # Move the model to GPU if available
     10 device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

File c:\Users\tumtom\AppData\Local\Programs\Python\Python313\Lib\site-packages\transformers\models\auto\auto_factory.py:564, in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    562 elif type(config) in cls._model_mapping.keys():
    563     model_class = _get_model_class(config, cls._model_mapping)
--> 564     return model_class.from_pretrained(
    565         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
    566     )
    567 raise ValueError(
    568     f"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\n"
    569     f"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}."
    570 )

File c:\Users\tumtom\AppData\Local\Programs\Python\Python313\Lib\site-packages\transformers\modeling_utils.py:262, in restore_default_torch_dtype.<locals>._wrapper(*args, **kwargs)
    260 old_dtype = torch.get_default_dtype()
    261 try:
--> 262     return func(*args, **kwargs)
    263 finally:
    264     torch.set_default_dtype(old_dtype)

File c:\Users\tumtom\AppData\Local\Programs\Python\Python313\Lib\site-packages\transformers\modeling_utils.py:3698, in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)
   3695     hf_quantizer = None
   3697 if hf_quantizer is not None:
-> 3698     hf_quantizer.validate_environment(
   3699         torch_dtype=torch_dtype,
   3700         from_tf=from_tf,
   3701         from_flax=from_flax,
   3702         device_map=device_map,
   3703         weights_only=weights_only,
   3704     )
   3705     torch_dtype = hf_quantizer.update_torch_dtype(torch_dtype)
   3706     device_map = hf_quantizer.update_device_map(device_map)

File c:\Users\tumtom\AppData\Local\Programs\Python\Python313\Lib\site-packages\transformers\quantizers\quantizer_gptq.py:65, in GptqHfQuantizer.validate_environment(self, *args, **kwargs)
     60 gptq_supports_cpu = (
     61     is_auto_gptq_available()
     62     and version.parse(importlib.metadata.version("auto-gptq")) > version.parse("0.4.2")
     63 ) or is_gptqmodel_available()
     64 if not gptq_supports_cpu and not torch.cuda.is_available():
---> 65     raise RuntimeError("GPU is required to quantize or run quantize model.")
     66 elif not (is_auto_gptq_available() or is_gptqmodel_available()):
     67     raise ImportError(
     68         "Loading a GPTQ quantized model requires gptqmodel (`pip install gptqmodel`) or auto-gptq (`pip install auto-gptq`) library. "
     69     )

RuntimeError: GPU is required to quantize or run quantize model.
Loading the pretrained model

from transformers import AutoModelForSequenceClassification, AutoTokenizer

# Load the pretrained model and tokenizer
model_name = "ModelCloud/DeepSeek-R1-Distill-Qwen-7B-gptqmodel-4bit-vortex-v1"
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
c:\Users\tumtom\AppData\Local\Programs\Python\Python313\Lib\site-packages\tqdm\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[1], line 5
      3 # Load the pretrained model and tokenizer
      4 model_name = "ModelCloud/DeepSeek-R1-Distill-Qwen-7B-gptqmodel-4bit-vortex-v1"
----> 5 model = AutoModelForSequenceClassification.from_pretrained(model_name)
      6 tokenizer = AutoTokenizer.from_pretrained(model_name)

File c:\Users\tumtom\AppData\Local\Programs\Python\Python313\Lib\site-packages\transformers\models\auto\auto_factory.py:564, in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    562 elif type(config) in cls._model_mapping.keys():
    563     model_class = _get_model_class(config, cls._model_mapping)
--> 564     return model_class.from_pretrained(
    565         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
    566     )
    567 raise ValueError(
    568     f"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\n"
    569     f"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}."
    570 )

File c:\Users\tumtom\AppData\Local\Programs\Python\Python313\Lib\site-packages\transformers\modeling_utils.py:262, in restore_default_torch_dtype.<locals>._wrapper(*args, **kwargs)
    260 old_dtype = torch.get_default_dtype()
    261 try:
--> 262     return func(*args, **kwargs)
    263 finally:
    264     torch.set_default_dtype(old_dtype)

File c:\Users\tumtom\AppData\Local\Programs\Python\Python313\Lib\site-packages\transformers\modeling_utils.py:3698, in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)
   3695     hf_quantizer = None
   3697 if hf_quantizer is not None:
-> 3698     hf_quantizer.validate_environment(
   3699         torch_dtype=torch_dtype,
   3700         from_tf=from_tf,
   3701         from_flax=from_flax,
   3702         device_map=device_map,
   3703         weights_only=weights_only,
   3704     )
   3705     torch_dtype = hf_quantizer.update_torch_dtype(torch_dtype)
   3706     device_map = hf_quantizer.update_device_map(device_map)

File c:\Users\tumtom\AppData\Local\Programs\Python\Python313\Lib\site-packages\transformers\quantizers\quantizer_gptq.py:65, in GptqHfQuantizer.validate_environment(self, *args, **kwargs)
     60 gptq_supports_cpu = (
     61     is_auto_gptq_available()
     62     and version.parse(importlib.metadata.version("auto-gptq")) > version.parse("0.4.2")
     63 ) or is_gptqmodel_available()
     64 if not gptq_supports_cpu and not torch.cuda.is_available():
---> 65     raise RuntimeError("GPU is required to quantize or run quantize model.")
     66 elif not (is_auto_gptq_available() or is_gptqmodel_available()):
     67     raise ImportError(
     68         "Loading a GPTQ quantized model requires gptqmodel (`pip install gptqmodel`) or auto-gptq (`pip install auto-gptq`) library. "
     69     )

RuntimeError: GPU is required to quantize or run quantize model.
